Job Title,Company Name,Location,Salary Estimate,Rating,Job Description,Founded,Industry,Revenue,Sector,Size,Type,Easy Apply
Data Engineer,"Billings Clinic
","Billings, MT",$68K - $102K (Employer est.),3.3,"You’ll want to join Billings Clinic for our outstanding quality of care, exciting environment, interesting cases from a vast geography, advanced technology and educational opportunities. We are in the top 1% of hospitals internationally for receiving Magnet® Recognition consecutively since 2006.

And you’ll want to stay at Billings Clinic for the amazing teamwork, caring atmosphere, and a culture that values kindness, safety and courage. This is an incredible place to learn and grow. Billings, Montana, is a friendly, college community in the Rocky Mountains with great schools and abundant family activities. Amazing outdoor recreation is just minutes from home. Four seasons of sunshine!

You can make a difference here.

About Us
Billings Clinic is a community-owned, not-for-profit, Physician-led health system based in Billings with more than 4,700 employees, including over 550 physicians and non-physician providers. Our integrated organization consists of a multi-specialty group practice and a 304-bed hospital. Learn more about Billings Clinic (our organization, history, mission, leadership and regional locations) and how we are recognized nationally for our exceptional quality.

Your Benefits
We provide a comprehensive and competitive benefits package to all permanent full-time employees (minimum of 24 hours/week), including Medical, Dental, Vision, 403(b) Retirement Plan with employer matching, Defined Contribution Pension Plan, Paid Time Off, employee wellness program, and much more.

Magnet: Commitment to Nursing Excellence
Billings Clinic is proud to be recognized for nursing excellence as a Magnet®-designated organization, joining only 97 other organizations worldwide that have achieved this honor four times. The re-designation process happens every four years.

Pre-Employment Requirements
All new employees must complete several pre-employment requirements prior to starting.

Data Engineer
PMO / OPERATIONS - 8732 (ROCKY MOUNTAIN PROFESSIONAL BUILDING)
req6747

Shift: Day
Employment Status: Full-Time (.75 or greater)
Hours per Pay Period: 1.00 = 80 hours (Exempt)
Starting Salary DOE: $68,161 - $102,211


This position is eligible for full-time remote and/or telework if located in Montana, Wyoming, Hawaii, Kansas, Minnesota, Michigan, Arizona, or Texas.


Depending on education and experience, applicants may be considered for Data Engineer, Senior Data Engineer, or Principal Data Engineer

The Data Engineer acts as subject matter experts of the data repositories available within the organization by obtaining an in-depth understanding of each source system. These engineers are primarily focused on the creation, management, and availability of data pipelines to support structured analytics.

Essential Job Functions

Develops data pipelines using extract, transform, and load (ETL/ELT) processing routines for enterprise level data warehousing initiatives.
Writes advanced queries using data scripting language including T-SQL, CCL, Python, Scala.
Create and maintain data structures and data models to support data at all stages of the data pipeline development lifecycle.
Maintain, tune, and scale big data infrastructure platforms (Snowflake, AWS, Azure) to appropriately handle workloads.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, data acquisition, data enrichment, re-designing infrastructure for greater scalability, etc.
Perform database performance tuning methods to ensure uptime and data refresh rate SLAs are met.
Participate in data engineering team code reviews and offer recommendations regarding data architecture.
Supports department BI Analysts in translating business requirements into Azure DevOps development tasks to properly track and implement solutions that meet the needs of stakeholders by way of agile development lifecycles.
Align with Billings Clinic data engineering development standards to deliver uniform results providing a consistent and supportable view of data across the organization.
Duties may require 24 hour, seven days per week on-call responsibility for project support or for escalated support issues.
Supports and models behaviors consistent with Billings Clinic’s mission, vision, values, code of business conduct and service expectations. Meets all mandatory organizational and departmental requirements. Maintains competency in all organizational, departmental and outside agency standards as it relates to the environment, employee, patient safety or job performance.
Performs all other duties as assigned or as needed to meet the needs of the department/organization.
Department or Level Specific Duties and Responsibilities

Drives end-to-end data pipeline development efforts for on-time delivery of high-quality data solutions that conform to requirements and comply with all applicable standards.
Identifies needs, designs, and implements solutions that are automated, scalable, and sustainable while minimizing defects and technical debt.
Evaluates and analyzes the current system architecture to improve uptime and responsiveness. Provides recommendation and executes on activities including database tuning, data structure optimization, and server resource scaling.
Proactively finds opportunities for data acquisition to enrich our data environment.
Actively participates in code reviews and effectively communicate issues and risks to stakeholders
Provides knowledge sharing or mentorship opportunities to other engineers.
Guide our efforts in all areas of database design, performance, and reliability.
Investigates and troubleshoots complicated analytic application and stability issues.
Ensures databases are operational and provide valid and relevant data.
Incorporate unit testing and regression testing to ensure defect-free builds and releases.

Minimum Qualifications

Data Engineer


Education


B.S or equivalent degree in Computer Science, Information Systems, or related field


Experience


Experience or knowledge in DBMS platforms (e.g., Oracle, MSSQL, Snowflake etc.)
Experience or knowledge in data programming languages (e.g., T-SQL, CCL, Python, Scala, etc.)
Familiarity with large-scale data warehouse concepts, ETL/ELT development, data modeling, and database tuning.
Experience or knowledge in database utilities and IDEs (e.g., SSMS, AQT, Visual Studio, etc.)
Experience with coordinating code deployments from development, test, and production environments.
Experience or knowledge of source/version control processes in tools such as (Azure DevOps, Github, etc.)
Knowledge of various software development best practices and methodologies for Agile product delivery
Familiarity with root cause analysis and problem resolution.
Strong verbal and written communication skills, ability to perform under pressure, attention to detail and appreciation for rigorous release management process is important.
An equivalent combination of education and experience will be considered

Senior Data Engineer

Education


B.S or equivalent degree in Computer Science, Information Systems, or related field

Experience

Minimum of 3 years related work experience
Demonstrated strength and proficiency in DBMS platforms (e.g., Oracle, MSSQL, Snowflake etc.)
Demonstrated strength and proficiency in data programming languages (e.g., T-SQL, CCL, Python, Scala, etc.)
Demonstrated strength and proficiency with large-scale data warehouse implementation, ETL/ELT development, data modeling, and database tuning.
Demonstrated strength and proficiency in database utilities and IDEs (e.g., SSMS, AQT, Visual Studio, etc.)
Advanced experience with coordinating code deployments from development, test, and production environments.
Advanced experience of source/version control processes in tools such as (Azure DevOps, Github, etc.)
Advanced experience in software development best practices and methodologies for Agile product delivery
Strong knowledge of root cause analysis and problem resolution.
Strong verbal and written communication skills, ability to perform under pressure, attention to detail and appreciation for rigorous release management process is important.
1 year experience in Vendor management
An equivalent combination of education and experience will be considered

Principal Data Engineer

Education


B.S or equivalent degree in Computer Science, Information Systems, or related field

Experience

Minimum of 8 years related work experience
Minimum of 3 years data architecture experience, including data ingestion, processing, storage and reporting in a big data environment.
Demonstrated ability in mentoring other data engineers on best practices and assisting on setting architectural direction.
Demonstrated ability in taking lead on large scale data engineering initiatives.
Advanced expertise in DBMS platforms (e.g., Oracle, MSSQL, Snowflake etc.)
Advanced proficiency in more than one data programming languages (e.g., T-SQL, CCL, Python, Scala, etc.)
Advanced expertise in large-scale data warehouse implementation, ETL/ELT development, data modeling, and database tuning.
Demonstrated strength and proficiency in database utilities and IDEs (e.g., SSMS, AQT, Visual Studio, etc.)
Advanced experience with coordinating code deployments from development, test, and production environments.
Advanced experience of source/version control processes in tools such as (Azure DevOps, Github, etc.)
Demonstrated strength in software development best practices and methodologies for Agile product delivery
Strong verbal and written communication skills, ability to perform under pressure, attention to detail and appreciation for rigorous release management process is important.
3 years’ experience in Vendor management

Billings Clinic is Montana’s largest health system serving Montana, Wyoming and the western Dakotas. A not-for-profit organization led by a physician CEO, the health system is governed by a board of community members, nurses and physicians. Billings Clinic includes an integrated multi-specialty group practice, tertiary care hospital and trauma center, based in Billings, Montana. Learn more at www.billingsclinic.com/aboutus

Billings Clinic is committed to the principles of Equal Employment Opportunity. All policies and processes are designed toward achieving fair and equitable treatment of all employees and job applicants. Employees are encouraged to discuss any concerns they have in this regard with their immediate supervisor and/or the Vice President People Resources. All employees and job applicants will be provided the same treatment in all aspects of the employment relationship, regardless of race, color, creed, religion, national origin, gender, gender identity, sexual orientation, age, marital status, genetic information or disability.",1939,Health Care Services & Hospitals,$500 million to $1 billion (USD),Healthcare,1001 to 5000 Employees,Nonprofit Organization,False
Data Warehouse Engineer II,First Interstate Bank,"Billings, MT",$82K - $120K (Glassdoor est.),-1.0,"**If you are a current FIB employee, please apply through the Career Worklet in the
Employee Portal
.
This position can be located in Billings, MT or Sioux Falls, SD
What’s Important to You
We know your career is just one aspect of a meaningful, complex, and demanding life. That’s why we designed our compensation and benefits package to provide employees and their families with as much choice as possible.
Accrue and use your paid time off (PTO) immediately – no waiting period – plus paid federal holidays in addition to PTO.
Child Care Assistance Program for eligible dependent(s).
We prioritize wellness by offering eligible employees a monthly stipend, toward a fitness club membership or exercise-related classes.
The health and happiness of the places we call home matter to us. Learn a little more about what we do for the
communities we serve
and why we want YOU to be a part of it.
We encourage you to apply. Reach for what you want and tell us why your work ethic and willingness to learn make you a natural fit for #TeamFirstInterstate.
SUMMARY
The role of Data Warehouse Engineer is to be involved in the design, analysis, and development efforts for the Data Warehouse and Business Intelligence projects, and to resolve complex business problems. This includes modeling, building, and populating the Data Warehouse, as well as providing reporting capabilities.
ESSENTIAL DUTIES & RESPONSIBILITIES include the following: other duties may be assigned.
Assess business rules; perform source to target data mapping; design, review, implement and optimize ETL processes.
Review project plans; develop and test specifications; perform data analysis.
Provide development support for existing systems; troubleshoot data and/or system issues, build and extend toolsets.
Create and/or maintain batch jobs.
Create and maintain online systems and printed documentation.
Provide data analysis and identify data-related issues within the Data Warehouse environment, as well as source systems as needed.
Provide quality service satisfaction by possessing strong customer service skills.
Create and maintain the logical and physical dimensional data models.
Work within an agile development methodology.
Participate in peer code reviews.
Provide feedback to junior data warehouse engineers.
Effectively manage time by analyzing workload, assign priorities, and maintain focus on productive endeavors.
Work with minimal supervision to complete tasks assigned.
Engage as a productive and positive member of the data warehouse team.
QUALIFICATIONS
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
EDUCATION and EXPERIENCE
A Bachelor's degree (BS/BA) from a four-year college or university in Computer Science (or related technical field) and three or more (3+) years' experience; or equivalent combination of education and experience. Experience using Informatica, Microsoft SSIS, or other leading ETL tools preferred. Experience with Spark, Hadoop, or cloud technologies a plus.
LANGUAGE SKILLS
Ability to read, analyze and interpret the most complex documents and to read and interpret data schemas, with emphasis on SQL Server Implementations. Ability to respond effectively to the most sensitive inquiries or complaints. Ability to prepare presentations using original or innovative style, techniques and make persuasive speeches and presentations on controversial or complex topics to top management, public groups and/or board of directors.
MATHEMATICAL SKILLS
Ability to apply advanced mathematical and statistical concepts to tasks such as frequency distributions, analysis of variance, correlation techniques, sampling theory and factor analysis.
REASONING ABILITY
Ability to apply principles, logical or scientific thinking to a wide range of intellectual and practical problems. Ability to deal with a variety of abstract and concrete variables.
PHYSICAL DEMANDS
While performing the duties of this job, the employee is regularly required to talk or hear. The employee frequently is required to stand, walk, and sit. The employee is occasionally required to use hands to finger, handle, or feel; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 25 pounds. Specific vision abilities required by this job include close vision and ability to adjust focus.
WORK ENVIRONMENT
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of the job.
The noise level in the workplace is usually moderate.

**If you are a current FIB employee, please apply through the Career Worklet in the
Employee Portal
.",-1,Banking & Lending,Unknown / Non-Applicable,Financial Services,Unknown,Company - Public,False
"Senior Data Engineer, Public Company","Recruiting From Scratch
","Bozeman, MT",$100K - $200K (Employer est.),4.0,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:

Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.

With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.

What’s so interesting about this role?

Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .

In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.

What’s the job?

Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.

What we’ll love about you

5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.

Salary Range: $165,000-$210,000 USD base. Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/",2019,Staffing & Subcontracting,$1 to $5 million (USD),Human Resources & Staffing,1 to 50 Employees,Company - Private,True
Senior Staff AI Data Engineer,"Recruiting From Scratch
","Bozeman, MT",$160K (Employer est.),4.0,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/

This is a hybrid role based in our Palo Alto or San Francisco offices and will require you to be in office Tuesdays and Thursdays.

What’s so interesting about this role?

We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.

What’s the job?

We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.

In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.

Responsibilities:

Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure

What we’ll love about you

Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)

We’ll really swoon if you have

2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects

What you'll love about us

Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/",2019,Staffing & Subcontracting,$1 to $5 million (USD),Human Resources & Staffing,1 to 50 Employees,Company - Private,True
Senior Data Engineer (Snowflake) - Contractor,"EY
",United States,$90.00 - $130.00 Per Hour (Employer est.),3.9,"The primary responsibility for the Sr. Data Engineer / Tech Lead is to ensure data accessibility, quality, and reliability within the client's data platform ecosystem.

RESPONSIBILITIES

Design and develop end-to-end data pipelines and ETL processes using Snowflake, Denodo, DBT Cloud, Fivetran, and AWS Glue, ensuring seamless data integration, transformation, and loading.
Collaborate with cross-functional teams to gather data requirements and implement scalable data models and schemas within Snowflake and Denodo, ensuring optimal performance and data consistency.
Implement and maintain data orchestration workflows using Stonebranch, ensuring efficient and reliable scheduling and automation of data processes.
Work with EnterpriseDataLake for ingestion and storage of data from various sources, ensuring the availability and accessibility of data for analytics and reporting purposes.
Utilize Azure DevOps and Teraform to implement and manage infrastructure as code, ensuring streamlined deployment and scalability of data engineering solutions.
Skills

snowflake

denodo

dbt cloud

fivetran

Qualifications

REQUIREMENTS

Proficiency in Snowflake and Denodo: Strong understanding and hands-on experience in working with Snowflake and Denodo for data integration, transformation, and analysis.
Knowledge of DBT Cloud: Familiarity with DBT Cloud, including experience in building and maintaining efficient data transformation workflows and deploying reliable data pipelines.
Experience with Fivetran: Demonstrated experience in utilizing Fivetran for data ingestion, source connectivity, and ensuring data accuracy and completeness.
Understanding of EnterpriseDataLake: Knowledge of EnterpriseDataLake principles and practices, including data ingestion, storage, and organization for analytics and reporting purposes.
Familiarity with Stonebranch: Proficiency in using Stonebranch for data orchestration and scheduling, ensuring efficient and reliable execution of data pipelines and workflows.
Proficiency in Azure DevOps and Teraform: Experience in leveraging Azure DevOps and Teraform for infrastructure as code, enabling streamlined deployment and scalability of data engineering solutions.

The expected pay rate range for this contract assignment is $90 to $130 per hour. The exact pay rate will vary based on skills, experience, and location. The position is approximately 65-70% remote and 30-35% travel to the client as needed.

#GigNowTechOpportunities

GigNowOpportunities

Equal Employment Opportunity Information
EY provides equal opportunities to applicants, employees, contractors, vendors, and stakeholders without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law. If you are an individual with a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please email gignow.recruiting@ey.com.",-1,-1,Unknown / Non-Applicable,-1,Unknown,Unknown,False
