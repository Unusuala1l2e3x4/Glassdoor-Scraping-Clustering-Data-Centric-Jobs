Job Title,Company Name,Location,Salary Estimate,Rating,Job Description,Founded,Industry,Revenue,Sector,Size,Type,Easy Apply
Data Engineer (13+ Years),"LTIMindtree
","Chicago, IL",$120K - $130K (Employer est.),3.5,"$120000-$130000

Dice- https://www.dice.com/employer/job/f6c37c38200e42afb6e6aa3671d441b1

About US:

LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.

Job Description:

Data Engineer - Stored procedures, SQL etc

Experience: 13 + years

· Extensive experience in design and development of Databases, SQL Server, stored procedures, Indexes, Views, and Triggers.

· Database concepts, ability to write complex stored procedures, functions, triggers etc. with close view on database performance.

· Strong SQL and performance improvement background.

· Good with designing table structure considering performance in mind.

· Good understanding of building data pipelines.

· Good understanding of Snowflake and associated workflows.

Azure data basics

Job Title: Data Engineer with strong experience with Stored procedures, SQL etc.

Work Location

Location: Chicago, IL

Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):

Benefits and Perks:

Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave

The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.

Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.

LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.

Safe return to office:

In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.

Job Type: Full-time

Salary: $120,000.00 - $130,000.00 per year

Benefits:

401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Relocation assistance
Retirement plan
Vision insurance

Compensation package:

Yearly pay

Experience level:

11+ years

Schedule:

8 hour shift

Ability to commute/relocate:

Chicago, IL 60609: Reliably commute or planning to relocate before starting work (Required)

Experience:

Snowflake: 10 years (Required)
SQL: 10 years (Required)
Azure: 10 years (Required)

Work Location: In person",1997,Information Technology Support Services,$1 to $5 billion (USD),Information Technology,10000+ Employees,Company - Public,True
Application Engineer(Data Engineering),"Discover Financial Services
","Riverwoods, IL",$85K - $143K (Employer est.),3.8,"Discover. A brighter future.

With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Design, develop, test, and implement data-driven solutions to meet business requirements.
Creates and enhances data solutions that enable seamless integration and flow of data across the data ecosystem.
Designs and develops data ingestion frameworks, leveraging open-source tools and data-processing frameworks.
Identifies improvement opportunities and recommends possible technical solutions.
Provides subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Optimize the performance of ETL processes and scripts by working with other technical staff as needed.
Provide business analysis and develop ETL code and scripting to meet all technical specifications and business requirements according to the established designs.
Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.
Ability to quickly identify an opportunity and recommend possible technical solutions.
Custom Data pipeline development (Cloud and locally hosted)

Minimum Qualifications

At a minimum, here’s what we need from you:

Bachelors – Computer Science
3+ Years – Information Technology, (Software) Engineering, or related
Internal applicants only: technical proficiency rating of competent on the Dreyfus engineering scale

Preferred Qualifications:

Experience in Data Platform Administration/Engineering, or related
Knowledge of Amazon Web Services (AWS) based solutions such as Lambda, Dynamo dB, Snowflake and S3.
Knowledge of Data Warehouse technology (Linux/Teradata/Ab Initio/Python/Spark/Snowflake/NoSQL)
Knowledge of graphical database like Neo4J
Experience in migrating ETL processes (not just data) from relational warehouse Databases to AWS based solutions.
Knowledge and experience using query languages for relational databases.

#LI-CM

Compensation:

The base pay for this position generally ranges between $84,500.00 to $142,500.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position.

Benefits:

We also offer a range of benefits and programs based on eligibility. These benefits include:

Paid Parental Leave

Paid Time Off

401(k) Plan

Medical, Dental, Vision, & Health Savings Account

STD, Life, LTD and AD&D

Recognition Program

Education Assistance

Commuter Benefits

Family Support Programs

Employee Stock Purchase Plan

Learn more at MyDiscoverBenefits.com.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",1985,Banking & Lending,$1 to $5 billion (USD),Financial Services,10000+ Employees,Company - Public,False
Data Engineer (On-Site),"PrismHR
","Rolling Meadows, IL",$75K - $102K (Glassdoor est.),3.9,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.

We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:

Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
Defining streaming event data feeds required for real-time analytics and reporting
Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance

As a Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.

Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!

Responsibilities:

Build our next generation data warehouse
Build our event stream platform
Translate user requirements for reporting and analysis into actionable deliverables
Enhance automation, operation, and expansion of real-time and batch data environment
Manage numerous projects in an ever-changing work environment
Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
Build processes for topnotch security, performance, reliability, and accuracy
Provide mentorship and collaborate with fellow team members

Qualifications:

Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
3+ years of experience building data pipelines
3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation
Fluency in Scala is required
Working knowledge of Apache Spark
Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)

Nice-to-Haves:

Experience with Machine Learning
Familiarity with Looker a plus
Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)

PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners.

Diversity, Equity and Inclusion Program/Affirmative Action Plan:
We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.

Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.

As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.

The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers.

Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.

PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.

#LI-ML1

LhiowHghLS",1985,Software Development,$500 million to $1 billion (USD),Information Technology,501 to 1000 Employees,Company - Private,True
Data Engineer,"Discover Financial Services
","Riverwoods, IL",$85K - $143K (Employer est.),3.8,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
At Discover, be part of a culture where diversity, teamwork and collaboration reign. Join a company that is just as employee-focused as it is on its customers and is consistently awarded for both. We’re all about people, and our employees are why Discover is a great place to work. Be the reason we help millions of consumers build a brighter financial future and achieve yours along the way with a rewarding career.

The Data Engineer is responsible for designing, developing, testing, and maintaining complex data solutions for the product. Data Engineers play a key role in mentoring and influencing peers to achieve commitments on data solutions in a timely fashion and with an emphasis on quality. This role also has a broader influence through technical thought leadership amongst their peer tech lead community. Actively manages and escalates risk and customer-impacting issues within the day-to-day role to management.

Responsibilities
Develops and troubleshoots data integration solutions with complex data transformations and provides guidance to other team members
Influences other team members to achieve commitments per guidance from Chapter Leads and actively contributes to agile ceremonies
Demonstrates strong technical aptitude across data engineering practices:
Utilizing variety of tools to profile, secure the data in transit and at rest; and to enforce data Governance Controls and Alerting
Designing advanced SQL queries
Leveraging metadata-driven framework for solutions
Developing test scripts for unit and integration testing
Develops test methodologies for specific products
Leads code review sessions and other process and operational improvement initiatives
Exhibits fluency with use of supplemental tools and technologies involved in data integration (Unix/Linux, TWS/Control-M or alike, BI stack)
Works on holistic solutions, driving feature and story delivery (Agile)
Identifies and effectively communicates upstream and downstream impacts for changes in the data pipeline
Participates in the on-call rotation for support
Demonstrates effective and clear communication in team and cross-functional meetings, and lead tech communities
Builds strong collaborative working relationship both within the team and cross-functionally

Minimum Qualifications

At a minimum, here’s what we need from you:
Bachelor's Degree in Computer Science or related field
3+ years of experience in Data Platform Administration/Engineering
Internal applicants only: technical proficiency rating of competent on the Dreyfus engineering scale

Preferred Qualifications

If we had our say, we’d also look for:
ETL/ELT Tools (AbInitio, DataStage, Informatica)
Cloud Tools and Databases (AWS, Snowflake)
Other programming languages (Unix scripting, Python, etc.)
Leverage CI/CD framework for data integration, Open Source
Experience working in cloud platforms (AWS, GCP, Azure)
Basic understanding of key infrastructure concepts (data centers as well as cloud hosting platform) to support business data needs
Experience optimizing SQL both relational and nosql
Multichannel consumer marketing and journey orchestration

External applicants will be required to perform a technical interview.

#LI-KE

Compensation: The base pay for this position generally ranges between $84,500.00 to $142,500.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position.

Benefits:
We also offer a range of benefits and programs based on eligibility. These benefits include:

Paid Parental Leave

Paid Time Off

401(k) Plan

Medical, Dental, Vision, & Health Savings Account

STD, Life, LTD and AD&D

Recognition Program

Education Assistance

Commuter Benefits

Family Support Programs

Employee Stock Purchase Plan

Learn more at MyDiscoverBenefits.com .

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",1985,Banking & Lending,$1 to $5 billion (USD),Financial Services,10000+ Employees,Company - Public,False
Manufacturing Data Engineer,"ABBVIE
",United States,-1,3.9,"This role is responsible to elicit, understand and document packaging project objectives and requirements, gathering information from diverse stakeholders, evaluating output deliverables and formats. Contributing to the design, implementation and adoption of business solutions is a foundational contribution of this role. Applies relevant expertise to complete tasks which support projects, solution requests, incident, and problem resolution. Provides solutions to the functional teams with a global perspective to ensure systems are integrated to the business and other platforms. Discovers and evaluates business needs to recommend, develop and implement solutions to bring value and continuous improvement to the business.

In this role you’ll be responsible for:

Responsible for compliance with applicable Corporate and Local Policies and procedures. This includes validation, change control, CAPA and other cGXP policies and procedures.
Uses appropriate tools to collect, correlate and analyze data; records and maintains technical data for use in developing operating and instruction manuals. Conducts tests; develops preliminary findings for review by experienced analysts. Assists in the development of standards and procedures. Prepares written reports of work and develops metrics to measure systems health and performance. Applies and executes standard information systems theories, concepts and techniques. Conducts training.
Determines approach and solution to problem when issue is within scope of own ability. Develops solutions to problems where casual relationships can be identified, and precedents exist. Brings together appropriate stakeholders both internal and external to develop and implement solutions while providing leadership during the resolution process. Work is performed and technical decisions are made with a high degree of independence and performance is judged by results obtained.
Applies analytical thinking skills to diagnose and make recommendations on systems. Incorporates knowledge of business and technology to make recommendations. Translates data gathered from user discussions into business requirements. Identifies business/system needs and recommends solutions. Assists with the testing strategies and develops preliminary findings.
Analyzes departmental processes and needs and makes recommendations that are most effective means to satisfy those needs. Analyses the most complex work procedures and job methods and presents concepts to simplify those procedures.
Works effectively with internal IT team, external IT teams, business functions and external vendors and partners to accomplish organizational goals. Actively participates in client or team meetings by offering suggestions. Provides leadership to project planning.
Monitors and organizes the efforts of technical and business support staff. Manages projects according to milestones and completes tasks assigned by more experienced analysts and managers. Guides the efforts of less experienced staff.
Exercises latitude in approach to problem and solution. Work is reviewed for soundness of business judgement and to determine if overall results and objectives have been achieved. Errors or failure to achieve expected results may result in project delays and/or the expenditure of more resources than was planned. Regular assignments are completed with minimal supervision.
Analyzes problems or system changes and determines impact to the business processes supported, global system interfaces and other systems supporting the business operations. This includes impact of any change in the systems over their state of validation, their performance and their ability to continue to meet the business requirements. This is performed in a fast and accurate way to avoid a negative impact on business operations.
Provides direct support to the assigned systems. Applies relevant expertise to complete tasks that support projects, solution requests, incident, and problem resolution. Provides solutions to the functional teams with a global perspective to ensure systems are integrated to the business and other platforms. Discovers and evaluates business needs to recommend, develop and implement solutions to bring value and continuous improvement to the business. Adapt work schedule to support the business operations. This includes maintenance works, replacements, upgrades, migrations and other changes to systems in production during off-peek hours. This might require working overtime, overnight or on weekends and holidays and to be on call during the off hours to attend any emergency or critical problem that may arise with the systems in the area of responsibility. Work may be required inside a pharmaceutical products manufacturing facility that require special restriction conditions. Systems used to support the business operations must be supported 7 days a week and 24 hours a day continuously.




Experiences that make you a strong candidate for this role:

Required:

Bachelors degree in relevant field and at least 5 years relevant experience OR Masters degree in relevant field and at least 4 years experience.

Beneficial:

Experience with manufacturing execution systems development, integration, and support
Knowledge and experience with all phases of SDLC in a cGXP environment
Technical expert who will independently identify, engineer, and optimize new data, software, or technology solutions that address critical business needs.
Continuously seek to improve existing methods and processes. Read and adapt literature and available information to accomplish assignments.
Skilled with shop floor devices infrastructure application integration support (PC’s, printers, barcode scanners, serial devices, manufacturing testing equipment)
Demonstrate proficiency in a broad range of techniques and methods for information technology engineering, including software development, data warehousing, statistics, machine learning, and/or technology infrastructure (including servers, storage, network connectivity, and virtualization/cloud environments).
Technology Engineers will be aligned to a specific area: Information, Software, Data, or Infrastructure.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

If you believe you’re a great fit for this job but don’t have all of the experiences listed above, we encourage you to apply anyway!
Why Business Technology Solutions
For anyone who wants to use technology and data to make a difference in people’s lives, shape the digital transformation of a leading
biopharmaceutical company, and secure sustainable career growth within a diverse, global team: we’re ready for you.





AbbVie is committed to operating with integrity, driving innovation, transforming lives, serving our community, and embracing diversity and inclusion. It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a protected veteran, or any other legally protected group status.",2013,Biotech & Pharmaceuticals,$10+ billion (USD),Pharmaceutical & Biotechnology,10000+ Employees,Company - Public,False
Data Engineer - NCSA,"Universtiy of Illinois Urbana-Champaign
","Urbana, IL",$75K - $106K (Glassdoor est.),4.1,"DATA ENGINEER/SENIOR DATA ENGINEER
The National Center for Supercomputing Applications

The National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign provides supercomputing and advanced digital resources for the nation's scientific enterprise. At NCSA, University of Illinois faculty, staff, students, and collaborators from around the globe use advanced digital resources to address and research grand challenges for the benefit of science and society. NCSA has been advancing one third of Fortune 50 companies for more than 30 years by bringing industry, researchers and students together to solve grand challenges at rapid speed and scale.

The National Center for Supercomputing Applications (NCSA) is currently seeking a Data Engineer and/or a Senior Data Engineer to join NCSA. The selected candidate will serve as expert, data-focused liaison to collaborate with institutions and partners by partnering in NCSA's mission to address complex research problems in science and society through the development and application of tailored, comprehensive digital environments. They will also collaborate with researchers and data scientists to answer challenging, heterogeneous research questions, deliver targeted solutions through innovative data engineering and research, design, implement and deploy data and database services at NCSA and collaborating institutions to advance academic, scientific, and industrial research.

NCSA is committed to increasing the diversity of the campus community. Candidates who have experience working with a diverse range of faculty, staff, and students, and who can contribute to the climate of inclusivity are encouraged to apply.


Job Summary

Serve as expert, data-focused liaison to collaborate with institutions and partners. Research, design, implement and deploy data and database services at NCSA and collaborating institutions to advance academic, scientific, and industrial research. Partner in NCSA's mission to address complex research problems in science and society through the development and application of tailored, comprehensive digital environments. Collaborate with researchers and data scientists to answer challenging, heterogeneous research questions. Deliver targeted solutions through innovative data engineering.

Duties & Responsibilities

Research Data Architecture:
• Plan, design, implement, and deploy data services and infrastructure used to: Transform data into actionable information for researchers and data scientists. Enable end-to-end data flows for research and industry partner systems. Curate, monitor, record, and assess the quality of data generated by NCSA-developed data processing pipelines. • Implement services for research and clinical healthcare data. • Design, development, and programming of cloud-based data infrastructure. • Facilitate interactions with data systems and databases through: Exploratory data analysis and first-line data analytics. Pipeline design and development. ETL process implementation and deployment. Data API design and deployment. SQL and programmatic query development. • Sustain and maintain daily operations of data services and databases, including capacity planning, backups and recovery, performance and tuning, and data security. • Provide ad hoc queries as needed, and assist users and application projects in developing queries, often in large and complex database schemas. • Prepare and present technical recommendations regarding data architecture, implementation, and operations. • Contribute to academic publications, technical reports, proposals, and documentation • Communicate findings to primary investigators and project colleagues at academic and industry conferences.
Problem Solving and Analytical Requirements:
• Understand the role of chosen data service technology in the overall context of a project’s architecture. • Plan and execute incremental changes of data and database systems, especially as a project adapts to growing dataset size and increased throughput and access demands. • Analyze and solve performance and dataset integration problems in a complex data architecture. • Plan and integrate data systems in keeping with NCSA’s cyberinfrastructure policies and procedures

Minimum Qualifications

BA or BS degree in computer science, information science, engineering or related science field required. Alternative degree fields will be considered if accompanied by equivalent experience (depending on nature and depth of experience as it relates to current NCSA business needs). 1 years’ experience with transactional databases, analytical databases, data warehouses, or NoSQL systems. Data architecture and data modeling experience. Programming skills and experience appropriate for automating database operations

Preferred Qualifications

Experience working in a Linux environment.
Familiarity with relational database modeling.

Knowledge, Skills and Abilities

Excellent interpersonal skills.
Communicates clearly with all types of audiences
Ability to work in a distributed team setting, with shared responsibilities.
Work independently, once given direction.
Ability to work on multiple projects simultaneously
Appointment Information

This is a 100% full-time Academic Professional position, appointed on a 12-month basis. The expected start date is as soon as possible after 12/16/2023. Salary is commensurate with experience.

Application Procedures & Deadline Information

Applications must be received by 6:00 pm (CST) on December 6, 2023. Apply for this position using the Apply Now button at the top or bottom of this posting. Applications not submitted through https://jobs.illinois.edu will not be considered. For further information about this specific position, please contact Jewel Goodly at jgoodly@illinois.edu. For questions regarding the application process, please contact 217-333-2137.

The University of Illinois System is an equal opportunity employer, including but not limited to disability and/or veteran status, and complies with all applicable state and federal employment mandates. Please visit Required Employment Notices and Posters to view our non-discrimination statement and find additional information about required background checks, sexual harassment/misconduct disclosures, COVID-19 vaccination requirement, and employment eligibility review through E-Verify.

Applicants with disabilities are encouraged to apply and may request a reasonable accommodation under the Americans with Disabilities Act (2008) to complete the application and/or interview process. Requests may be submitted through the reasonable accommodations portal, or by contacting the Accessibility & Accommodations Division of the Office for Access and Equity at 217-333-0885, or by emailing accessibility@illinois.edu.

Requisition ID: 1021141
Job Category: Research
Apply at: https://jobs.illinois.edu",-1,Colleges & Universities,$1 to $5 billion (USD),Education,10000+ Employees,College / University,False
Data Engineer,"WALGREENS
","Deerfield, IL",$85K - $120K (Glassdoor est.),3.1,"Job Summary:

Builds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies valuable internal and external data. Collaborates closely with data scientists to define data for the design, development, and deployment of new solutions that support business priorities.

Job Responsibilities:
Develops software that processes, stores and serves data for use by others.
Develops data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Develops data pipelines that are scalable, repeatable and secure.
Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Effectively resolves problems and roadblocks as they occur.
Interacts with internal and external peers and/or managers to exchange semi-complex information related to assigned activities.
About Walgreens and WBA

Walgreens (www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance, Inc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for communities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations across America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination serving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system by providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for the nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a true omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to deliver high quality products and services in communities nationwide.
Basic Qualifications

Bachelor's degree and at least 2 years of experience in data engineering; OR Graduate Degree in a technical discipline.
Advanced knowledge of SQL
Experience establishing and maintaining key relationships with internal (peers, business partners and leadership) and external (business community, clients and vendors) within a matrix organization to ensure quality standards for service.
Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement of results and find solutions.
Willing to travel up to 10% of the time for business purposes (within state and out of state).

Preferred Qualifications

Graduate Degree in a technical discipline
Experience with REST API development
Experience with Azure application deployment
Experience in Azure technologies like Azure Data Factory, Azure Databricks using Python or Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic App
Experience in building Azure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.",1901,Drug & Health Stores,$10+ billion (USD),Retail & Wholesale,10000+ Employees,Company - Public,False
People Data Engineer,"EquipmentShare
","Chicago, IL",$83K - $119K (Glassdoor est.),3.9,"EquipmentShare is Hiring a Data Engineer.

EquipmentShare is searching for a Data Engineer to work fully remotely.

EquipmentShare is seeking a Data Engineer to be responsible for how we manage our Human Resources data

Primary Responsibilities
Establish sound pipelines and clear table structures for a wide variety of metrics used by the People Analytics team, with particular emphasis on ensuring data security and anonymity, and helping to ensure compliance with all legal requirements for this data
Help shape best practices within our data engineering teams more broadly, along with mentoring and developing junior data engineers
Manage AWS and Snowflake resources and databases
Collaborate with other data engineers as well as be the direct point of contact for the people analytics team for data ingestion, transformation, and automation.
Why We're a Better Place to Work

Competitive salary.

Medical, Dental and Vision coverage for full-time employees.

401(k) and company match.

Unlimited Paid time off (PTO) plus company paid holidays.

Stocked breakroom and full kitchen, chef prepared meals daily (breakfast and lunch).

State of the art onsite gym (Corporate HQ) with instructor led-courses/Gym stipend for remote employees.

Seasonal and year round wellness challenges.

Company sponsored events (annual family gatherings, happy hours and more).

Volunteering and local charity initiatives that help you nurture and grow the communities you call home. Employees receive 16 hours of paid volunteer time per year.

Opportunities for career and professional development with conferences, events, seminars and continued education.

About You

Our mission to change an entire industry is not easily achieved, so we only hire people who are inspired by the goal and up for the challenge. In turn, our employees have every opportunity to grow with us, achieve personal and professional success and enjoy making a tangible difference in an industry that's long been resistant to change.

Skills & Qualifications
Degree, or equivalent practical experience, in computer science, machine learning, software engineering, statistics or related field
3+ years working on technology-powered products as either a data platform engineer, data engineer, ML engineer, analytics engineer, software developer or a closely related role
Experience working with HR data, or other sensitive data types.
Experience with Workday is a plus
Demonstrated understanding of business analytics, data modeling, and warehousing

EquipmentShare is committed to a diverse and inclusive workplace. EquipmentShare is an equal opportunity
employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation,
protected veteran status, disability, age, or other legally protected status.




#LI-Remote",2014,Commercial Equipment Services,Unknown / Non-Applicable,"Construction, Repair & Maintenance Services",1001 to 5000 Employees,Company - Private,True
Azure Data Engineer,"Infocodec Solutions
","North Chicago, IL",$83K - $191K (Employer est.),4.9,"Role: Azure Data Engineer

New skill set required for GRS modularization and cloud migration: Expertise in Azure cloud, middle tier, back-end, SQL/Synapse

Job Type: Contract

Salary: $83,436.31 - $190,876.31 per year

Experience level:

8 years

Schedule:

Monday to Friday

Ability to commute/relocate:

North Chicago, IL 60064: Reliably commute or planning to relocate before starting work (Required)

Application Question(s):

Work on W2 Role? Must answer??

Experience:

Azure Cloud: 7 years (Required)
Middle Tier: 6 years (Required)
SQL/ Synapse: 7 years (Required)
Back-end development: 6 years (Required)

Work Location: In person",2015,Information Technology Support Services,Unknown / Non-Applicable,Information Technology,51 to 200 Employees,Company - Private,True
Data Engineer,"Sourcebooks
","Naperville, IL",$82K - $121K (Glassdoor est.),2.3,"Are you a dynamic and versatile Data Engineer ready to make an impact on every aspect of the data lifecycle? We're actively seeking a talented professional to shape the future of our data-driven ecosystem.

In this role, you'll take the lead in:

Designing Scalable Data Architectures:

Craft efficient and scalable data architectures to meet evolving business needs.

Optimizing ETL Processes:

Develop and optimize ETL processes for seamless and efficient data movement.
Implement data cleansing and validation procedures to ensure data accuracy and consistency.

Robust Database Administration Practices:

Implement routine maintenance tasks, ensuring data security, and monitoring performance.
Install, configure and upgrade database systems as needed.
Perform routine database maintenance tasks, including backups and restores.

Collaborating Across Teams:

Work closely with data scientists, analysts, and development teams to deliver effective data solutions.

Ensuring Data quality and Governance:

Enforce data governance policies and standards to maintain high-quality.

Responsibilities will span designing cutting-edge data solutions to resolving complex data challenges, offering you a unique opportunity to shape our organization's data landscape.

Requirements

Bachelor’s degree and minimum 3 years of experience in a technical field.
Understanding of relational and dimensional data modeling.
Data lake and data warehouse experience.
Experience with Power BI.
Impeccable attention to detail.
Extensive experience with database technologies (MySQL, MS SQL).
Strong organizational and analytical skills.
Other duties as assigned.
Assist personnel of other departments as a technical resource.

There is the opportunity to work onsite/hybrid in Naperville, IL or New York, NY or 100% remotely from the following states ONLY: New York, Illinois, Connecticut, Arizona, Tennessee, Texas, Wisconsin, New Jersey, Minnesota, Maryland, Nevada, Pennsylvania or Washington. This is a full-time, permanent position and you must be authorized to work in the U.S.

Full-time employees are eligible for our comprehensive benefits program. Our range of benefits include, but are not limited to, Medical/Prescription drug insurance, Dental, Vision, Health Care/Dependent Care Flexible Spending Account, Health Savings Account 401(k), Short and Long-Term Disability Insurance, Life/AD&D Insurance, & generous paid time off.

We value the array of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

Sourcebooks is a fast-growing entrepreneurial company seeking impact players and difference makers. We create books that transcend categories and defy odds; with hundreds of national bestsellers and awards to show for it. We are a company of enthusiastic, book-loving employees who are dedicated to connecting books to readers in new and innovative ways. Every day, we are transforming the publishing industry with bold thinking and unprecedented results. Story by story, book by book, we have touched over 100 million lives. Join us as we strive to change 100 million more.

Sourcebooks, Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.

Sourcebooks is a thriving entrepreneurial company that brings extraordinary authors to readers in the most dynamic, data-driven ways. We create books that transcend categories and defy odds, and we have been honored with hundreds of national bestsellers and awards. We are a company of enthusiastic booklovers passionate about connecting books to readers in new and innovative ways. Story by story, book by book, we have changed more than 100 million lives. Join us as we change 100 million more.

kD0ybJ7qqS",1987,Publishing,$25 to $100 million (USD),Media & Communication,51 to 200 Employees,Company - Private,False
Data Engineer - SQL/SSRS/PowerBI,"Michael Page
","Chicago, IL",$100K - $115K (Employer est.),4.4,"Must be able to prioritize multiple projects, while meeting deadlines and quality expectations.
Must be able to adapt quickly in a changing environment.
Has strong interpersonal communication skills working with both internal and external customers.
Ability to relate or explain technical information to non-industry personnel.
Adept at working with cross-functional IT teams to meet customer objectives.




MPI does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, disability, veteran status, marital status, or based on an individual's status in any group or class protected by applicable federal, state or local law. MPI encourages applications from minorities, women, the disabled, protected veterans and all other qualified applicants.

Lives in the Chicagoland area
Bachelor's Degree or equivalent experience in Information Systems, Information Technology, Computer Science, Informatics or a related technology field.
3+ years of experience in data warehouse environments.
3+ years' experience reporting tools including SSRS and Crystal Reports
3+ years' experience with BI tools such as Power BI or Tableau
Excellent communication skills
Loves building things and working in a team.
Eager to move into a lead role with rapid expected company growth.
Dependable individual who can take initiative on cleaning up and expanind projects.
Experience with the following will be viewed as a plus ( TMS, WMS, SQL/Oracle server, Data Democratization, Machine learning models in Power BI)

My client is 3PL logistics firm that has been around since the early 1980s and quickly established itself as a major player in the industry with massive growth potential. They are an asset based company, owning their own fleet of trucks, operating their own warehouses and develop their very own cutting edge proprietary logistics technologies.

Salary based on experience and interviews
Expected company growth
Growth in individual role in short and long-term
Innovative projects
Autonomy on projects
Great culture focused on building on self-driven team
Generous PTO",-1,Security & Protective,Unknown / Non-Applicable,Management & Consulting,201 to 500 Employees,Company - Private,False
Test Data Management Engineer,"Iver Solutions, LLC
","Riverwoods, IL",$70.00 - $75.00 Per Hour (Employer est.),3.1,"IBM/DFS (Discover Financial Services)

Test Data Management Engineer

Location: Riverwoods, Illinois (Onsite)

Skills / Experience Required :

Technical solid knowledge of Optim
Competent understanding of Test Data Management related to data security, data masking, synthetic data creation and test data strategy planning.
Experience on DB migration/consolidation projects
Competent in TDM functions like data masking, data provisioning, sub setting, profiling, data mining, data handling, data management
Collaborates with subject matter experts and data stewards to identify PII definitions, data strategies, policies, controls, and programs to ensure enterprise data is accurate, complete, secure, and reliable.
Reviewing data needs and driving data solutioning decisions. Ability to build data masking configurations in the data masking tools. Provide mentoring to Test Data Analysts as needed.
Exceptional analytic and technical skills to innovate, build, and maintain well-managed data solutions and capabilities to tackle business problems.
Excellent spoken and written communication to explain the methods to a technical and non-technical audience.
The ability to think laterally and ‘outside the box.’
Teamwork skills, to support colleagues and share techniques.
Exceptional analytical and problem-solving skills and the persistence to apply different techniques to get the job done.

Job Type: Contract

Pay: $70.00 - $75.00 per hour

Schedule:

Monday to Friday

Work Location: In person",-1,-1,Unknown / Non-Applicable,-1,Unknown,Company - Private,True
Data Engineer,"Mitsubishi HC Capital America Inc
","Itasca, IL",$97K - $112K (Employer est.),3.1,"For this role, we will consider candidates located near our Norwalk, CT office or Itasca, IL office.

Position Overview:

The Data Engineer holds the primary responsibility for skillfully designing, developing, implementing, and supporting Mitsubishi HC Capital America, Inc. (MHCCNA)'s enterprise Microsoft Azure Data Warehouse. This role assumes accountability for ensuring the dependable, efficient, and secure operation and advancement of MHCCNA's On-Premise and Azure Synapse Data Warehouses to effectively meet crucial business needs.


Commitment to Internal Control:

The Data Engineer is required to possess a comprehensive understanding of and adhere to the system of internal controls associated with the fundamental duties and responsibilities of the role. This includes compliance with SOX and all other pertinent regulatory and compliance policies and requirements.


Essential Duties and Responsibilities:

The responsibility of Data Engineer encompasses the entire lifecycle of the Data Warehouse environments that underpin the vital business requirements of MHCCNA. This includes the design, development, implementation, operation, and ongoing support of these critical systems.

The individual in this position is tasked with developing a flexible, enterprise-level environment that integrates multiple warehouses to guarantee precise, comprehensive, consistent, and timely data. Their primary goal is to create a cohesive system that fulfills these demands while catering to diverse business requirements.

The role necessitates the capacity to explore and grasp emerging technologies while collaborating closely with peer teams to establish strategic roadmaps and priorities. The ability to swiftly acquire and proficiently apply hands-on administration skills is essential. As a Subject Matter Expert in technical requirements, this position will play a crucial role in supporting and implementing data projects, as well as engaging effectively with users and other IT staff.

The Data Engineer responsibilities include:
Provide support in designing and overseeing enterprise-grade data pipelines and data stores.
Implement automation and streamline processes to optimize the entire data and analytics platform, ensuring efficient throughput and high-performance outcomes.
Recognize, devise, and execute internal process enhancements, including automation of manual tasks, optimizing data delivery, and redesigning architecture or infrastructure to enhance scalability.
Collate large, intricate datasets that align with functional and non-functional business demands.
Develop processes that facilitate data transformation, manage data structures, metadata, dependencies, and workload management.
Collaborate with business users to understand functional and data requirements, contributing to the enhancement of data models and pipelines.
Apply analytical and problem-solving skills to diagnose and resolve intricate technical issues.
Create, maintain, and continuously enhance scalable data pipelines, while also developing new data source integrations to accommodate the growing volume and complexity of data.
Designing, implementing, and managing data extraction, transformation, and loading (ETL) processes.
Creating comprehensive technical specification documents and application interface designs.
Creating data processing and integration solutions for both batch and real-time scenarios, proficiently handling structured and unstructured data.
Participate in design discussions, code reviews, and project-related team meetings.
Ensuring data security and compliance with relevant regulations and best practices in all data operations.
Troubleshooting and resolving data and system issues, stepping in when necessary to address outages and challenges.
Other duties and responsibilities as assigned or needed.


KPI’s (Key Performance Indicators):

Deliver Business Intelligence solutions that are 95% defect-free providing that adequate written business requirements, development time, and business test review were afforded during the project. This standard does not apply to legacy remediation efforts or ready to serve emergency production response activities.
Effectively utilize consulting resources on all significant projects (over 40 hours) to allow for development power of scale. Consultants should do lower value work that is considered heavy lift, freeing up programmer analysts to spend more time in analysis and design while maintaining tight control over quality, code, and company intellectual property.
These are overarching KPI metrics that are applicable to all goals that are defined over the course of the business year.

Responsibility and Decision-Making Authority:
Exercise independent judgment and decision-making while adhering to Company Policy.


Management/Supervisory Responsibilities:

N/A


Qualifications/Competencies:

Key Technical Knowledge, Skills, and Abilities:
Must have experience deploying modern data solutions leveraging components like Azure functions, Azure Synapse, Azure Data Factory, Data Flows, Azure Data Lake, Azure SQL.
Strong level of understanding on Azure Synapse, ADLS, and Azure DevOps.
Exhibit an understanding of Data Lake architectures, including raw, enriched, and curated layer concepts, and ETL/ELT operations.
Exhibit a solid understanding of database design, data warehousing concepts, big data platforms, and ETL operations.
Experience working with data integration techniques & self-service data preparation.
Experience in requirements analysis, design, and prototyping.
Experience with DevOps tools like Azure DevOps, Jenkins, Maven etc.
Experience in building/operating/maintaining fault tolerant and scalable data processing integrations.
Demonstrated experience of turning business use cases and requirements into technical solutions.
Ability to conduct data profiling, cataloging, and mappings for technical design and construction of data flows.
Strong collaboration and experience working with remote teams.
Strong problem-solving skills with emphasis on optimization data pipelines.
Showcase excellent communication and presentation skills for effective collaboration with technical and non-technical stakeholders.
Strong analytical skills and a drive to learn and master new technologies and techniques.
Experience working with third party providers and vendors for critical support requirements.

Competencies:
Possesses the professional or technical skills required to effectively assume job responsibilities and perform tasks.
Conscientiously attends to detail in order to produce precise and error-free work.
Able to identify and analyze a problem, evaluate possible solutions, and select the most suitable one.


Education and Experience:
Demonstrated expertise in Microsoft Azure development.
3-5 years of hands-on Data Warehouse architecture and development experience within the Microsoft Azure environment.
Bachelor’s degree with a minimum of 3-5 years of related work experience outside of educational studies.

Working Hours / Travel Requirements:

Hours may vary and will require periodic overtime, including occasional evening and weekends, depending on business needs.
On call 24x7 for emergency support
Occasional travel for business meetings, seminars or training may be required.

Physical Demands:
Digital dexterity and hand/eye coordination in operation of office equipment.
Light lifting and carrying of supplies, files, etc.
Ability to speak to and hear customers and/or other employees via phone, in-person or virtually.
Body motor skills sufficient to enable incumbent to move from one office location to another.

The job description does not constitute an employment contract, implied or otherwise, other than an “at will” relationship and is subject to change by the employer as the needs of the employer and requirements of the job change.


Salary Range: ($97,400 to $111,700) per year, plus a discretionary Bonus.

The salary range is determined and based on internal equity, market data/ranges, applicant's skills, prior relevant experience, and education.


Additional Benefits:

Medical, Dental and Vision Plans
401(k) and matching
Generous Paid Time Off
Company paid Life Insurance
Employee assistance program
Training and Development Opportunities
Employee discounts",1952,Banking & Lending,Unknown / Non-Applicable,Financial Services,51 to 200 Employees,Company - Private,True
Data Engineer - 5050304,"Accenture
","Chicago, IL",-1,3.9,"Accenture Flex offers you the flexibility of local fixed-duration project-based work powered by Accenture, a leading global professional services company. Accenture is consistently recognized on FORTUNE's 100 Best Companies to Work For and Diversity Inc's Top 50 Companies For Diversity lists.

As an Accenture Flex employee, you will apply your skills and experience to help drive business transformation for leading organizations and communities. In addition to delivering innovative solutions for Accenture's clients, you will work with a highly skilled, diverse network of people across Accenture businesses who are using the latest emerging technologies to address today's biggest business challenges.

You will receive competitive rewards and access to benefits programs and world-class learning resources. Accenture Flex employees work in their local metro area onsite at the project, significantly reducing and/or eliminating the demands to travel.

Utilize strong SQL Python skills to engineer sound data pipelines and conduct routine and ad hoc analysis to assess the performance of legacy products and the saliency of new features.

Build reporting dashboards and visualizations to design, create and track campaign program KPIs.

Perform analyses on large data sets to understand drivers of operational efficiency.

Manage end to end process of analytic tooling feature development, including request intake, requirements evaluation, cross functional team alignment, feature execution, QA testing, and stakeholder communication.

Consult with business analysts, technical data SMEs, and cross functional partners to understand their reporting and data needs, serving as the point of contact for requests, inquiries, and actions.

Interface with other data engineering, product, and data science teams to implement client needs and initiatives.




Basic Qualifications:

Minimum 2 years experience with Python.

Minimum 3 years experience SQL.

Minimum 3 years experience Big Data Analytics.

High School Diploma or GED.

Preferred Qualifications:

Experience with large enterprise data sets.

Bachelors Degree

Compensation at Accenture varies depending on a wide array of factors, which may include but are not limited to the specific office location, role, skill set and level of experience. As required by local law, Accenture provides a reasonable range of compensation for roles that may be hired in California, Colorado, New York, or Washington as set forth below.

Information on benefits is here.

Role Location

California: $61.53 to $71.53/hour

Colorado: $61.53 to $71.53/hour

New York: $61.53 to $71.53/hour

Washington: $61.53 to $71.53/hour


What We Believe


We have an unwavering commitment to diversity with the aim that every one of our people has a full sense of belonging within our organization. As a business imperative, every person at Accenture has the responsibility to create and sustain an inclusive environment.


Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and more creative, which helps us better serve our clients and our communities. Read more here


Equal Employment Opportunity Statement


Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation.


All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.


Accenture is committed to providing veteran employment opportunities to our service men and women.


For details, view a copy of the Accenture Equal Employment Opportunity and Affirmative Action Policy Statement.


Requesting An Accommodation


Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired.


If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter.


Other Employment Statements


Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.


Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.


Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.


The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information.",1989,Business Consulting,$10+ billion (USD),Management & Consulting,10000+ Employees,Company - Public,False
Test Data Engineer (OPTIM) (W2 ROLE ONLY),Cybotic System,"Riverwoods, IL",$70.00 - $80.00 Per Hour (Employer est.),-1.0,"JOB DESCRIPTION:

(ITS A CONTARCT W2 ROLE FOR USC, GC AND EAD CANDIDATES)

Technical solid knowledge of Optim Competent understanding of Test Data Management related to data security, data masking, synthetic data creation and test data strategy planning. Experience on DB migration/consolidation projects Competent in TDM functions like data masking, data provisioning, sub setting, profiling, data mining, data handling, data management Collaborates with subject matter experts and data stewards to identify PII definitions, data strategies, policies, controls, and programs to ensure enterprise data is accurate, complete, secure, and reliable. Reviewing data needs and driving data solutioning decisions. Ability to build data masking configurations in the data masking tools. Provide mentoring to Test Data Analysts as needed. Exceptional analytic and technical skills to innovate, build, and maintain well-managed data solutions and capabilities to tackle business problems. Excellent spoken and written communication to explain the methods to a technical and non-technical audience. The ability to think laterally and ‘outside the box.’ Teamwork skills, to support colleagues and share techniques. Exceptional analytical and problem-solving skills and the persistence to apply different techniques to get the job done.

Job Type: Contract

Pay: $70.00 - $80.00 per hour

Experience level:

5 years

Schedule:

8 hour shift

Experience:

OPTIM: 1 year (Required)
Test management tools: 5 years (Required)

Work Location: On the road",-1,-1,-1,-1,-1,-1,True
Data Engineer,"MedSpeed
","Elmhurst, IL",-1,3.0,"Data Engineer

Qualified candidates are comfortable with a hybrid schedule of being in-office 1x/week on Thursdays. They also must reside in the Chicagoland area and require no visa sponsorship.


About Us

Come join MedSpeed to help us deliver health! MedSpeed is a healthcare logistics company that partners with healthcare organizations throughout your communities to transport a wide range of medical supplies, specimens, and materials. At MedSpeed we work as a team, keep our promises and strive to get better every day. We are looking for individuals who believe in and represent those values.

Today, we have over 150 locations in 32 states but still have kept that small business, entrepreneurial feel and remain committed to the same culture established day 1!

Our people are at the heart of what we do and how we support our customers.

Why become a MedSpeeder? Take a look at what MedSpeed offers:

Medical, Dental, Vision and Flexible Spending Account - We offer plans that help you and your family take care of your whole self.
401(K) with Company Match - Helping you make good financial decisions today and for the future.
Paid Time Off - We value well-being and encourage work life balance.
Opportunities for Career Advancement – Over 50% of our market managers have been promoted into their roles.
Training Provided – Our Blue Shirt Certified program ensures you excel in your role.

The Data Engineer will help MedSpeed use our data to drive our business forward. This role will be responsible for enhancing and adding functionality to our enterprise data warehouse and will help us turn our data into insight and action. This will also include being responsible for designing, developing, and maintaining data pipelines, ETL processes, and data warehousing solutions. Expertise in Python, Snowflake, data modeling, and data engineering, along with experience in data visualization tools like Tableau, will be crucial in ensuring the reliability and scalability of MedSpeed’s data infrastructure.

What you will be doing as a Data Engineer at MedSpeed:

Develop and maintain efficient data pipelines to extract, transform, and load data from various sources into our data warehouse
Design and implement ETL processes to cleanse, transform, and aggregate data to ensure high data quality and consistency.
Manage and optimize data warehousing solutions, ensuring data is stored and organized for easy access and analysis.
Create and maintain data models that meet the business requirements and ensure data integrity.
Maintain, troubleshoot, and work to improve robustness of our data pipelines.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Collaborate with Analytics team to ensure that data pipelines meet business needs.
Work with stakeholders to address data related issues and support data needs.
Create tools for the analyst and field ops teams to enable them to effectively use our data to optimize our business, provide clarity, and improve efficiency.
Work closely with the development team as a subject matter expert in the area of data and assist with development initiatives.
Present work product results via agile ceremonies and participate in technical knowledge sharing sessions.
Maintain thorough documentation of data engineering processes, best practices, and system architecture.

What you will need to become a Data Engineer at MedSpeed:

Degree in Computer Science, Statistics, Mathematics, Business Analytics or related field
3-5 years of hands-on experience working with Snowflake, Python, data modeling, ETL processes, and data integration.
Experience with Salesforce
Proficiency in ETL tools for data extraction, transformation, and loading.
Strong proficiency in SQL and knowledge of Snowflake-specific SQL features. Ability to understand and author complex SQL queries, stored procedures, and functions.
Understanding of Agile software development methodology.
Experience with Integrate.IO
Experience with data modeling and data warehousing principles.
Experience with cloud computing platforms such as AWS or Azure.
Excellent communication and collaboration skills.
Strong analytical and problem-solving skills.
Familiar with continuous integration environments (e.g. Azure DevOps, Git, etc.).

#INDP

#LI-Hybrid",-1,Taxi & Car Services,$25 to $100 million (USD),Transportation & Logistics,201 to 500 Employees,Company - Private,True
Intermediate Data Engineer,"ampliFI Loyalty Solutions
","Naperville, IL",$80K - $106K (Glassdoor est.),4.2,"About ampliFI Loyalty Solutions
ampliFI provides fully outsourced, customized credit and debit card loyalty programs exclusively focused on banks and credit unions nationwide. For almost two decades, we have delivered compelling rewards programs, unique earn and burn opportunities and card-linked programs to leverage merchant funded offers. Here at ampliFI, we are always looking for more great people to be a part of the relentless pursuit of excellence in everything we do. Our core values are Integrity, Curiosity, and Advocacy for our clients. We are looking for a Intermediate Data Engineer (Level 2). Local candidates with a hybrid schedule are strongly preferred. However, well qualified, fully remote candidates outside the area may be considered.
Summary
As an Intermediate Data Engineer, you are familiar with the data warehousing technical components, infrastructure, and their integration. You’ll analyze large amounts of data, discover and solve real world problems. You love the idea of being able to provide insight as well as presenting those insights. You are responsible for high level design/architecture. You are comfortable fostering relationships with internal business partners and other members of the development team.
Key Responsibilities:
Design, develop, and maintain modular code base to solve “real” world problems.
Conduct regular peer code reviews to ensure code quality and compliance following best practices in the industry.
Work in cross-disciplinary teams to understand client needs and ingest rich data sources.
Research, experiment, and utilize leading Big Data technologies in AWS.
Help drive the process for pursuing innovations, target solutions, and extendable platforms for ampliFI’s products.
Participate in developing and presenting thought leadership and assist in ensuring that ampliFI’s “data source” technology stack incorporates and is optimized for using specific technologies.
Education and Experience:
3+ years of professional work experience in Data Engineering using big data methodologies with multiple programming languages and technologies.
Experience working efficiently under Unix/Linux environment.
Experience with source code management systems like GIT.
Experience and demonstrated proficiency with Python, Pandas, Spark and Java
Bachelor of Science degree from an accredited college/university in Computer Science, Information Technology, Computer Engineering, or related field (i.e. math and physics) required. Master’s degree a plus.
Competencies, Knowledge and Skills
Qualified individuals possess the ampliFI attributes of being smart, curious, committed to vision, passionate, fun/pleasant, an achiever and having a sense of urgency.
Ability to manage established relationships internally as well as with clients.
Ability to communicate complex technical concepts succinctly to non-technical colleagues, understand & manage interdependencies between all facets of a project.
Ability to interface with internal clients; Must have demonstrated advanced proficiency in complex, mature and sophisticated design & analysis technologies and solutions.
Skilled ability to rapidly ingest, transform, engineer, and visualize data, both for ad hoc and product-level (e.g., automated) data & analytics solutions.
Experience with large-scale, AWS big data methods such as EC2, S3, EMR, Kinesis, DynamoDB, Athena, AWS-Glue and Redshift.
Demonstrated strong knowledge of programming methodologies (Terraform, version control, testing, QA) and agile development methodologies.
Physical Requirements:
Frequently required to sit and stand.
Required to use hands to handle or feel objects, tools or controls.
Visual acuity and manual dexterity required to code using software and a laptop computer.
Other Duties:
Duties, responsibilities and activities are not all encompassing and may change at any time with or without notice. To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions


ampliFI Loyalty Solutions embraces diversity and equal opportunity. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We believe the more inclusive we are the better our company will be.",-1,Financial Transaction Processing,Unknown / Non-Applicable,Financial Services,Unknown,Company - Public,True
Senior Data Engineer,"Grainger
",Illinois,$104K - $160K (Employer est.),4.0,"About Grainger:

Grainger is a leading broad line distributor with operations primarily in North America, Japan and the United Kingdom. We achieve our purpose, We Keep the World Working®, by serving more than 4.5 million customers with a wide range of products that keep their operations running and their people safe. Grainger also delivers services and solutions, such as technical support and inventory management, to save customers time and money.

We're looking for passionate people who can move our company forward. As one of the 100 Best Companies to Work For, we have a welcoming workplace where you can build a career for yourself while fulfilling our purpose to keep the world working. We embrace new ways of thinking and recognize everyone is an individual. Find your way with Grainger today.



Position Details:

As the Supply Chain Senior Data Engineer at Grainger, your primary role is to transform data from Grainger's core domains into, real-time analytics solutions that cater to essential requirements. Your responsibilities include constructing data pipelines vital for analytics and diverse supply chain applications. You'll contribute to shaping the team's strategy, assessing, and incorporating data patterns and technologies, and collaborating with domain experts to develop data-driven products. Your strength lies in your ability to dissect challenges and create data-driven solutions. Additionally, you serve as a technical mentor, guiding teams in the adoption of the capabilities and products you develop.

You will report to the Manager of Data Engineering.



Pay:

This position is salaried and will pay between 104,000 to 160,000 with a target bonus of 10%.

The range provided is a guideline and not a guarantee of compensation. Other factors that are involved in offer decisions include, and are not limited to: a candidate's experience, qualifications, geographical area, and internal equity of the team.



You Will:
Architect and develop efficient and scalable data processing systems and pipelines on Snowflake, Kubernetes, Databricks, Airflow, Kafka, APIs, and AWS services using Python.
Create data models, mappings, and new data assets that can be leveraged by other supply chain teams for informed decision-making and optimization. Conduct exploratory data analysis on existing products and datasets.
Develop scalable and reusable frameworks for the ingestion and transformation of large datasets, improving data processing efficiency within the supply chain.
Collaborate within an Agile delivery and DataOps methodology to deliver product increments in iterative sprints.
Build data pipelines and assets that align with the needs of the supply chain, supporting their decision-making processes.
Identify and prioritize data acquisition targets in partnership with analytics teams across the Grainger Supply Chain's organization.
Build required tools for optimal extraction, transformation, and loading of data from various data sources.
Partner with stakeholders including data, design, product, and executive teams and assist them with data-related technical issues.
Modify and enhance previously developed datasets in partnership with other functional areas as business conditions and strategies evolve.
Support the Grainger Supply Chain Analyst community's proficient use of the cloud warehouse through clear onboarding procedures and governance protocols.
You Have:
5+ years of experience in batch and streaming ETL using SQL, AWS S3, Snowflake, Python, Docker, Airflow, CI/CD, GitHub Actions / Circle CI, DAGs, DBT, GitHub / Bitbucket and data visualization software for Data Engineering or Machine Learning workloads.
Bachelor's degree or equivalent experience in Supply Chain, Computer Science, Data Science, Engineering, or related disciplines.
Hands-on experience with modern data engineering projects and practices and experience prepping structured and unstructured data for data products.
Demonstrated experience implementing data management life cycle, using data quality functions like standardization, transformation, rationalization, linking and matching.
Experience leading data integration efforts of internal and external data sources.
Familiarity with containerization and orchestration technologies (Docker, Kubernetes) and experience with shell scripting in Bash, Unix or windows shell is preferable.
Rewards and Benefits:

With benefits starting day one, Grainger is committed to your safety, health and wellbeing. Our programs provide choice to meet our team members' individual needs. Check out some of the rewards available to you at Grainger.

Paid time off (PTO) days and 6 company holidays per year
Benefits starting on day one, including medical, dental vision and life insurance
6% 401(k) company contribution each pay period with no personal contribution required.
Employee discounts, parental leave, tuition reimbursement, student loan refinancing, free access to financial counseling, education and more.
DEI Statement

We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.

We are committed to fostering an inclusive, accessible environment that includes both providing reasonable accommodations to individuals with disabilities during the application and hiring process as well as throughout the course of one’s employment. With this in mind, should you need a reasonable accommodation during the application and selection process, please advise us so that we can provide appropriate assistance.

#LI - RA1 #Remote",1927,Wholesale,$10+ billion (USD),Retail & Wholesale,10000+ Employees,Company - Public,False
Data Engineer,"Kraft Heinz Company
","Chicago, IL",$102K - $141K (Glassdoor est.),3.5,"General information

All posting locations: Chicago, Illinois, United States of America

Job Function: 16 - Digital

Date Published: 25-Oct-2023

Ref #: R-75630

Description & Requirements
Job Description

3+ years of experience working in data engineering or architecture role.
Expertise in ELT and data analysis and experience with SQL and at least one programming language (Python/R preferred)
Experience developing and maintaining data warehouses in big data solutions e.g., Snowflake
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Experience with cloud service providers including AWS, Azure, or Google.
Database development experience using Hadoop, SPARK or Big Query and experience with a variety of relational, NoSQL, and cloud database technologies.
Experience with BI tools such as Alteryx, Tableau, Power BI, Looker.
Experience and/or knowledge of CI/CD (continuous integration and continuous deployment) practice using GitHub or Azure repos.
Conceptual knowledge of data and analytics, such as dimensional modeling, ELT, reporting tools, data governance, data warehousing, structured and unstructured data.
Familiarity with the Linux operating system
Familiarity with data engineering and workflow management frameworks such dbt.
Nice to have exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics.
An agile learner who brings strong problem-solving skills, and enjoys working as part of a technical, cross functional team to solve complex data problems.
Bachelor’s degree required; Computer Science, MIS, or Engineering preferred or equivalent experience.

Location(s)

Chicago/Aon Center

About Us

Kraft Heinz is a global food company with a delicious heritage. With iconic and emerging food and beverage brands around the world, we deliver the best taste, fun and quality to every meal table we touch. We’re on a mission to disrupt not only our own business, but the global food industry. A consumer obsession and unexpected partnerships fuel our progress as we drive innovation across every part of our company.

Around the world, our people are connected by a culture of ownership, agility and endless curiosity. We also believe in being good humans, who are working to improve our company, communities, and planet. We’re proud of where we’ve been – and even more thrilled about where we’re headed – as we nourish the world and lead the future of food.

Why Us

We grow our people to grow our business. We champion great people who bring ambition, curiosity, and high performance to the table as the guardians of our beloved and nostalgic brands. Good isn't good enough. We choose greatness every day by challenging the ordinary and making bold decisions. All while celebrating our wins - and our failures – as we work together to lead the future of food.

Challenging the status quo takes talent. We invest in your purpose and potential by developing skills and nurturing strengths that leave a legacy on our business and a lasting impact on your career. Because great people make great companies, and we’re growing something great here at Kraft Heinz.

Office Collaboration & Hybrid Work Environment

We believe our office environment fuels our collaboration, connection & community as an organization and allows our employees to grow toward greatness. We also believe providing a more flexible and agile model is essential in today’s workplace. A majority of our office-based employees will be able to work remotely for up to two days each week. Additionally, employees who are subject to this hybrid model will be eligible to work from anywhere for up to six weeks in a rolling 12-month period (in maximum two-week increments and according to benefits and tax guidelines). Some jobs may be required to be performed fully in office depending on the role’s responsibilities and requirements.

Kraft Heinz is an Equal Opportunity Employer that prohibits discrimination or harassment of any type. All qualified applicants are considered for employment without regard to race, color, national origin, age, sex, sexual orientation, gender, gender identity or expression, disability status, protected veteran status, or any other characteristic protected by law. Applicants who require an accommodation to participate in the job application or hiring process should contact NATAI@kraftheinz.com.",2015,Food & Beverage Manufacturing,$10+ billion (USD),Manufacturing,10000+ Employees,Company - Public,False
Data Center Engineer,"Ingram Micro
","Carol Stream, IL",$70K - $112K (Employer est.),3.8,"It's fun to work in a company where people truly BELIEVE in what they're doing!
Ingram Micro is the business behind the world’s brands reaching nearly 90 percent of the world’s population. Our market reach, diverse solutions portfolio, and digital platform Ingram Micro Xvantage™ set us apart. We have approximately 27,000 associates committed to serving our more than 161,000 customers and 1,500 vendor partners worldwide. Learn more at www.ingrammicro.com.

Ingram Micro has earned Great Place to Work Certification™ for 2022-2023 in the United States! This prestigious recognition reflects our commitment to our people and our culture.
Come join our team where you’ll make technology happen in surprising ways. Let’s shape tomorrow - it’ll be a fun journey!
This position is located in Elk Grove Village, IL and will need on-site support five days out of the week.
Your role:
As a Data Center Engineer, you will be responsible for the installation, maintenance, and troubleshooting of data center infrastructure, ensuring the smooth and efficient operation of our critical systems. The ideal candidate is technically proficient, detail-oriented, and possesses excellent problem-solving abilities.
Responsibilities:
1.Data Center Operations:
Install, rack, and cable server and network equipment according to specifications.
Perform equipment configuration, firmware updates, and system checks.
Monitor and maintain optimal environmental conditions, including temperature and humidity levels.
Ensure proper power and cooling connections for equipment.
2.Equipment Maintenance and Troubleshooting:
Conduct routine maintenance tasks on servers, storage systems, and network devices.
Diagnose and resolve hardware and network connectivity issues in a timely manner.
Perform troubleshooting and root cause analysis for equipment failures or performance issues.
Collaborate with cross-functional teams to resolve complex technical problems.
3.Cable Management:
Ensure proper labeling, organization, and management of data center cabling.
Maintain neat and accessible cable runs, following cable management best practices.
Assist with cable runs and new equipment installations while adhering to cable standards.
4.Documentation and Reporting:
Maintain accurate documentation of equipment inventory, configurations, and changes.
Document maintenance activities, incident reports, and troubleshooting procedures.
Generate regular reports on equipment performance, maintenance activities, and incident response.
5.Physical Security and Access Control:
Enforce data center security protocols and access control measures.
Monitor and control access to the data center facility, server rooms, and equipment racks.
Report any security incidents or breaches promptly.
6.Collaboration and Support:
Collaborate with network engineers, system administrators, and facilities personnel to address operational needs and implement changes.
Provide technical support and assistance to other IT teams as needed.
Participate in cross-functional projects and initiatives related to data center operations and infrastructure.
7.Compliance and Safety:
Ensure compliance with industry regulations, data privacy requirements, and safety standards.
Follow safety protocols and guidelines when working with equipment, electrical systems, and hazardous materials.
Report any safety concerns or incidents to appropriate personnel.
What you bring to the role:
High school diploma or equivalent; technical certifications or relevant degree preferred.
Proven experience as a Data Center Technician or similar role.
Strong understanding of data center infrastructure, including servers, storage systems, and network devices.
Proficient in equipment installation, configuration, and maintenance.
Solid knowledge of cabling standards and cable management best practices.
Familiarity with environmental monitoring and power distribution systems.
Familiar with Dell/HP server/storage technologies and firmware upgrade.
Excellent troubleshooting skills and ability to diagnose and resolve hardware and network connectivity issues.
Strong documentation and reporting skills.
Commitment to following safety protocols and maintaining a secure data center environment.
Ability to work independently and collaborate effectively in a team-oriented environment.
Strong communication and interpersonal skills.
#LI-SK1
#LI-On-site
The typical base pay range for this role across the U.S. is USD $70,200.00 - $112,300.00 per year.
The ranges above reflect the potential annual base pay across the U.S. for all roles; the applicable base pay range will depend on the candidate’s primary work location, pay grade, and variable compensation plan. Individual base pay within each range depends on various factors, in addition to primary work location, such as complexity and responsibility of role, job duties/requirements, and relevant experience and skills. Base pay ranges are reviewed and typically updated each year. Offers are made within the base pay range applicable at the time of hire. New hires starting base pay generally falls in the bottom half (between the minimum and midpoint) of a pay range.
At Ingram Micro certain roles are eligible for additional rewards, including merit increases, annual bonus or sales incentives and long-term incentives. These awards are allocated based on position level and individual performance. U.S.-based employees have access to healthcare benefits, paid time off, parental leave, a 401(k) plan and company match, short-term and long-term disability coverage, basic life insurance, and wellbeing benefits, among others.
This is not a complete listing of the job duties. It’s a representation of the things you will be doing, and you may not perform all these duties.
Please be prepared to pass a drug test and successfully pass a pre-employment (post offer) background check.
Ingram Micro believes there is no place in our society for social injustice, discrimination, or racism. As a company we do not – and will not – tolerate these actions.
Ingram Micro Inc. is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, or any other protected category under applicable law.",1979,Computer Hardware Development,$10+ billion (USD),Information Technology,10000+ Employees,Company - Private,False
Senior Data Engineer,"Hopscotch Health
","Chicago, IL",$106K - $141K (Glassdoor est.),3.5,"About Hopscotch Health

At Hopscotch Health, we believe great healthcare should be accessible to all people across all communities. Today, almost 20% of Americans live in a rural community, yet only 11% of physicians practice in those same communities. We are on a mission to transform healthcare in rural America. We provide high-quality primary care tailored to meet the needs of our patients through our robust care model and comprehensive care team, delivering care in our clinics, and across settings, and wrapping resources around the patients who need them most.

Our patients and the care teams who serve them sit at the center of everything we do at Hopscotch. Hopscotch Health takes a team approach to serve patient needs and provide the best care possible. Our goal is to provide the care each of us would want for ourselves or for our family members, in the right setting, and at the right time.

Today, we are serving thousands of patients in our value-based care model and the number is growing every day. If you want to bring your experience, skill and passion to make a lasting impact in healthcare, we'd like to meet you.

ABOUT THE ROLE

We are looking for a mission-driven Senior Data Engineer to join our early team at a critical and exciting time for the company. In this position, you will play a pivotal role in building out Hopscotch Health's data and analytics platform and data team. You will have an opportunity to own end-to-end execution of data products and initiatives that directly impact the care delivery process. You will be part of a skilled, passionate, and collaborative team committed to working together to realize Hopscotch Health's ambitious and important vision.

RESPONSIBILITIES

Near-term focus areas for this role will include, but are not limited to:

Build out and maintenance of the data infrastructure that supports end-user workflows, 3rd party integrations, and analytics needs
Support integration of new data (e.g. HIE, ADT feeds) as needed to enhance quality and breadth of information available to clinical care teams
Build out and maintenance of data pipelines that drive actionable intelligence
Requirements scoping and documentation with key stakeholders to continually improve the design of data products across the company
Monitor and maintain data pipeline health from ingest to final data products
Contribute to our organizational ontology (i.e. data modeling)
Collaborate with product teams to design and build out the backing data and infrastructure for organizational workflows
Generate analytic insights, build backend KPIs and other metrics to help provide visibility into company performance towards critical goals
ABOUT YOU:

You would be a great fit for this position if you have 4+ years of experience in data engineering, preferably in healthcare services, and you are:

Knowledgeable about healthcare, preferably in the services/provider sector
Results-driven and focus time/resources against the most important priorities
Creative in finding solutions to arrive at mutually beneficial outcomes
Strategic and can bring a structured, proactive approach to get things done
Can quickly develop insights from data, and translate findings into action
Demonstrate first-principles approach to understanding and solving new problems
Can translate complex concepts, verbally and in writing, and use synthesized communications to collaborate across an organization

From a cultural perspective, you are:

Thoughtful and can work effectively in a fast-paced, ever-changing environment
Constantly seeking ways to simplify and improve how things are done
Accountable, holding yourself and others to a high standard
Willing to roll up your sleeves to support the work required and collaborate effectively with people of all backgrounds
Concise and articulate, drive towards clarity
Collaborative, assuming positive intentions
Mission-driven, with a passion for serving patients and communities
Intellectually curious, with a desire to learn new skills and explore new spaces
ADDITIONAL QUALIFICATIONS:
Experience shipping production level code and pipelines for data purposes
Familiarity with healthcare data sources, interfaces and software including EHR, population health tools, patient portals, and clinical decision support technology
Comfortable writing clear, testable python code, with pyspark experience preferred
Experience with tools to build re-usable, testable, and maintainable data models (e.g. ontologies)
Experience with Airflow, Prefect, or other task execution platforms
Experience with data lake solutions such Palantir Foundry, Databricks
Comfortable in Docker and on the Unix command line
Experience with cloud infrastructure (e.g. AWS) and working with serverless computing services (e.g. Lambda)

At Hopscotch Health, we embrace diversity, invest in a culture of inclusion and positivity and encourage all to apply to join our team. You will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.",2017,Health Care Services & Hospitals,Unknown / Non-Applicable,Healthcare,1 to 50 Employees,Company - Private,True
Data Warehouse Engineer (Remote Eligible),"Busey Bank
","Champaign, IL",$50K - $77K (Glassdoor est.),3.8,"WHAT YOU'LL DO

The Data Warehouse Engineer is a key technical role responsible for the design, development, maintenance, and optimization of data warehouse processes. This includes creating and maintaining ETL (Extract, Transform, Load) processes, defining database architecture, and assisting in cloud migration efforts. The Data Warehouse Engineer collaborates with cross-functional teams to deliver data solutions that support Business Intelligence and analytics, ensuring data is accurate, timely, and easily accessible

Additional Duties:

Design, develop, and maintain data warehouse ETL processes to ensure accurate and timely data availability for business intelligence and analytics.
Define and implement efficient database architecture, including data modeling, indexing, partitioning, and performance tuning.
Collaborate with cross-functional teams to gather requirements, define data integration and transformation strategies, and deliver data solutions.
Assist in migrating existing on-premise data infrastructure to cloud platforms (e.g., AWS, Azure, or Google Cloud Platform).
Monitor and troubleshoot data pipeline performance, ensuring data quality, and addressing any data-related issues.
Stay up-to-date with the latest data engineering best practices, tools, and technologies to continuously improve and optimize data warehouse processes.
Develop and maintain technical documentation, including data flow diagrams, data dictionaries, and system specifications.

WHAT YOU'LL BRING

Knowledge of:

Proficiency in SQL and experience with various database management systems (e.g., MySQL, PostgreSQL, Oracle, SQL Server).
Solid understanding of data warehouse concepts, including star schema, snowflake schema, and dimensional modeling.
Experience with ETL tools such as Microsoft SQL Server Integration Services (SSIS).
Knowledge of cloud platforms and experience in cloud migration, specifically with AWS, Azure, or Google Cloud Platform.
Strong programming skills in languages such as Python, Java, or Scala.
Knowledge of data visualization tools such as IBM Cognos, Tableau, or Power BI.

Ability to:

Excellent problem-solving skills, attention to detail, and ability to work effectively both independently and in a team environment.
Strong communication skills, with the ability to explain complex technical concepts to non-technical stakeholders
Ability to lead and influence discussions across a variety of stakeholders to achieve results and proactively address sources of conflict
Self-starter with great communication skills (verbal and written)

EDUCATION AND TRAINING

Bachelor's degree in Computer Science, Engineering, Information Systems, or a related field. A Master's degree is preferred.
Minimum two years of experience in the technology industry, knowledge of the Financial Industry is a plus.
3+ years of experience in data engineering, ETL development, and database management.
Certifications in data engineering, database management, or cloud platforms is a plus.
Requires knowledge of Microsoft Office.



Remote Eligible In: Florida, Georgia, Illinois, Indiana, Iowa, Kentucky, Maine, Michigan, Missouri, Ohio, South Dakota, Tennessee, Texas, Virginia and Wisconsin

Busey values a diverse and inclusive workplace and strives to recruit, develop and retain individuals with exceptional talent. A team with diverse talent, working together, is essential to Busey’s commitment of delivering service excellence.

Busey is an Equal Opportunity Employer including Disability/Vets. Visit Busey.com/Careers to learn more about Busey’s Equal Opportunity Employment.",1979,Banking & Lending,$100 to $500 million (USD),Financial Services,1001 to 5000 Employees,Company - Public,False
Sr. Data Engineer / Cloud,"PeopleCaddie
","Chicago, IL",$70.00 - $75.00 Per Hour (Employer est.),4.1,"Job ID:
4500
Pay rate range:
$70 - $75
City:
Chicago
State:
Illinois
Duration:
10/29/2023 - 04/29/2024
Job Type:
Contract
Job Description

Senior Data Engineer (Cloud)

Location: Downtown Chicago

Duration: 6 months

$70-$75 / HR

Description:

The Senior Cloud Data Engineer will work directly with our product lead on multiple algorithmic data science products. design, code, test, and analyze software programs and applications. This includes researching, designing, documenting, and modifying software specifications throughout the production lifecycle. This role will also create business critical reports, analyze, and amend software errors in a timely and accurate fashion and provide status reports where required. The position responsibilities outlined below are not all encompassing. Other duties, responsibilities, and qualifications may be required and/or assigned as necessary.

Responsibilities:


Work with Product team to determine requirements and propose approaches to address users' needs

Analyze requirements to determine approach/proposed solution

Design and build solutions using relevant programming languages

Thoroughly test solutions using relevant approaches and tools

Conduct research into software-related issues and products

Bring out-of-box thinking and solutions to address challenging issues

Effectively prioritize and execute tasks in a fast-paced environment

Work both independently and in a team-oriented, collaborative environment

Flexible and adaptable to learning and understanding new technologies

Highly self-motivated and directed


Experience and Skills:


Experience in designing, developing, and maintaining data pipelines on AWS cloud platform

Experience in developing solutions using Snowflake database

Hands-on software troubleshooting experience

Proven analytical and problem-solving abilities

Experience with every phase of the software development life cycle including requirements gathering, requirements analysis, design, development, and testing

Work closely with the product leads to identify and prioritize data needs for the algorithmic data science products


Must have skills:


Hands on experience with AWS cloud architecture and development data pipelines using S3, Redshift, Athena, DynamoDB, Lambda, Glue, EMR, Kinesis, API Gateway, and other AWS technologies

Hands on experience is building and monitoring CloudWatch alarms

Hands on experience with AWS CICD suite (Code commit, Code pipeline, Cloud formation)

Hands on experience is using Python (intermediate/expert level)

Hands on experience using Spark (strongly preferred)

Hands on experience building solutions using Snowflake database

Hands on experience in trouble shooting complex SQL problems


#PCIT #LI-REMOTE",2016,Staffing & Subcontracting,Unknown / Non-Applicable,Human Resources & Staffing,1 to 50 Employees,Company - Private,False
Senior Lead Data Engineer (Remote-Eligible),"Capital One
","Chicago, IL",-1,4.0,"West Creek 8 (12080), United States of America, Richmond, Virginia

Senior Lead Data Engineer (Remote-Eligible)

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.

What You’ll Do:

Manage and develop a Java-based pipeline and query tools depending on HIve Metastore, AWS S3, Kafka and ORC

Develop analytics tooling to solve business problems driven by scale and international expansion

Optimize configurations for analytics tools to support growing business and organization

“Capital One is open to hiring a Remote Employee for this opportunity.”

Basic Qualifications:

Bachelor’s Degree

At least 8 years of experience in application development (Internship experience does not apply)

At least 2 years of experience in big data technologies

At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)

Preferred Qualifications:

9+ years of experience in application development including Python, Javascript, or Java

4+ years of experience with AWS

5+ years experience with Distributed data/computing tools (Trino, Hive, Kafka or Spark)

4+ year experience working on real-time data and streaming applications

4+ years of experience with NoSQL implementation (Cassandra)

4+ years of experience with UNIX/Linux including basic commands and shell scripting

2+ years of experience with Agile engineering practices

Capital One will consider sponsoring a new qualified applicant for employment authorization for this position.

The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.

New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Data Engineer

San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Data Engineer

Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Data Engineer

Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.

This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.

Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.

No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.

For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com

Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.

Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",1994,Banking & Lending,$10+ billion (USD),Financial Services,10000+ Employees,Company - Public,False
Arity - Lead Data Analytics Engineer - Remote,"Allstate
","Chicago, IL",$130K - $180K (Employer est.),3.4,"Founded by The Allstate Corporation in 2016, Arity is a data and analytics company focused on improving transportation. We collect and analyze enormous amounts of data, using predictive analytics to build solutions with a single goal in mind: to make transportation smarter, safer and more useful for everyone.

At the heart of that mission are the people that work here—the dreamers, doers and difference-makers that call this place home. As part of that team, your work will showcase both your intelligence and your creativity as you tackle real problems and put your talents towards transforming transportation.

That’s because at Arity, we believe work and life shouldn’t be at odds with one another. After all, we know that your unique qualities give you a unique perspective. We don’t just want you to see yourself here. We want you to be yourself here.
The Team
Our Data Science team is fueled by a passion to impact the future of mobility. We love to find the meaning within the 100’s of billions of miles of driving data we collect each year. We push the boundaries of telematics and transportation by building data and machine learning solutions to power our groundbreaking products.
We are a team of data scientists, data engineers, and analysts that take full ownership of our data pipelines and feature generation and use AWS Managed Services as the basis of our technology stack. As a part of the team, in collaboration with product owners and other experts, your work will showcase your intelligence and creativity as you solve real problems that will improve transportation and even save lives. Personally, we think that’s game-changing stuff. The Role
We are seeking a highly motivated Senior Data Analytics Engineer to join our Analytics Data Engineering team inside of Data Science. You’ll help us continue to build out an analytics data ecosystem that organizes our data for research, merges disparate sources together, and ensures data privacy best practices. You will work with a team of data engineers dedicated to creating the best telematics data and insights platform in the market.
You will help us build the next generation of deep mobility insights by extracting relevant behavioral and geospatial patterns from users’ trip data. Your team will help us find new sources of telematics data and figure out how to ingest, normalize, and process it at a scale of over 500 trips per second with full system observability. Responsibilities
Build pipelines that source data from operational systems and then process and organize it to optimize for R&D efforts across Arity
Apply appropriate methodologies for the problem using a variety of tool sets and writing code in Scala or Python.
Create highly reusable and reliable code and leverage CI/CD principles to create robust data applications
Explore data across the company and work cross-functionally to find opportunities for new data sets that can support our products
Ensure projects have appropriate measures of success that support data-driven development
Create reusable validations of data pipelines and guide the implementation of monitoring
Support a culture of reproducibility via peer review, code review, and documentation
Drive continuous improvement of data science and data engineering practices to create world-class capabilities
Influence analytics strategy and roadmap with your combination of data engineering expertise and domain experience
Build the Arity technical brand by engaging in conferences, meetups, blogs, and other external public engagements
Drive recruiting by building relationships in the industry and supporting our interviewing efforts Qualifications
Bachelor’s degree in Geospatial Science, Mathematics, Statistics, Physics, Computer Science, Engineering, or related quantitative field. A Master's degree is preferred.
At least five years of industry experience in data engineering or related roles such as Software Engineering
Remote employment experience
Experience with several of EMR, Spark, Kinesis, Athena, and Airflow
Respected by peers for technical prowess in Scala. Python is a plus
Ability to translate business problems into well-defined data and analytics problems with quantitative success measures
Experience driving end-to-end data engineering projects to generate measurable business value, including ideation, development, deployment, and maintenance & monitoring
Ability to envision and articulate radical change through highly innovative thought leadership
Inspires, mentors, and enables team to be bold and deliver high quality work
Comfortable in a fast-paced environment with high ambiguity; inspires others to embrace these conditions Nice to Have
Geospatial, sensor, or telematics experience is quite valuable, but not required
Docker, Kubernetes, CI/CD, Terraform
Data Science and machine learning (Pandas, Scikit learn)
You write code to transform data between data models and formats, preferably in Scala / Spark, Python or PySpark.
Experience moving trained machine learning models

Compensation offered for this role is $130,400.00-$179,675.00 per year and is based on experience and qualifications.

The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
That’s the day-to-day, now let’s talk about the rest of it. As we mentioned, Arity was founded by The Allstate Corporation. But you’ll be working for—and at—Arity. It’s the best of both worlds. You’ll get access to the full suite of Allstate benefits and work in a fast-paced startup culture. That’s more than just free breakfasts and brain breaks. It’s a culture that encourages you to be you.
Sound like a fit? Apply now! We can’t wait to meet you.
Arity.com Instagram Twitter LinkedIn
#LI-JB2
Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.

For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.

To view the “EEO is the Law” poster click “ here ”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click “ here ”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.
It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment",1931,Insurance Agencies & Brokerages,$10+ billion (USD),Insurance,10000+ Employees,Company - Public,False
Senior Data Engineer II,"Kirkland and Ellis
","Chicago, IL",$107K - $136K (Glassdoor est.),4.2,"About Kirkland & Ellis

At Kirkland & Ellis, we are united in our ambition and drive to move forward. We share core values that help us achieve excellence: collaboration, talent empowerment, service, inclusion, respect and gratitude. Our people are our greatest asset, and we invest in the brightest talent and encourage a diversity of perspectives and strengths to create dynamic teams that operate at the pinnacle of their field. Our talented professionals show up every day knowing they will engage in meaningful work, continuous learning and professional development.

As one of the world's leading law firms, we serve a broad range of clients with market-leading practices in private equity, M&A and other complex corporate transactions; investment fund formation and alternative asset management; restructurings; high-stakes commercial and intellectual property litigation; and government, regulatory and internal investigations. We handle the most complicated and sophisticated legal matters because we don't just meet industry standards, we create them. We bring innovation and entrepreneurialism to every engagement and, as a result, have long-standing client relationships with leading global corporations and financial sponsors. With 6,500 employees (including 3,500 lawyers) operating from 20 offices across the United States, Europe, the Middle East and Asia, we are one of the largest law firms in the world and a top financial performer.

Essential Job Functions

Owns and drives Kirkland & Ellis data integration solutions in terms of design, build and deployment, DevOps with best-in-class data models, data quality and data architecture standards
Possesses strong data capabilities in terms of data analysis, data models, and hands on expertise in crafting and deploying data pipes using Azure data platform and tools, as well as enterprise ETL tool Talend, leveraging its DQ, DI and Data Catalogue features.

ESSENTIAL FUNCTIONS (This list is not exhaustive and may be supplemented and changed as necessary.)

Accountable for the technical leadership regarding the data integration solutions and delivery. Ensuring a sound and best in class design, with enterprise implementation, deployment and operational meets the technical quality standards
Responsible for planning and coordinating in carving out the needed dev/test environments, as well as defining and managing code branching/config strategies supporting concurrent releases
Works with Data and Enterprise architecture team to define the data integration design/coding/deployment/operational standards and technology stack
Responsible for data operations, in terms of scheduling, successful execution, and reconciliation of the data pipes in production
Works collaboratively with other dev teams to guide and review their deliverables against the set standards
Works collaboratively with Data Analytics, applications, DBA, and cloud operations teams to ensure end to end integrity and usage of data assets.
Provides inputs in shaping K&E DevOps and DataOps practices
Partners with internal and external data experts to infuse innovation with focus on cross training and upskilling the existing teams.

OTHER FUNCTIONS (This list is not exhaustive and may be supplemented and changed as necessary. Delete if not applicable.)

Fosters and promotes heathy working relationships across the board to propagate end-to-end value of data.

Qualifications & Requirements

Education, Work Experience, Skills

Bachelor's degree in data, computer science or relevant discipline.
8+ years of experience in ETL, ELT and data engineering
At least 3+ years of working experience on Azure data platforms
Experience working in agile delivery, Jira usage and other agile delivery best practices
Data architecture, Data Modeling, and data visualization experience is a plus
Ability to interact with business, other teams to create data mapping documents, ETL architecture/design artifacts, performance improvements, improve delivery & operational excellence.

Technologies/Software

8+ years of end-to-end implementation experience of deploying enterprise data warehouse, data mart and data lake solutions
3+ years of working experience with Azure data solutions including but not limited to ADLS, Data Bricks, ADF, Synapse etc
Azure ADLS/Databricks administration experience
5+ years of Implementation and maintenance experience with Talend DI, DQ capabilities
Demonstrable understanding of Data Governance, and enabling technical tools and technologies

Certificates, Licensures, Registrations

Certification in Azure cloud stack, Talend Data Integration Certified Administrator/Developer will be a plus

How to Apply

Thank you for your interest in Kirkland & Ellis LLP. To complete an application and submit your resume, please click ""Apply Now.""

Equal Employment Opportunity

All employment decisions, including the recruiting, hiring, placement, training availability, promotion, compensation, evaluation, disciplinary actions, and termination of employment (if necessary) are made without regard to the employee's race, color, creed, religion, sex, pregnancy or childbirth, personal appearance, family responsibilities, sexual orientation or preference, gender identity, political affiliation, source of income, place of residence, national or ethnic origin, ancestry, age, marital status, military veteran status, unfavorable discharge from military service, physical or mental disability, or on any other basis prohibited by applicable law.

Closing Statement

The www.kirkland.com job postings and recruiting mailbox are for candidates only. If you are a recruiter, search firm or employment agency, and do not have a signed contract with Kirkland & Ellis LLP (""K&E"") and have not been asked specifically to submit candidates, you will not be compensated in any way for your referral of a candidate even if K&E hires the candidate. Direct contact with K&E employees in an attempt to present candidates is inappropriate and will be a factor in determining any future professional relationship with the Firm. #LI-Hybrid #LI-JN1",1909,Legal,$1 to $5 billion (USD),Legal,1001 to 5000 Employees,Company - Private,False
Data Engineer,"McDonald's Corporation
","Chicago, IL",$84K - $123K (Glassdoor est.),3.5,"Company Description


McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and uses our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.

Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the meaningful role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.

Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always eager for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.



Job Description

Builds and maintains relevant and reliable data products that support the business needs. Develops and implements new technology solutions as needed to ensure ongoing improvement with data reliability and observability in-view.
Participates in new software development engineering. Helps to define business rules that determines the quality of data, assists the product owner in writing test scripts that validates business rules, and performs detailed and rigorous testing to ensure data quality
Develops a solid understanding of the technical details of data domains, and clearly understands what business problems are being solved
Designing and developing data pipelines and ETL processes to extract, transform, and load data from various sources into AWS data storage solutions (e.g., S3, Redshift, Glue).
Implementing and maintaining scalable data architectures that support efficient data storage, retrieval, and processing.
Collaborating with data scientists and analysts to understand data requirements and ensure data accuracy, integrity, and availability.
Building and optimizing data integration workflows to connect data from different systems and platforms.
Monitoring and troubleshooting data pipelines, identifying and resolving performance issues and bottlenecks.
Ensuring data security and compliance with data governance policies and regulations.
Managing data infrastructure on AWS, including capacity planning, cost optimization, and resource allocation.
Staying up to date with emerging data engineering technologies, trends, and best practices, and evaluating their applicability to improve data systems and processes.
Documenting data engineering processes, workflows, and solutions for knowledge sharing and future reference.
Ability and flexibility to coordinate and work with teams distributed across time zones, as needed. For instance, early morning/late evening hours to coordinate with teams in India


Qualifications

Bachelor's or Master's degree in Computer Science or related engineering field and deep experience with AWS infrastructure
5+ years of strong experience in data engineering, specifically with AWS backend tech stack, including but not limited to S3, Redshift, Glue, Lambda, EMR, and Athena.
7+ years of proficiency in programming languages commonly used in data engineering, such as Python.
3+ years of hands-on experience with big data processing frameworks, such as Apache Spark.
5+ years of hands-on experience with data modeling, ETL development, and data integration techniques.
Working knowledge of relational and dimensional data design and modeling in a large multi-platform data environment
Solid understanding of SQL and database concepts.
Expert knowledge of quality functions like cleansing, standardization, parsing, de-duplication, mapping, hierarchy management, etc.
Expert Knowledge of data, master data and metadata related standards, processes and technology
Ability to drive continuous data management quality (i.e. timeliness, completeness, accuracy) through defined and governed principles
Ability to perform extensive data analysis (comparing multiple datasets) using a variety of tools
Demonstrated experience in data management & data governance capabilities
Familiarity with data warehousing principles and best practices.
Excellent problem solver - use of data and technology to solve problems or answer complex data related questions
Excellent communication and collaboration skills to work effectively in cross-functional teams.

Preferred Requirements:

Experience with JIRA and Confluence as part of project workflow and documentation tools is a plus
Experience with Agile project management methods and terminology a plus
Experience with Prometheus, Grafana

Additional Information


McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com

McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

Nothing in this job posting or description should be construed as an offer or guarantee of employment.

All your information will be kept confidential according to EEO guidelines.",1955,Restaurants & Cafes,$10+ billion (USD),Restaurants & Food Service,10000+ Employees,Company - Public,True
Lead Software Engineer - Pricing Reference Data,"JPMorgan Chase & Co
","Chicago, IL",$84K - $119K (Glassdoor est.),4.0,"JOB DESCRIPTION


We have an opportunity to impact your career and provide an adventure where you can push the limits of what's possible.

As a Lead Software Engineer at JPMorgan Chase within the Corporate Sector's Reference Data Engineering Division, you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. As a core technical contributor, you are responsible for conducting critical technology solutions across multiple technical areas within various business functions in support of the firm’s business objectives.

Job responsibilities

Executes creative software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Develops secure high-quality production code, and reviews and debugs code written by others
Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems
Leads evaluation sessions with external vendors, startups, and internal teams to drive outcomes-oriented probing of architectural designs, technical credentials, and applicability for use within existing systems and information architecture
Leads communities of practice across Software Engineering to drive awareness and use of new and leading-edge technologies
Design resilient, secure, and high performing platforms in Public Cloud using JPMC best practices
Improve reliability, quality, and time-to-market of our suite of software solutions moving to public cloud
Drive, support, and deliver on a strategy to build broad use of Amazon's utility computing web services (e.g., AWS EC2, AWS S3, AWS RDS, AWS CloudFront, AWS EFS, CloudWatch, EKS)
Provide primary operational engineering for the public cloud platform tp debug and optimize systems and automate routine tasks
Drive Cost management through the effective design and optimization of public cloud platforms and technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills

Formal training or certification on software engineering concepts and 5+ years of applied experience
Hands-on practical experience delivering system design, application development, testing, and operational stability
Advanced in one or more programming language(s) - Java
Advanced understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security
Experience using DevOps tools in a cloud environment, such as Ansible, Artifactory, Docker, GitHub, Jenkins, Kubernetes, Maven, and Sonar Qube
Experience using monitoring solutions like CloudWatch, Prometheus, Datadog
Knowledge of writing Infrastructure-as-Code (IaC), using tools like CloudFormation or Terraform
Practical cloud native hands-on experience
Experience with high volume, mission critical applications, and building upon messaging and or event-driven architectures
Experience with production/non-production support of highly available applications
Measure and optimize system performance, with an eye toward pushing our capabilities forward, getting ahead of customer needs, and innovating to continually improve
Preferred qualifications, capabilities, and skills
AWS Certification
ABOUT US

JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents and perspectives that they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as any mental health or physical disability needs. (If you are a US or Canadian applicant with a disability and wish to request an accommodation to complete the application process, please contact us by calling the Accessibility Line (US and Canada Only) 1-866-777-4690 and indicate the specifics of the assistance needed.)

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, we offer discretionary incentive compensation which may be awarded in recognition of firm performance and individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.

JPMorgan Chase is an Equal Opportunity Employer, including Disability/Veterans







ABOUT THE TEAM

Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we’re setting our businesses, clients, customers and employees up for success.",1799,Banking & Lending,$10+ billion (USD),Financial Services,10000+ Employees,Company - Public,False
Data Engineers,"TransUnion
","Chicago, IL",$85K - $120K (Glassdoor est.),3.9,"TransUnion's Job Applicant Privacy Notice

Personal Information We Collect

Your Privacy Choices

We'd Love to See:

Job Description
Data Engineers for Chicago, IL location. Enable new software application features and optimize strategic analytics software platforms. Contribute to Agile software development team by building systems using software development lifecycle (SDLC), data integration/ETL, big data and relational database design, business intelligence reporting and dashboard design. Build backend data software applications and reporting services to support analytics platforms. Technical environment: SQL, relational and nonrelational databases, query optimization, data modeling; data integration (AbInitio), Agile Scrum Methodology, Linux environment (shell scripting, system administration); Agile Scrum delivery methods; CI/CD environments, big data (Hive, Spark, AWS).

Job Requirements
Bachelor’s degree in Computer Science or Computer Engineering or Information Technology or related field plus three years of experience in the job offered or software development required. Required skills: software development experience with SQL, relational and nonrelational databases, query optimization, data modeling; data integration (AbInitio); Agile Scrum Methodology; Linux environment (shell scripting, system administration); CI/CD environments, big data (Hive, Spark). 60% telecommuting permitted.",1968,Research & Development,$1 to $5 billion (USD),Management & Consulting,10000+ Employees,Company - Public,False
Data Engineer 2-3,"UNCOMN
","Scott Air Force Base, IL",$75K - $112K (Glassdoor est.),4.6,"Here at UNCOMN, our mission is to empower systems thinkers to create elegant solutions to complex problems – to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients’ most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.


We’re an award-winning firm, one of the country’s fastest-growing and—more importantly—a consistent ‘Top Workplace’ as evaluated by our own employees. We are a values-driven organization (see the Core Values section of our website) and we’re looking for new Uncommon Geniuses to join our growing team, so if you’re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an ""UNCOMN Genius"" and we encourage you to check out the specifics of this position below!



UNCOMN is seeking a Data Engineer 2-3 to:

Analyze raw source data to determine contents and meaning and apply to client requirements
Design and build processes for loading and transforming data including pipelines, warehouses, and platforms
Create and present analytical dashboards to executive-level decision-makers
Turn complex data sets into useful information
Work closely with the customer to understand the business problem and need, then dig into their data to find the information to solve it
Provide the customer with a deep understanding of their data, what it means, and how it can be used
Work unsupervised and participate in team meetings, design reviews, and peer reviews
Apply exceptional communication skills and the ability to communicate appropriately at all levels of the organization
Apply expertise to multiple complex work assignments
Perform assignments that may be broad in nature, requiring originality and innovation in accomplishing tasks
Operate with appreciable latitude in developing methodology and presenting solutions to problems
Perform all functional duties independently
3+ years of experience in a professional work environment
Experience with Python or R
Experience using Databricks, or another similar tool, to perform data transformations or modeling
Experience with SQL
Bachelor’s Degree from an accredited university in a related field
Ability to navigate a complex matrix environment and manage competing priorities
Active (top) secret clearance, granted by the US Government



Preferred

Experience with BI Dashboard tools, including Qlik Sense, Tableau, or Power BI
Experience with Qlik dashboarding, development functions, and mashups
Experience with using Cloud Native Services
Ability to comprehend stakeholder needs effectively, communicate development plans, and track progress milestones
Superb organizational and time management skills
Excellent critical thinking skills for assessing numbers, trends, and data to reach new conclusions based on findings
Possession of excellent quantitative skills, including statistical analysis, process design, and data management
Qlik Data Architect, Qlik Data Analyst, or QlikView Developer Certification
Flexible PTO effective on day 1*
7 Paid Holidays & up to 3 Floating Holidays*
Eligible for Health Benefits on day 1*
401K Safe Harbor Match Program*
Training and Education Assistance*
Must be a full-time employee



Don’t meet every single requirement? We’re dedicated to building an uncommon, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.



All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.",2010,Business Consulting,$25 to $100 million (USD),Management & Consulting,51 to 200 Employees,Company - Private,False
Data Engineer,"WALGREENS
","Deerfield, IL",$95K - $152K (Employer est.),3.1,"Job Summary:

Builds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies valuable internal and external data. Collaborates closely with data scientists to define data for the design, development, and deployment of new solutions that support business priorities.

Job Responsibilities:
Develops software that processes, stores and serves data for use by others.
Develops data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Develops data pipelines that are scalable, repeatable and secure.
Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Effectively resolves problems and roadblocks as they occur.
Interacts with internal and external peers and/or managers to exchange semi-complex information related to assigned activities.
About Walgreens and WBA

Walgreens (www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance, Inc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for communities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations across America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination serving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system by providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for the nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a true omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to deliver high quality products and services in communities nationwide.
Basic Qualifications

Bachelor's degree and at least 2 years of experience in data engineering; OR Graduate Degree in a technical discipline.
Advanced knowledge of SQL
Experience establishing and maintaining key relationships with internal (peers, business partners and leadership) and external (business community, clients and vendors) within a matrix organization to ensure quality standards for service.
Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement of results and find solutions.
Willing to travel up to 10% of the time for business purposes (within state and out of state).

Preferred Qualifications

Graduate Degree in a technical discipline
Experience with REST API development
Experience with Azure application deployment
Experience in Azure technologies like Azure Data Factory, Azure Databricks using Python or Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic App
Experience in building Azure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.",1901,Drug & Health Stores,$10+ billion (USD),Retail & Wholesale,10000+ Employees,Company - Public,False
Senior Enterprise Data Engineer,"U.S. Cellular
","Chicago, IL",$103K - $149K (Glassdoor est.),3.6,"About This Role

Responsible for developing and maintaining the interfaces and data extracts for business processes. This person will design and write programs and scripts required to extract, transform, clean, and move data from the business systems into the enterprise data warehouse, data mart and operational data stores. Other related responsibilities include ongoing support, maintenance, and other ETL related information technology endeavors such as data migration, data cleansing and data modeling. Establish and maintain working relationships with technical and architectural team members across the technology organization. This person will participate in requirement meetings with Business Analysts to understand analytical content needs.

Location Note

This role is a hybrid work arrangement where the candidate can perform this role in-office or remote. In-office expectations within our operating footprint will be limited and based on an on purpose, for a purpose model should a meeting or collaborative session be scheduled. For this role, we prefer candidates that either live in the Chicagoland area or can reasonably commute to our office in Chicago should a meeting or collaborative session be scheduled.

Essential Functions

Provide strategic thinking leadership pertaining to new ways of leveraging information to improve business processes.
Design and document detailed ETL specifications based on business requirements.
Design and develop using procedural and record-based patterns. Analyze functional specifications and assist in designing potential technical solutions.
Assist in documenting the requirements, resolving ambiguities and conflicts, and ensuring requirements are complete.
Define, document, develop, enhance, and maintain Software application modules in Informatica, Talend, ksh, Perl, SQL.
Develop high performance application using tuning ETL methods related to Informatica/Talend and SQL.
Develop test plans and tests all code following standards and best practices.
Perform design validation, reconciliation, and error handling for all code.
Understand and execute release management and change management processes for changes to applications.
Identifies, troubleshoots, and resolves application issues and code defects.
Production support activities involving the identification, troubleshooting and resolution of application issues and code defects.
Work with source system developers and business owners to define data extraction methodologies.
Work with Data Modelers to develop data warehouse models, design specifications, metadata process and documentation.
Understand SDLC concepts and complies with the process specifically any iteration of an Agile process.
Demonstrate success working in a team-based environment by following standardized processes.
Participate and share expertise in peer reviews of documentation and code.
Manage individual workload and deliver to the agreed upon project milestones.
Coach and mentor junior team members to facilitate their professional development.
Demonstrate ability to work in team environment and help resolve issues.
Communicate status / risks to manager regarding delivering solution on time.

Minimum Requirements

Required Experience

Bachelor’s degree in the following areas of study: Computer Science, Information Systems, Electrical Engineering or related field or equivalent work experience required.
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
5+ years development experience with database engines including Oracle, Mongo dB and Hadoop.
5+ years of experience in ETL development, Informatica Programming, Talend Programming, Strong Database (Modeling, SQL), Oracle, Mongo dB, Hadoop, SQL, and PL/SQL.
Knowledge of database performance factors, monitoring tools and tuning procedures
Familiarity with Data Integration/Warehousing methodology and techniques.
Knowledge of the company's business practices, and familiarity with the industry's products and services is preferred.
Understanding of SDLC and production environments.
Familiarity with SCM tools specifically, GIT, Jenkins and JFrog.
Strong verbal and written communication skills.
Participate in 24/7 on-call support rotation.

Technical Skills - skills that will be involved in this environment:

Business Applications: Billing & Rating, Activation & Provisioning, Inventory Management, Sales & Marketing, Campaign Management, Dealer Compensation/Sales Commission, CRM/Customer Service
Office Automation: Word, Excel, PowerPoint, Outlook
Planning & Design: Microsoft Project, Visio
Software Engineering: PL-SQL, SQL Plus, K Shell, C Shell, Perl
Integration Technologies: Service Oriented Architecture (SOA), Informatica, Talend, or other leading ETL tools
Databases: Oracle, Mongo dB, SQL, Data Warehousing, OLAP Systems, and/or Hadoop
O/S & Network: IBM AIX, Sun Solaris, HP/UX, LINUX

#LI-JP1

Benefits

Associates have access to healthcare benefits (medical, dental and vision), retirement plans (a 401(k) plan with company match and a pension plan funded by the company), Paid Parental Leave (6 weeks after 6 months of employment), Basic Life Insurance (if eligibility requirements are met), Education Assistance (after 3 months of employment), paid Vacation Days (15 days accrued per year for full-time/6 days accrued per year for part-time), paid Sick/Care-Giver Days (6 days accrued per year), and seven paid national holidays and one floating holiday, among others. Short Term Disability (after 6 months of employment) and Term Disability (180 day waiting period) coverage is also available for full-time associates. Associates Scheduled to work under twenty hours per week or for a limited term are eligible for medical plans and retirement plans (a 401(k) plan with company match and a pension plan funded by the company).",1983,"Cable, Internet & Telephone Providers",$1 to $5 billion (USD),Telecommunications,5001 to 10000 Employees,Company - Public,False
Data Engineer II,"Zurich Insurance Company Ltd.
","Schaumburg, IL",$95K - $124K (Glassdoor est.),4.3,"Zurich North America is currently looking for a Data Engineer Role to join our Data & Analytics Engineering team, ideally based out of our North American Headquarters in Schaumburg, Illinois but we are open to filling this role virtually. Zurich’s Data & Analytics Engineering Team is responsible for the building cloud native analytics ecosystem at scale, and machine learning operations.




Responsibilities:




Use cloud design principles to develop real-time, near-real time, and batch data pipelines for enterprise data and analytics platforms.
Provides integration support for cloud data platforms, and AI toolsets.
Demonstrated experience of turning business use cases and requirements into technical solutions
Ability to produce technical documentation, architecture diagrams.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery.
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data needs
Experience with Azure Data Lake store, Blob Storage, VMs, Data Factory, SQL Data Warehouse, Azure Databricks, etc
Build large-scale batch and real-time data pipelines with data processing frameworks in Azure cloud platform.


Basic Qualifications:




Bachelor’s Degree in Computer Science or Mathematics/Statistics and 6 or more years of experience in the Data Engineering area OR
High School Diploma or Equivalent and 8 or more years of experience in the Data Engineering area OR
Zurich Certified Insurance Apprentice including an Associate Degree in Computer Science or Mathematics/Statistics and 6 or more years of experience in the Data Engineering area AND
5 or more of experience in Enterprise Data Engineering
Experience in scripting languages
Experience with data transformation and aggregation tools
Experience in data analysis, data profiling


Preferred Qualifications:




Experience using Cloud platforms/technologies (Azure ADF, Databricks, ADLS, PowerShell, cloud native databases)
Experience in scripting languages including Python, R, Java, Scala
Experience Big Data tools including with Hive, Spark SQL, Databricks, Delta Lake
Experience with ETL technologies; strong SQL and stored procedure skills
Experience with Dev/Ops,Power Apps
Insurance Industry Experience (nice to have)
Good understanding of Azure Databricks platform and ability to build data analytics solutions to support the required performance & scale.
Good understanding and experience working on Deltalake using Unity Catalog in Azure Databricks


As a condition of employment at Zurich, employees must adhere to any COVID-related health and safety protocols in place at that time (https://www.zurichna.com/careers/faq).


A future with Zurich. What can go right when you apply at Zurich?


Now is the time to move forward and make a difference. At Zurich, we want you to share your unique perspectives, experiences and ideas so we can grow and drive sustainable change together. As part of a leading global organization, Zurich North America has over 150 years of experience managing risk and supporting resilience. Today, Zurich North America is a leading provider of commercial property-casualty insurance solutions and a wide range of risk management products and services for businesses and individuals. We serve more than 25 industries, from agriculture to technology, and we insure 90% of the Fortune 500®. Our growth strategy is not limited to our business. As an employer, we strive to provide ongoing career development opportunities, and we foster an environment where voices are diverse, behaviors are inclusive, actions drive equity, and our people feel a sense of belonging. Be a part of the next evolution of the insurance industry. Join us in building a brighter future for our colleagues, our customers and the communities we serve. Zurich maintains a comprehensive employee benefits package for employees as well as eligible dependents and competitive compensation. Please click here to learn more.


As a global company, Zurich recognizes the diversity of our workforce as an asset. We recruit talented people from a variety of backgrounds with unique perspectives that are truly welcome here. Taken together, diversity and inclusion bring us closer to our common goal: exceeding our customers’ expectations. Zurich does not discriminate on the basis of age, race, ethnicity, color, religion, sex, sexual orientation, gender expression, national origin, disability, protected veteran status or any other legally protected status. EOE disability/vet


Zurich does not accept unsolicited resumes from search firms or employment agencies. Any unsolicited resume will become the property of Zurich American Insurance. If you are a preferred vendor, please use our Recruiting Agency Portal for resume submission.


Location(s): AM - Schaumburg
Remote Working: Hybrid
Schedule: Full Time
Employment Sponsorship Offered: Yes

Linkedin Recruiter Tag: #LI-MG1 #LI-ASSOCIATE",1872,Insurance Carriers,$10+ billion (USD),Insurance,10000+ Employees,Company - Private,False
Data Engineer 4,"Intuites LLC
","Mossville, IL",$91K - $127K (Glassdoor est.),4.5,"Role: Data Engineer 4

Location: Mossville, IL

1 Year

100% Remote

Identity and Access Management (IAM),Salesforce

Description:


This position is a member of a small team that coordinates and implements Salesforce security risk assessments providing consulting services to define, design, develop, implement, and maintain security processes, technologies, and systems. The position monitors security risks and must be aware of security trends to deliver appropriate risk mitigation and proactive auditing.


Technical Skills Required:

(Required)


Previous experience as a Salesforce Administrator
Knowledge and experience implementing security controls for Salesforce
Experience with Cybersecurity concepts such as Threat Monitoring, Privileged Access, and Identity and Access Management.
Security expertise with the Salesforce.com platform Experience leading remediation efforts across a complex IT landscape, 5 years experience with overall cyber security, Working experience with Salesforce or enterprise-scale cloud security or engineering, experience working with architectural designs, processes and procedures. Experience working closely with Salesforce information security tools and processes (e.g. Salesforce Health Checks, Salesforce Shield, Security Center)
(Desired)


Ranger level badges for Security and Administration on Salesforce Trailhead training platform
Demonstrated experience communicating technical information to business clients and less experienced technologists.
CISSP, CISM or equivalent
Soft Skills Required:

(Required)


Excellent communication skills (incl. documentation) Ability to collaborate with a team Ability to influence Experience with Agile methodologies (using Azure DevOps a plus) Taking initiative, self-starter Focus on quality
Education Requirements:


Bachelor's Degree in IT security focus preferred",-1,Advertising & Public Relations,Less than $1 million (USD),Media & Communication,1 to 50 Employees,Company - Private,True
Sr. Data Engineer,Egen Solutions Inc,"Naperville, IL",$75K - $115K (Employer est.),-1.0,"Egen is a data engineering and cloud modernization firm helping industry-leading companies achieve digital breakthroughs and deliver for the future, today. We are catalysts for change who create digital breakthroughs at warp speed. Our team of cloud and data engineering experts are trusted by top clients in pursuit of the extraordinary. An Inc. 5000 Fastest Growing Company 7 times, and recently recognized on the Crain’s Chicago Business Fast 50 list, Egen has also been recognized as a Great Place to Work 4 times.

You will join a team of insatiably curious data engineers, software architects, and product experts who never settle for ""good enough"". Our Data Engineering teams build scalable data platforms and fault-tolerant pipelines for modern analytics and AI services using Python, SQL, Linux, Bash, and AWS, GCP, or Azure data storage and warehousing services.

As a Senior Data Engineer, you will be the subject matter expert in building event-driven data pipelines, supporting & solving data integration challenges, and cloud-native data warehouse and data lake processing.

Responsibilities:

Lead and develop passionate data engineering teams to design and develop distributed and event-driven data pipelines with cloud-native data stores such as Snowflake, Redshift, Big Query, or ADW.
Design, document, and lead development of end-to-end data product pipelines
Consult business, product, and data science teams to understand end-user requirements or analytics needs to implement the most appropriate data platform technology and scalable data engineering practices.
Prepare data mapping, data flow, production support, and pipeline documentation for all projects.
Lead data engineering and/or operations teams in delivering completeness of source system data by performing a profiling analysis and triaging issues reported in production systems.
Facilitate fast and efficient data migrations through a deep understanding of design, mapping, implementation, management, and support of distributed data pipelines.

What we're looking for:

Minimum of Bachelor’s Degree or its equivalent in Computer Science, Computer Information Systems, Information Technology and Management, Electrical Engineering or a related field.
You have productionized real-time data pipelines through EDA leveraging Kafka or a similar service.
You know what it takes to build and run resilient data pipelines in production and have experience implementing ETL/ELT to load a multi-terabyte enterprise data warehouse.
You have a strong understanding and exposure to data mesh principles in building modern data-driven products and platforms
You have a strong background in distributed data warehousing with Snowflake, Redshift, Big Query, and/or Azure Data Warehouse.
You have expert programming/scripting knowledge in building and managing ETL pipelines using SQL, Python, and Bash.
You have implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph.

Job Type: Full-time

Pay: $75,000.00 - $115,000.00 per year

Benefits:

Health insurance

Experience:

Informatica: 6 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)

Ability to Commute:

Naperville, IL 60563 (Required)

Ability to Relocate:

Naperville, IL 60563: Relocate before starting work (Required)

Work Location: In person",-1,-1,-1,-1,-1,-1,True
"Data Engineer, Financial Reference Data","Teza Technologies
","Chicago, IL",$79K - $114K (Glassdoor est.),4.2,"About Teza

Teza is a systematic quantitative hedge fund. Every day we translate complex mathematical concepts into financial strategies trading on the world's largest markets with synergy between people and technology.
There's nothing more important to us than collaborative research, uniting the brightest minds on the planet. With offices in six time zones, we never skip a beat – trading 24/7, with holding periods from milliseconds to months.


Location
Chicago, IL (Hybrid mode with 3 days in-office requirement)

About the Role
Teza Technologies is looking for a Data Engineer to join our core services technology team. Data drives systematic trading and is critical to all aspects of the firm's business.
This is a hands-on position on a small team of data engineers with significant growth potential, as this team will grow rapidly over the next couple of years. The firm is looking for outstanding technical skills, strong attention to detail, and experience architecting and building data platforms.




Key Responsibilities
Timely onboarding of structured and alternative data vendors
Communicate with quantitative researchers and other end-users to understand their requirements and potential future requests
Investigate vendor data thoroughly to become a subject matter expert on its characteristics and irregularities
Design and implement efficient reusable ETL processing components using cutting edge technologies, and write robust tests for on-going quality control
Build flexible APIs in iterations with end-users to ensure their needs are met
Enhance existing data infrastructure and services:
Optimize data IO and load balancing for grid computation
Automate data storage management



Basic Requirements
Strong programming skills, preference for Python and Java
Proficient with SQL
Detail oriented approach
Experience supporting Financial Products/Reference Data is a big plus
Nice To Have Requirements

Experience with AWS, Hadoop, Spark would be a plus
What You'll Get
Working with world-class Quantitative Traders to bring strategies to market
Opportunity to work with the best and the brightest in the industry
To offer and implement the best practices and have major impact
Challenging tasks to help you grow professionally
Open door culture
What Makes You a Match
You are experienced data practitioner
You confidently communicate technical ideas
Difficult problems make you excited
You are not afraid of the unknown and love to learn
You have A LOT of passion and drive

Benefits
Health, life and disability insurance
Flexible sick time policy
Office lunch and snacks",2009,Research & Development,Unknown / Non-Applicable,Management & Consulting,51 to 200 Employees,Company - Private,True
Senior Data Engineer (Azure),"iManage
","Chicago, IL",$105K - $144K (Glassdoor est.),4.5,"We offer a flexible working policy that supports the health and well-being of our iManage employees. As an organization, we value collaborating and learning from our peers in person, while providing the necessary flexibility for our employees to have a meaningful work-life balance. Please reach out to learn more.

Being a Senior Data Engineer at iManage Means…

You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse. You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You will develop data models and data pipelines for our Enterprise Data Lake, and provide integration with BI platforms and tools such as Totango and Power BI. You are passionate about lakehouse architecture and have experience using Delta Lake and bronze, silver, and gold data lake design.

Here is what one of our leaders, Cloud Services Director (Jacqueline Toepfer), has to say about the role: “As a Senior Data Engineer on our team, you will get the opportunity to showcase your expertise and make a real difference across the organization. You will be part of a truly collaborative team that is passionate about delivering quality solutions. You will be the in-house expert in the data models of multiple, disparate enterprise SaaS systems and utilize your wealth of knowledge to provide recommendations and solutions for consolidation, transformation, and integration of the disparate data sources.”

iM Responsible For…

Modeling, managing, and reporting of data stored in Azure Data Lake.
Gathering data requirements from various business units and translating these requirements into data models.
Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems.
Data modeling, defining data pipelines, and integrations necessary to present data in BI platforms such as Totango, or BI tools like Power BI.
Identifying and modeling all current disparate data sources and the data flows between these data sources.
Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success.
Understanding the business needs of data integration and governance from disparate systems to drive the enhancement of the enterprise data lake.
Applying best practices to ensure the security and privacy of the data repositories.
Ensuring data repositories meet company standards for storage of PII.
Developing proficiency with the iManage product APIs for all iManage Cloud services.

iM Qualified Because I Have...

A Bachelor’s degree or higher in Computer Science or equivalent field.
3-5 years of experience working with data in a business setting.
Proficiency in data extraction, manipulation, and subsequent reporting with Spark and Python.
Experience designing data pipelines with a cloud-native mindset using Azure or AWS.
Knowledge and experience with architecting a data lake with Azure Synapse or adjacent technologies like Databricks.
Experience ingesting data from SaaS solutions and other services via API or other related technologies.
A passion to be a thought leader and work collaboratively within a team.
Commitment to understanding data requirements and delivering scalable, robust solutions that meet those requirements.
A creative mindset with a desire to explore new technologies and create innovative solutions.

Bonus Points If I Have…

Familiarity with Delta Lake.
A background with relational databases and data warehouse design using star schemas.
Experience with cloud-based data models for business solutions like Salesforce, Zendesk, and NetSuite.

Don't meet every qualification listed above? Studies show that women and people of color are less likely to apply to jobs unless they meet all qualifications. At iManage, we are committed to building a diverse and inclusive environment and encourage everyone to show up as their full authentic selves. We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways!

iM Getting To…

Join a supportive, experienced team with an inclusive, encouraging, and vibrant culture.
Have flexible work hours that allow me to balance my ‘me time’ with my work commitments.
Collaborate in a modern open plan workspace, with a gaming area, free snacks, drinks and regular social events.
Focus on impactful work, solving complex, real challenges utilizing the latest technologies and protocols.
Own my career path with our internal development framework. Ask us more about this!
Learn new skills and earn certifications with access to unlimited courses in LinkedIn Learning.
Join an innovative, industry leading SaaS company that is continuing to grow & scale!

iManage Is Supporting Me By...

Creating an inclusive environment where I can help shape the culture not just by fitting in, but by adding to it.
Providing a market competitive salary that is applied through a consistent process, equitable for all our employees, and regularly reviewed based on industry data.
Rewarding me with an annual performance-based bonus.
Offering comprehensive Health/Vision/Dental/Life Insurance, and a 401k Retirement Savings Plan with a company match up to 4%.
Giving access to HealthJoy, a healthcare concierge service, to help me maximize my health benefits.
Granting enhanced leave for expecting parents; 20 weeks 100% paid for primary leave, and 10 weeks 100% paid for secondary leave.
Providing me with a flexible time off policy to take the time off that I need. Be it for vacation, volunteering, celebrating holidays, spending time with family, or simply taking time to recharge and reset.
Caring for my mental health and well-being with multiple company wellness days and free access to the Healthy Minds app for mindfulness, meditation and more.

About iManage…

iManage is dedicated to Making Knowledge WorkTM. Over one million professionals across 65+ countries rely on our intelligent, cloud-enabled, secure knowledge work platform to uncover and activate the knowledge that exists inside their business content and communications.

We are continuously innovating to solve the most complex professional challenges and enable better business outcomes; Our work is not always easy but it is ambitious and rewarding.

So we’re looking for people who love a challenge. People who are happiest when they’re solving problems and collaborating with the industry’s best and brightest. That’s the iManage way. It’s how we do things that might appear impossible. How we develop our employees’ strengths and unlock their potential. How we find meaning in everything we do.

Whoever you are, whatever you do, however you work. Make it mean something at iManage.

iManage provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

Learn more at: www.imanage.com

Please see our privacy statement for more information on how we handle your personal data: https://imanage.com/privacy-policy/

#LI-Hybrid

#LI-LM1

9xLUIkh0Y7",2015,Enterprise Software & Network Solutions,$100 to $500 million (USD),Information Technology,501 to 1000 Employees,Company - Private,True
Senior Cloud Data Engineer - Oracle Exdata,"Xcelacore
","Chicago, IL",$70.00 Per Hour (Employer est.),5.0,"Job description

Xcelacore is a Chicago-based technology consulting firm. As a trusted partner, we help businesses implement technology to drive innovation with flexibility, quality, and for less.

Position: Senior Cloud Data Engineer

Location: Onsite at Client location in Chicago
Company: Xcelacore
Job Type: Contract
Background check. Candidates must agree to the BGC process.

Job Description:

As a Senior Cloud Data Engineer specializing in Oracle Exadata, you will have a pivotal role in architecting, designing, and managing our data infrastructure on cloud platforms, focusing on Oracle Exadata. You will collaborate with cross-functional teams to ensure data is accessible, reliable, and optimized for analytics. Key responsibilities include:

Key Responsibilities:

Oracle Exadata Expertise: Utilize your deep knowledge of Oracle Exadata to design, develop, and maintain high-performance, highly available, and secure database solutions in the cloud.
Data Architecture: Collaborate with data architects to design and implement efficient data storage and retrieval solutions that adhere to industry best practices.
Data Integration: Create and manage ETL pipelines for seamless data extraction, transformation, and loading from various sources into Oracle Exadata on cloud platforms.
Cloud Proficiency: Leverage cloud platforms, including Oracle Cloud, AWS, or Azure, to establish and maintain data environments that are scalable and cost-efficient.
Performance Optimization: Continuously monitor and optimize the performance of Oracle Exadata databases and queries to meet the analytical needs of data scientists and analysts.
Data Security: Implement robust data security protocols to safeguard sensitive data and ensure compliance with data protection regulations.
Documentation: Maintain comprehensive documentation of data architecture, ETL processes, and system configurations.
Collaboration: Work closely with data scientists, analysts, and other stakeholders to understand their data requirements and ensure data availability and quality.
Technical Leadership: Provide guidance to junior data engineers, stay updated on the latest data engineering trends and technologies, and actively contribute to the team's growth and knowledge-sharing.

Qualifications

Onsite in Chicago
Hands-on coding/ development experience with Oracle Exadata database for cloud migrations
Experience with writing ETL code base to move data from Oracle Exadata to Azure NoSQL DB
Experience with Azure Data Factory for migrating data from on-prem databases to Azure Cloud
Experience with PL/SQL-based scripting, including stored procedures, scripts, database migrations
Hands-on development experience with Azure NoSQL databases including Cosmos
Experience working with Azure Cloud
8+ years of experience with modern data engineering tech stacks

Job Type: Contract

Pay: From $70.00 per hour

Benefits:

Employee discount

Experience level:

8 years

Schedule:

Monday to Friday

Application Question(s):

Can you work onsite in Chicago?

Work Location: In person",-1,-1,Less than $1 million (USD),-1,1 to 50 Employees,Company - Private,True
Data Engineer,"Booz Allen Hamilton
","O'Fallon, IL",$58K - $133K (Employer est.),4.2,"Job Description
Location:
O'Fallon,IL,US
Remote Work:
Hybrid
Job Number:
R0184977



Data Engineer

The Opportunity:

Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there’s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need a data professional like you to help our clients find answers in their big data to impact important missions from fraud detection to cancer research, to national intelligence.

As a big data engineer at Booz Allen, you’ll use your skills and experience to implement data engineering activities on some of the most mission-driven projects in the industry. You’ll develop and deploy the pipelines and platforms that organize and make disparate data meaningful.

Here, you’ll work with a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll sharpen your skills in analytical exploration and data examination while you support the assessment, design, development, and maintenance of scalable platforms for your clients.

Work with us to use big data for good.

Join us. The world can’t wait.

You Have:

2+ years of experience utilizing programming languages, including C++, Java, or Python
2+ years of experience developing and maintaining scalable data stores that supply big data in forms needed for business analysis
Experience creating software for retrieving, parsing, and processing structured and unstructured data
Experience developing scalable ETL and ELT workflows for reporting and analytics
Experience creating solutions within a collaborative, cross-functional team environment
Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms
Secret clearance
Bachelor’s degree

Nice If You Have:

Experience in application development utilizing SQL or Scala
Experience with data visualization technologies, including Tableau, PowerBI, QlikSense, Grafana, or Kibana
Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud
Experience with distributed data and computing tools such as Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka
Experience working on real-time data and streaming applications
Experience with NoSQL implementation using MongoDB or Cassandra
Experience with data warehousing, including AWS Redshift, MySQL, or Snowflake
Experience with UNIX or Linux, including basic commands and Shell scripting
Experience with Agile engineering practices
TS/SCI clearance with a polygraph

Clearance:

Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.

Create Your Career:

Grow With Us

Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.

A Place Where You Belong

Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll develop your community in no time.

Support Your Well-Being

Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.

Your Candidate Journey

At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.

Compensation

At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.

Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $58,300.00 to $133,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.

Work Model
Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.

If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.

EEO Commitment

We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",1914,Business Consulting,$5 to $10 billion (USD),Management & Consulting,10000+ Employees,Company - Public,False
Senior Developer/Data Engineer,"Sammons Financial Group Companies
","Chicago, IL",$109K - $135K (Glassdoor est.),4.6,"Title: Senior Developer / Data Engineer

Location: Des Moines, IA; Chicago, IL; Sioux Falls, SD; Fargo, ND or Remote

Do you love working with middleware integrations? Do you thrive in collaborative environments? Do you have a proven ability to manage multiple initiatives simultaneously to deliver solutions at company wide scale?

If so, you are at the right place! We are looking for someone who will be instrumental in the growth of the organization by managing and enhancing our middleware technology product portfolio. We want someone who will build solutions to optimize our business success. We want innovators, we want doers!

As a Senior Lead Developer , you will play a critical role in designing, developing, and maintaining our data integration processes. Your primary responsibility will be to ensure the seamless flow of data across our organization, making data accessible, reliable, and actionable for various teams and stakeholders. The successful candidate will be responsible for developing enterprise-level solutions in MuleSoft and Snowflake.

Key Responsibilities:

Design, develop, and maintain real-time, APIs and batch processes by ensuring data quality, accuracy, and efficiency.
Collaborate with cross-functional teams to gather data integration requirements and design solutions that meet business needs.
Develop and maintain data pipelines, integrating data from various sources, both structured and unstructured.
Ensure data security and compliance with industry regulations, implementing best practices for data governance.
Troubleshoot and resolve data integration issues in a timely manner, working closely with the IT and data teams.
Design and develop integration solutions levering MuleSoft technology.
Mentor and provide guidance to junior developers or team member, fostering professional growth and collaborative success.
Create and maintain comprehensive documentation, ensuring knowledge transfer within the organization.
Stay up to date with industry trends and emerging technologies, providing recommendations for improving our data infrastructure.

Qualifications:

Bachelor’s degree in Computer Science, Information Technology, or a related field. Z
Minimum 10 years' IT development experience or equivalent preferred
Experience as Senior Developer and/or Systems Analyst on complex multi-subsystem solution delivery projects preferred.
Experience in hands-on development of MuleSoft Integrations and experience in developing APIs on MuleSoft platform.
Strong knowledge in Mulesoft suite, including Anypoint Platform/Studio, API development.
Experience with Mulesoft to Snowflake integration is preferred.
Experience on continuous integration and continuous development (CI/CD) practices for MuleSoft
Experience with event based integrations, connectors and other tools for Amazon S3, Amazon MQ, HTTP RESTful APIs, OIDC/OAuth (Okta), Java libraries, relational databases, file share
Expertise in MS-SQL and Snowflake-specific SQL for data manipulation and transformation.
Experience in Snowflake using SQL and ingesting external data into Snowflake using Snowflake capabilities like SnowSQL, SnowPipe/Streaming.
Experience in integration tools and frameworks, such as Mulesoft, Azure Data factory or Informatica PowerCenter.
Experience with different data formats, such as JSON, XML, and CSV, and experience with data transformation techniques
Experience understanding data modeling and data warehousing.
Experience with data integration from various sources, including databases, APIs, and flat files.
Strong problem-solving skills, attention to detail, and the ability to work in a fast-paced, dynamic environment.
Knowledge of data security, and compliance best practices.
Strong communication and collaboration skills, with the ability to work effectively in a team.
$87,412 - $182,109 - Range includes data points from multiple labor markets. Specific range is dependent on the labor market where the incumbent will be hired to perform the position. Starting salary is dependent on candidate qualifications and experience. For a narrower salary range specific to your labor market, please inquire.
Sammons Financial Group offers incentive programs for defined goals subject to eligibility and performance. Monetary rewards are based on individual and/or overall company performance.
Our competitive benefit package includes: Health, Dental, Vision, Company Paid Retirement, PTO and Holiday Pay

Preferred Skills:

Mulesoft certification is a plus.
Snowflake Certification is a plus.
Cloud platform experience, especially on AWS or Azure.",1996,Insurance Carriers,$1 to $5 billion (USD),Insurance,1001 to 5000 Employees,Company - Private,False
Data Quality Engineer,"Wellspring
","Chicago, IL",$86K - $117K (Glassdoor est.),3.5,"Wellspring is the leading provider of software tools to manage R&D and technology commercialization. Our suite of web-based software manages the transformation of scientific discoveries into real world products. We work with clients to accelerate the pace of innovation at premier research universities, R&D groups at Fortune 500 companies, and leading medical institutions. Wellspring also operates Flintbox®, the largest online marketplace for inventions emerging from research labs around the world.

If you have a passion for software and technology in general, enjoy and thrive in an agile, fast-moving, ever-changing startup environment, welcome and take on challenges of all shapes and sizes, have excellent interpersonal skill and sense of humor and enjoy rolling up your sleeves and jumping in, then read on!

This role is U.S. based and is remote work. You will have the option to travel to the HQ location in Chicago, IL for Team based meets. Applicants MUST be authorized to work in the U.S.

Position SummaryAs a Data Quality Engineer at Wellspring, you will collaborate closely with the team to procure, curate, and process raw data being fed into the Wellspring Scout data pipeline and to validate the results that it produces. We are looking for energetic and motivated individuals ready to join our top-notch engineering team and take on the exciting challenges that our search and data platform provides.

Responsibilities:

Investigate entity and document data from third-party sources
Validate and curate data within the Wellspring Scout application
Work with engineers to verify and clean data
Validate the output from machine learning and AI models

Requirements:

Proven experience as Data Analyst, Data Quality Engineer, or similar role
Knowledge of data formats and types, as well as issues or errors related to them.
General understanding of data formats including CSV, JSON, and XML.
Ability to write automation code to extract and validate data. Proven experience in a programming language such as Javascript, Ruby, Python, Java, C# etc….
Excellent communication skills
Ability to work in a team
Outstanding analytical and problem-solving skills

We invite qualified applicants to send a resume and brief introduction to tech.careers@wellspring.com.

Wellspring is an Equal Opportunity Employer.",2003,Information Technology Support Services,$1 to $5 million (USD),Information Technology,1 to 50 Employees,Company - Private,False
Data Engineer,"Medline Industries Inc
","Northfield, IL",$88K - $126K (Glassdoor est.),3.6,"Primary Location US-IL-Northfield



ETL Developer/Analyst

Summary

We are looking for a full-stack data engineer who can design, develop, and maintain data pipelines and solutions for our business needs. You will be responsible for creating and optimizing data models, extracting, transforming, and loading data from various sources, and delivering insights and reports using data visualization tools. You will also collaborate with other data engineers, analysts, and stakeholders to ensure data quality, reliability, and scalability.

Major Responsibilities

Development and Testing: Designs and implements ETL processes, including data capture, data quality, cleansing, testing, and validation methods. Develops standards, policies, and procedures for data integration-related activities.

Involved in reviewing BI enhancements during all project phases – requirements gathering, design, testing, implementation, and issue resolution.

Working closely with the team to ensure on-time delivery of quality solutions with minimal rework.

Responsible for driving continuous improvement within the team processes and deliverables.

Develops and documents use cases when necessary.

Assist with QA testing when necessary.


COVID-19 Vaccination

Please be aware that Medline requires all employees starting in this position to be fully vaccinated against COVID-19. This position will require the successful candidate to provide proof that they are fully vaccinated by their start date. Medline is an equal opportunity employer, and will provide reasonable accommodations to those individuals who are unable to be vaccinated for COVID-19 consistent with federal, state, and local law.

Requirements:

A bachelor's degree in computer science, engineering, mathematics, statistics, or a related field
At least 5 years of experience in data engineering or a similar role
Proficiency in SQL and Python or R for data manipulation and analysis
Experience with data modeling techniques and tools such as ERD, UML, Star Schema, Snowflake Schema, etc.
Experience with ETL tools and frameworks such as SSIS, Informatica, Talend, Airflow, etc.
Experience with cloud platforms and services such as the Power Platform, Azure Data Factory, Azure Databricks, Azure Synapse Analytics, etc.
Experience with API consumption
Experience with data visualization tools such as Power BI, Tableau, Alteryx, etc.
Knowledge of best practices and standards for data engineering and data governance
Excellent communication and problem-solving skills
Ability to work independently and as part of a team
Ability to test and document end-to-end processes.

About Medline:
Medline is the largest privately held manufacturer and distributor of healthcare supplies in the United States, providing more than 550,000 products that serve the entire continuum of care. Our innovative products and programs can be found in most hospitals, extended-care facilities, surgery centers, physician offices, home care dealers, home health agencies, and retail outlets.

Founded in 1910, Medline has grown from a small manufacturer of aprons, surgical gowns, and uniforms to a thriving $20 billion global enterprise because of our dedicated people, entrepreneurial spirit, and honest values.

Again named one of the country’s ""Best and Brightest Companies to Work For,” and once again named to Chicago Tribune’s Top Workplaces, Medline has experienced fifty-plus years of consecutive annual growth, and is headquartered in Northfield, IL.",1966,Health Care Products Manufacturing,$10+ billion (USD),Manufacturing,10000+ Employees,Company - Private,False
Senior Staff Data Engineer,"Grainger
",Illinois,$140K - $215K (Employer est.),4.0,"About Grainger:

Grainger is a leading broad line distributor with operations primarily in North America, Japan and the United Kingdom. We achieve our purpose, We Keep the World Working®, by serving more than 4.5 million customers with a wide range of products that keep their operations running and their people safe. Grainger also delivers services and solutions, such as technical support and inventory management, to save customers time and money.

We're looking for passionate people who can move our company forward. As one of the 100 Best Companies to Work For, we have a welcoming workplace where you can build a career for yourself while fulfilling our purpose to keep the world working. We embrace new ways of thinking and recognize everyone is an individual. Find your way with Grainger today.




Position Details:

This new team at Grainger is focused on transforming data from Grainger’s important domains into reliable and real-time analytics products that address key business needs. You will be focused on building and operating data pipelines that power analytics ranging from key financial reports to production models that define Grainger.com’s user experience. You will play an important part in defining the strategy of the team, evaluating, and integrating data patterns and technologies, and building data products alongside domain experts. You are a thoughtful observer who enjoys investigating business problems and building data solutions that address them. You are a technical teacher that can guide teams to adopt the capabilities and products you build.




Compensation:

This position is salaried and will pay between 139,700 to 214,710 with a target bonus of 20%.

The range provided is a guideline and not a guarantee of compensation. Other factors that are involved in offer decisions include, and are not limited to: a candidate's experience, qualifications, geographical area, and internal equity of the team.




You Will:

As the technical lead and architect, your primary responsibility will be to design and implement highly efficient, reusable, and scalable data processing systems and pipelines in Databricks and Snowflake.
Design and implement technical solutions and processes to ensure data reliability and accuracy.
Develop data models and mappings and build new data assets required by users. Perform exploratory data analysis on existing products and datasets.
Educate data engineering teams in adopting new data patterns and tools.
Understand trends and emerging technologies and evaluate the performance and applicability of potential tools for our requirements.
Work within an Agile delivery / DevOps methodology to deliver product increments in iterative sprints.
Function as SME within this area when engaging with our AI, Platform, and Business Analytics teams to build useful pipelines and data assets.
Work with product and business to define roadmap, communication and architecture.
Mentor junior team members.



You Have:

8+ years of experience in batch and streaming ETL using Spark, Python, Scala, Snowflake or Databricks for Data Engineering or Machine Learning workloads.
3+ years of experience prepping structured and unstructured data for data science models.
3+ years of experience with containerization and orchestration technologies (Docker, Kubernetes) and experience with shell scripting in Bash, Unix or windows shell is preferable.
Familiarity with AWS Services not limited to Glue, Athena, Lambda, S3, and DynamoDB
Demonstrated experience implementing data management life cycle, using data quality functions like standardization, transformation, rationalization, linking and matching.



Rewards and Benefits:

With benefits starting day one, Grainger is committed to your safety, health and wellbeing. Our programs provide choice to meet our team members' individual needs. Check out some of the rewards available to you at Grainger.

Medical, dental, vision, and life insurance coverage starts day one
Paid time off (PTO) days and 6 company holidays per year
6% 401(k) company contribution each pay period
Education assistance, including financial counseling, tuition reimbursement and low-cost degree options
Employee discounts, parental leave, and more



DEI Statement

We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.",1927,Wholesale,$10+ billion (USD),Retail & Wholesale,10000+ Employees,Company - Public,False
Sr. Data Analyst: Sales Engineer,EDGE,"Chicago, IL",$90K - $150K (Employer est.),-1.0,"We are a cutting-edge fintech company located in the heart of Chicago. Our primary focus revolves around leveraging bank transaction data for consumer risk analysis, making finance more accessible and secure for everyone.




As we continue to grow, we are keen to onboard a Senior Data Analyst or Sales Engineer with a proven ability to lead and drive customer engagements.

Key Responsibilities:



Serve as the lead technical consultant for both prospective and existing customers, translating intricate data insights into actionable strategies and solutions.
Independently conduct in-depth data analysis on financial datasets, unveiling insights to bolster customer growth and drive product adoption.
Partner with the sales and customer success teams, spearheading technical discussions and ensuring our offerings align seamlessly with customer objectives.
Develop and deliver high-impact presentations and reports for a diverse set of customers, underscoring the unparalleled value of our data-driven propositions.
Take the initiative in educating and mentoring customer technical and business resources, ensuring they derive the utmost value from our suite of solutions.

Requirements:

3-5 years of experience in a senior role within sales engineering, risk analysis, analytics consulting, or lending.
Medium to advanced proficiency in data analysis tools, particularly Python and SQL.
Exceptional communication and presentation skills, with a talent for elucidating complex concepts to a varied audience.
Proven capability to function independently, owning customer engagements from inception to completion.
Familiarity with AWS will be considered a strong advantage.
Prior experience in the lending sector or an in-depth understanding of its landscape will be given preference.
A foundational grasp of data science and machine learning methodologies is highly desirable.

Benefits:

Competitive salary and benefits package
Fun, fast-paced work environment

Dynamic start-up culture
Ability to make an immediate impact in a growth stage company
Convenient downtown Chicago office located in the heart of the city
Equal opportunity employer",2004,Advertising & Public Relations,Unknown / Non-Applicable,Media & Communication,1 to 50 Employees,Company - Private,True
GCP Data Engineer (Insurance),"Capgemini
","Chicago, IL",$90K - $122K (Glassdoor est.),3.7,"Title: GCP Data Engineer (Insurance)
Location: Chicago, IL

Responsibilities:

Plan, implement and oversee data warehousing projects
Work with partners to understand the business requirements for the project and develop a project plan
Manage the project budget, schedule and resources
Track project progress and identify and mitigate risks
Communicate with partners about the project status and progress
Ensure that the project meets its objectives and results

Required Skills:

Experience with cloud computing platforms such as GCP and AWS
Experience with data warehousing tools such as BigQuery
Experience with data pipeline design and implementation
Previous experience leading a team of engineers in developing a data lake for an Insurance company
Staying up to date on the latest GCP data warehousing technologies and standard processes

Life at Capgemini:
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:

Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief

About Capgemini:
Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.

Get The Future You Want | www.capgemini.com

Disclaimer:
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.

Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law",1967,Enterprise Software & Network Solutions,$10+ billion (USD),Information Technology,10000+ Employees,Company - Public,False
Senior Governance Data Engineer,"Mars
","Chicago, IL",$108K - $152K (Glassdoor est.),4.3,"Job Description:

Senior Governance Data Engineer, Supply Chain and Operations Analytics
Multiple locations: Slough/London-UK, Veghel-NL
Mars Inc is undergoing a significant Digital Transformation journey. Our ability to solve the most critical problems across Mars in a User Centric way through Data & Analytics is fundamental to our Digital Engine and transformation. The opportunities are significant for Mars, and the opportunities for those working in this space are both hugely exciting and rewarding. Connecting and deriving break-through insight from our Mars Snacking data ecosystems, leveraging the rapidly growing world of external data to get closer to our customers and consumers than ever before, and unlocking efficiencies and automation in our End-to-end Supply Chain operations are just some of our big focus areas.
Building on this momentum, we are recruiting an experienced Senior Governance Data Engineer to drive innovation, design and development for Data Mesh delivery and adoption within our organization in a transformational role. You are a part of the Data Engineering capability, working alongside teams of Data Scientists, and Delivery Managers, to design data frameworks and services which drive efficiency and quality across Mars Snacking’s end-to-end Supply Chain function.
This transformational role aims to support a 36-month transformative program and will be focused primarily on the reinvention and enhancement of the digital acceleration effort ongoing at Mars. This role has a strong potential to transition into a permanent Mars role prior to the conclusion of the transformation period, where appropriate.
The Role:
Lead the charge in establishing new ideas, governance, methodologies, and best practices in Data Engineering and analytics within Supply Chain Data & Analytics, with a focus on driving transformative change.
Engage and collaborate with our Chief Data Office and Data Stewards community across Mars to drive data quality, metadata.
Ensure data improvement plans are in place to support effective governance, by democratizing and maintaining coherent Data Models.
Own the design, build and execution of Data Frameworks for managing, governing, and improving the data quality of our Data Mesh assets, Data Catalogue and overall metadata management.
Evangelise the adoption of data driven decisions across Mars by leading the development of advanced analytical tools that utilize the vast amounts of data available to the company within the appropriate governance frameworks.
Establishes credibility with stakeholders at all levels of the organisation. Facilitates and influences complex or ambiguous business discussions, while flexing to communicate Data Engineering related possibilities and concepts in a relatable way.
Requirements:
You are a strong software engineer with deep expertise in cutting-edge technologies, robust design, development and testing of high quality and scalable data solutions.
Your deep expertise in data analytics, as well as strong practical experience with modern engineering technologies, standards and frameworks will enable you and the team to intelligently transform data into value, while creating robust and standardised capabilities.
You will work on Data Engineering projects in collaboration with other team members (including with offshore talent partners) to ensure rapid development of robust, scalable consistent and sustainable data solutions, with global best practices.
Own the design and technical roadmap for the evolution of our Data Mesh product to enable continuous adoption of Mesh assets into the organisation.
Day-to-day you will work in multidisciplinary teams & collaborate with Mars Chief Data organisation, Mars Global Analytics, Big Data Engineering, and Advanced ML teams.
Within this team, you will:
Create and implement Data Catalogue and underpinning Governance processes to allow users to access data in a standard approach as well as supporting discovery, prototyping and data science, which can subsequently be turned into enterprise solutions.
Implement the Data Quality Management standards and frameworks within the organisation through profiling, rules implementation, monitoring, alerting and resolution whilst being able to measure improvements in quality and identify the value that it delivers to the business teams.
Elevate overall data management maturity within the e2e supply chain organisation.
Understand complex business processes and combine your business acumen, problem solving skills, and curiosity to identify value-add opportunities for the applications of Data Engineering.
Present results in a cohesive, intuitive, and concise manner that can be understood by both technical and non-technical audiences.
QUALIFICATIONS & COMPETENCIES
Education:
Bachelor’s degree in Analytics or related quantitative fields (Statistics, Operations Research, Mathematics, Econometrics etc.) Advanced degree is preferred.
Preferred – Post-graduate qualifications in Computer Science, Data Engineering or professional qualifications in Advanced Data Design, Big Data.
Experience:
Minimum 8 years of experience in an Applied Data Engineering role or equivalent, ideally within the FMCG, Consumer Products, Insurance or Financial Services industries.
Technical expertise in Big Data Tech Stack and experience in database and data Engineering capabilities
Experience with Azure cloud-based computing platform.
Experience using Azure Data factory, Databricks, Azure Functions, Logic Apps , Power BI and Synapse technologies across traditional and contemporary databases (e.g. SQL Server, Hadoop, NoSQL DB), programming languages such as SQL, C#, Python and visualization tools
Experience using Azure DevOps for CI/CD, unit testing, and AGILE development.
Experienced with implementation and being the consumer of a Data Cataloguing capability, such as Alation, Collibra, MSFT Purview or other.
Understanding of design and development of data stores, digital solutions and data warehouses and associated toolsets.
Strong problem solving, communication, presentation, and stakeholder engagement skills.
Planning, project management and organizational orchestration skills and a strong attention to detail
What can you expect from Mars?
Work with over 130,000 diverse and talented Associates, all guided by the Five Principles.
Join a purpose driven company, where we’re striving to build the world we want tomorrow, today.
Best-in-class learning and development support from day one, including access to our in-house Mars University.
An industry competitive salary and benefits package, including company bonus.

Mars is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request.",1911,Food & Beverage Manufacturing,$10+ billion (USD),Manufacturing,10000+ Employees,Company - Private,False
Sr. Staff Data Engineer(Remote),"Stryker
","Chicago, IL",$110K - $233K (Employer est.),4.1,"Why engineering at Stryker?

At Stryker we are dedicated to improving lives, with a passion for researching and developing new medical device products. As an engineer at Stryker, you will be proud of the work that you will be doing, using cutting-edge technologies to make healthcare better. Here, you will work in a supportive culture with other incredibly talented and intelligent people, creating industry-leading medical technology products. You will also have growth opportunities as we have a culture that supports your personal and professional development.

Need another reason to apply? Check out these 8 reasons to join Stryker's engineering team: https://www.strykercareersblog.com/post/8-reasons-to-join-strykers-engineering-team

We are proud to be named one of the World’s Best Workplaces and a Best Workplace for Diversity by Fortune Magazine! Learn more about our award-winning organization by visiting stryker.com


Position Summary

The Senior Staff Data Engineer is responsible for supporting and advancing data intelligence projects by building and managing data ingestion, transformation, and storage to deliver accurate and reliable data to stakeholders. This position will be part of the Data Engineering team and will bring strong quantitative skills to the team. In this position, you will be engaged on projects that will ultimately use the data for various analytics techniques, such as optimization, forecasting, machine learning, predictive maintenance, visualization, and statistical analysis to develop solutions that help deliver significant value to customers.


Who we Need

Analytical problem solvers. People who go beyond just fixing to identify root causes, evaluate optimal solutions, and recommend comprehensive upgrades to prevent future issues.
Dedicated achievers. Relentless about quality, people who thrive in a fast-paced environment and will stop at nothing to ensure a project is complete and meets regulations and expectations.
Goal-oriented developers. Keeping the customer and system requirements squarely in focus, people who deliver safe and robust solutions.

What you will do
Technical Skills

Skilled and proficient in data engineering languages including Python and SQL

Experience building data pipelines and data storage solutions in a cloud environment
Proficient working knowledge of native cloud-based tools on Microsoft Azure or AWS, including SQL/NoSQL databases, data lakes & blob storage as well as various cloud-based compute solutions such as Databricks or Snowflake and Kubernetes/Docker
Skilled in data modeling, including relational and dimensional modeling, and storage of structured, semi-structured, and unstructured data
Experience with various data types (structured, semi-structured, unstructured) and analytics needs from reporting to machine learning
Expert/Master in defining and developing robust ETL pipelines and data orchestration solutions
Expert in using version control (Gitlab preferred)
Identify and use the most appropriate tool for particular project or use case
Continuously seek out industry best practices and develop skills to create new capabilities for data engineering
Develop talent and provide project leadership in the team's day-to-day project portfolio. Experience with scrum and agile frameworks expected
Provide a standard common business vocabulary, express strategic requirements, outline high-level integrated designs to meet those requirements, and align with enterprise strategy and related business architecture
Familiar with data security & data privacy best practices

Business Skills

Demonstrate financial acumen to understand financial impact for existing projects
Translate stakeholder requirements to data solutions independently
Lead discussions with Stryker enterprise across functions to leverage domain expertise and capabilities
Lead and deliver presentations and communications that build data engineering credibility and rapport to a medium size group with some guidance
Provides helpful insights in discussions on identifying opportunities where data engineering can be applied to real-world business opportunities with key stakeholder

General Skills:
Ability to manage individual tasks independently and provide leadership for owned projects
Experience leading end-to-end data engineering projects: from problem and requirements definition to model/algorithm validation and deployment
Work cooperatively with all stakeholders to ensure project success
Apply a strong understanding of Agile/Scrum procedures to enhance digital product development
Lead and mentor others' root cause/problem-solving efforts, including advanced troubleshooting
Hold self and others accountable to deliver high-quality results with passion, energy, and drive to meet business priorities
Ability to communicate complex plans and technical information to team members
Collaborate and influence others on cross-functional teams advancing partnerships to achieve business objectives

What you will need
Must Haves:
6+ years of experience & Bachelor's Degree in computer science, data analytics, mathematics, statistics, data science or related field with applicable data engineering & architecture work experience
4+ years of experience in developing and optimizing ETLs and pipelines with cloud-based solutions in Azure (preferred), Amazon Web Services, or Google Cloud Platform
Coding experience with Python
Database experience.(SQL Preferred)
Nice to Haves:
Azure Data Engineer Associate a plus
Strong problem-solving skills with the ability to evaluate and implement the most efficient and effective data engineering solution
Ability to understand complex and ambiguous business needs
Strong interpersonal and communication skills
Strong analytical skills and ability to derive value from complex clinical, med device, and wearables data

$109,500 - $232,900 salary plus bonus eligible + Benefits (Health, Vision, Dental, 401K, Tuition Reimbursement, Employee Assistance Program, Wellbeing Program, Employee Stock Purchase Program). This information reflects the anticipated salary range for this position based on current national data. Actual minimum and maximum may vary based on location. Individual pay is based on skills, experience, and other relevant factors.


About Stryker

Our benefits:


12 paid holidays annually

Health benefits include: Medical and prescription drug insurance, dental insurance, vision insurance, critical illness insurance, accident insurance, hospital indemnity insurance, personalized healthcare support, wellbeing program and tobacco cessation program.

Financial benefits include Health Savings Account (HSA), Flexible Spending Accounts (FSAs), 401(k) plan, Employee Stock Purchase Plan (ESPP), basic life and AD&D insurance, and short-term disability insurance.

For a more detailed overview of our benefits or time off, please follow this link to learn more: US Stryker employee benefits

About Stryker
Stryker is one of the world’s leading medical technology companies and, together with its customers, is driven to make healthcare better. The company offers innovative products and services in Medical and Surgical, Neurotechnology, Orthopaedics and Spine that help improve patient and healthcare outcomes. Alongside its customers around the world, Stryker impacts more than 130 million patients annually. More information is available at stryker.com.

Know someone at Stryker?
Be sure to have them submit you as a referral prior to applying for this position. Learn more about our employee referral program on our referral page

Stryker is driven to work together with our customers to make healthcare better. Employees and new hires in sales and field roles that require access to customer accounts as a function of the job may be required, depending on customer requirements, to obtain various vaccinations as an essential function of their role.",1941,Health Care Products Manufacturing,$10+ billion (USD),Manufacturing,10000+ Employees,Company - Public,False
Data Scientist: Analyst: Sales Engineer,EDGE,"Chicago, IL",$60K - $120K (Employer est.),-1.0,"We are a cutting-edge fintech company located in the heart of Chicago. Our primary focus revolves around leveraging bank transaction data for consumer risk analysis, making finance more accessible and secure for everyone.

We're on the hunt for a dynamic Data Scientist, Data Analyst, or Sales Engineer who can bridge the gap between technical insights and customer engagement.

Key Responsibilities:



Act as the primary technical point of contact for both prospective and existing customers, demystifying complex data findings and insights.
Perform data analysis on financial datasets to extract actionable insights, supporting customer growth and optimizing product usage.
Collaborate closely with the sales and customer success teams, offering technical guidance to ensure that the product aligns with customer requirements.
Create detailed reports and presentations for customers, emphasizing the value and applicability of our data-driven solutions.
Educate and support customer technical and business resources, ensuring smooth integration and maximum value from our product.
Stay updated on industry trends and innovations to consistently offer fresh insights and solutions.

Requirements:

1-2 years of professional experience in sales engineering, risk analysis, analytics consulting, or lending.
Proficiency in data analysis using Python and SQL.
Outstanding communication skills, with an ability to explain technical concepts in a clear and concise manner to a non-technical audience.
Experience in a customer-facing role, with a proven track record of fostering positive customer relationships.
Familiarity with AWS is a notable plus.
Experience in lending or a deep understanding of the lending landscape will be an advantage.
Knowledge of data science and machine learning methodologies is a bonus, but not required.

Benefits:

Competitive salary and benefits package
Fun, fast-paced work environment

Dynamic start-up culture
Ability to make an immediate impact in a growth stage company
Convenient downtown Chicago office located in the heart of the city
Equal opportunity employer",2004,Advertising & Public Relations,Unknown / Non-Applicable,Media & Communication,1 to 50 Employees,Company - Private,True
Managing Software Engineer - GCP Data Architect,"Capgemini
","Chicago, IL",$107K - $143K (Glassdoor est.),3.7,"Managing Software Engineer - GCP Data Architect


Job Location – NYC NY, Chicago IL, Columbia SC, Atlanta GA, Houston TX, Nashville TN

Duration – Fulltime


Job Description

Recommend and architect GCP solutions that are cost effective scalable and secure.
Design and implement data pipelines and data warehouses on GCP.
Key Responsibilities
Develop and implement machine learning and artificial intelligence solutions on GCP.
Architecting Hierarchies for Data analysis and Reportings solutions.
Developing and implementing ETL pipelines to load data into data warehouses.
Optimizing and troubleshooting data warehouses for performance and reliability.
Work with data analysts and Business analyst to understand their data requirements and provide solutions to meet their needs.
Optimize and troubleshoot data pipelines to ensure performance and reliability.
Staying up to date on the latest GCP data warehousing technologies and best practices.
Designing and building data warehouses on GCP Architecting Hierarchies for Data analysis and Reportings solutions.
Design implement and manage GCP solutions for businesses B21 B22.
Required Skills
Experience with cloud native application development.
Strong knowledge of SQL Python and data processing frameworks such as Apache Spark.
Experience with data modeling and data warehousing concepts such as star schemas snowflakes and fact and dimension tables.
Experience with data warehousing technologies GCP BigQuery.
Understaning of GoogleSQL is a plus.
Experience with cloud based architectures and DevOps practices.
Excellent problem solving and analytical skills Strong communication and teamwork skills.
Experience with ETL tools and processes such as Talend.
Experience on Autosys for scheduling.
Knowledge of Airflow will be needed Design and implement cloud security solutions to protect data and applications Monitor and optimize cloud infrastructure for performance and cost efficiency
Life At Capgemini

Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:

Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief
About Capgemini

Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com

Disclaimer

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.

Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law",1967,Enterprise Software & Network Solutions,$10+ billion (USD),Information Technology,10000+ Employees,Company - Public,False
Sr. Data Engineer,Tekfocus Minds Pvt Ltd,"Chicago, IL",-1,-1.0,"experience with Insurance or life insurance (Life and Annuities) Clients

experience with data to understand the data pattern, rules and underlying business logic

experience in Data Engineering: SQL, AWS (Redshift)

experience with Marketo

experience in handling Marketo data and understand the business logic behind marketing data in Marketo and lead conversations with the Marketing team

experience in transformation / mapping logic, business rules, calculations, and data mapping requirement

experience in developing, enhancing data models and identify ETL optimization opportunities:

experience with advanced SQL functionalities (joins, nested query, procedures, PL/SQL):

Job Type: Full-time

Experience level:

7 years

Application Question(s):

Do you hold GC or USC?

Experience:

Marketo: 3 years (Required)
SQL: 1 year (Preferred)
Total: 7 years (Preferred)

Work Location: On the road",-1,-1,-1,-1,-1,-1,True
Data Center Operations Engineer – Intermediate (Hyde Park),"The University of Chicago Medicine
","Chicago, IL",$66K - $97K (Glassdoor est.),3.7,"Job Description:

Join one of the nation’s most comprehensive academic medical centers, UChicago Medicine, as a Data Center Operations Engineer – Intermediate for our IT Technology Services - Hyde Park Department. This position is based on-site at our Hyde Park location with occasional travel to off-site locations.


Job Summary


In this role, the Data Center Operations Engineer - Intermediate is responsible for day-to-day operation of UChicago Medicine’s data centers, IT spaces and associated support equipment including HVAC, UPS’S, power distribution units, emergency generators and critical monitoring. There will be ample opportunity to work independently with general direction from other team members and the Data Center Infrastructure Manager.




Essential Job Functions

Network Operations
Access Layer Data Jack Activations and troubleshooting (patching and port configuring)
Access Layer Hardware installation and support
Single Phase UPS Hardware installation and support
Firewall Rule processing (TCP IP understating a plus)
IP Address Reservations and reclaims
Responsible for overseeing vendors
Ability to oversee vendors
Ability to identify and resolve network issues in the hospital and data center
Role identifies, diagnoses, and resolves UPS, HVAC and other vendor problems
Assists Specialist and Senior to Identify and diagnoses complex problems and factors affecting infrastructure systems
Works closely with team to manage maintenance and repair activities
Understands theory of operation of all infrastructure equipment and systems
Must understand theory of operation in order to determine if equipment is in normal state or condition
Ability to read and understand one-line diagrams, schematics and blueprints
Ability to instruct subordinates in theory of operation and normal operating conditions
General knowledge of safe working practices (e.g., familiar with proper tool safety, eye protection, ladder safety, etc.)
Racks, stacks and provisions servers, switches and cabling in data center and other IT spaces
Runs fiber and copper patch cables, and follows proper termination techniques per data center established processes, procedures and policies
Maintains detailed written records of all work activity
Determines work priorities from the Service Now queue and ensures adherence to all commit times
Carries out decommissioning of network servers and switches
Able to lift heavy equipment/items.
Able to perform all essential job functions, including walking, standing, bending, stooping, climbing, lifting and manual dexterity, with or without reasonable accommodation
Ability to operate generators, UPS’s, HVAC equipment and switchgear
Ability to manually operate all critical infrastructure equipment which would correlate back to understanding of operations (if you don’t understand it, you can’t do it)
Additional projects and responsibilities as assigned by manager
Takes initiative in keeping order and cleanliness in data center and auxiliary spaces
Able to work days/nights/weekends/holidays, if needed and/or required

Qualifications

BS or BA degree, Computer Science, Engineering, or equivalent education, training or work experience
Two (2) or more years of Infrastructure Engineering experience
Experience in maintenance and monitoring of server systems
Experience working within a data center or network operation center environment
Experience with computer hardware and server hardware or troubleshooting/diagnosing
Must be able to work independently as well as work as part of a fast-moving team
Travel to off-site locations may be required; must possess a valid driver’s license and proof of insurance
Demonstrate strong work ethic
Take initiative and a proactive approach
Be adaptable to change

Preferred

Project Management experience a plus
Basic understanding of TCP IP networks and subnets
Experience with monitoring data center infrastructure
Experience with critical HVAC and fire detection/suppression systems

Position Details:

Job Type / FTE: Full Time (1.0 FTE)
Shift: Days and On-call
Unit/Department: IT Technology Services - Hyde Park
CBA Code: Non-Union

Must comply with UCMC’s COVID-19 Vaccination requirement as a condition of employment. If you have already received the vaccination, you must provide proof as part of the pre-employment process. This is in addition to your compliance with the Flu Vaccination requirement as well. Medical and religious exemptions will be considered consistent with applicable law. Lastly, a pre-employment physical, drug screening, and background check are also required for all employees prior to hire.
Why Join Us:
We’ve been at the forefront of medicine since 1899. We provide superior healthcare with compassion, always mindful that each patient is a person, an individual. To accomplish this, we need employees with passion, talent and commitment… with patients and with each other. We’re in this together: working to advance medical innovation, serve the health needs of the community, and move our collective knowledge forward. If you’d like to add enriching human life to your profile, The University of Chicago Medicine is for you. Here at the forefront, we’re doing work that really matters. Join us. Bring your passion.

________

Bring your career to the next level at a hospital that is thriving; from patient satisfaction to employee engagement, we are at the Forefront of Medicine. Take advantage of all we have to offer and #BringYourPassiontotheForefront.

University of Chicago Medicine is growing; discover how you can be a part of this pursuit of excellence at: www.uchospitals.edu/jobs

The University of Chicago Medical Center is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, ethnicity, ancestry, sex, sexual orientation, gender identity, marital status, civil union status, parental status, religion, national origin, age, disability, veteran status and other legally protected characteristics.",1986,Health Care Services & Hospitals,$1 to $5 billion (USD),Healthcare,5001 to 10000 Employees,Nonprofit Organization,False
"Data Engineer (AWS, Azure, GCP)","CapTech Consulting
","Chicago, IL",$87K - $117K (Glassdoor est.),4.0,"Company Description


CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.



Job Description


CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.

Specific responsibilities for the Data Engineer – Cloud position include:

Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly
Utilizing your skills in engineering best practices to solve complex data problems
Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization.
Articulating architectural differences between solution methods and the advantages/disadvantages of each


Qualifications


Typical experience for successful candidates includes:

Experience delivering solutions on a major cloud platform
Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture
Experience in the design and implementation of data architecture solutions
A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines
Ability to assess and utilize traditional and modern architectural components required based on business needs.
A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning.

Skills:

Successful candidates usually have demonstrable experience with technologies in some of these categories:

Languages: SQL, Python, Java, R, C# / C++ / C
Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle
DevOps: git, docker, subversion, Kubernetes, Jenkins
Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR
Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer

Additional Information


We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.

CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness

CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.

At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.

#LI-LM1",1997,Information Technology Support Services,$100 to $500 million (USD),Information Technology,1001 to 5000 Employees,Company - Private,False
Senior Data Center and Infrastructure Engineer,"Ingram Micro
","Carol Stream, IL",$94K - $160K (Employer est.),3.8,"It's fun to work in a company where people truly BELIEVE in what they're doing!
Ingram Micro is the business behind the world’s brands reaching nearly 90 percent of the world’s population. Our market reach, diverse solutions portfolio, and digital platform Ingram Micro Xvantage™ set us apart. We have approximately 27,000 associates committed to serving our more than 161,000 customers and 1,500 vendor partners worldwide. Learn more at www.ingrammicro.com.

Ingram Micro has earned Great Place to Work Certification™ for 2022-2023 in the United States! This prestigious recognition reflects our commitment to our people and our culture.
Come join our team where you’ll make technology happen in surprising ways. Let’s shape tomorrow - it’ll be a fun journey!
Senior Data Center and Infrastructure Engineering Responsibilities:
Install, rack, and cable server and network equipment according to specifications.
Perform equipment configuration, firmware updates, and system checks.
Monitor and maintain optimal environmental conditions, including temperature and humidity levels.
Ensure proper power and cooling connections for equipment.
Conduct routine maintenance tasks on servers, storage systems, and network devices.
Diagnose and resolve hardware and network connectivity issues in a timely manner.
Ensure proper labeling, organization, and management of data center cabling.
Maintain neat and accessible cable runs, following cable management best practices.
Assist with cable runs and new equipment installations while adhering to cable standards.
Ability to produce detailed documentation according to as build docs and Visio drawings.
General knowledge centralized storage concepts and have ability to configure servers for connectivity and use.
Installation and configuration of VMware vCenter Server and ESXi servers
Administration of VMware virtual environment using vSphere spanning multiple data centers and networks (AMER, APAC, EMEA).
Perform troubleshooting of existing environment and new deployments in VMWare servers.
Conduct active monitoring of environment and respond to alerts and errors.
Configure and troubleshoot VMware virtual switches and underlying network infrastructure.
Configure and troubleshoot fiber channel SAN, vSAN, iSCSI, and NFS datastores.
Assist other system engineers with troubleshooting virtual machines (Microsoft, RedHat, Ubuntu, etc.) and virtual desktops (VDI).
Provide follow-up reports (technical findings, feedback, resolution steps taken) for Root Cause Analysis (RCA), engineering technical assessment and process improvements.
Interacting with internal users and management to handle service requests tickets, inquiries, and problems.
Work closely with other engineering and service teams to build unique technical skills and best practices to optimize the use of our technology in modern data center environments.
Develop and nurture strong relationships with the data center design, operations, and support personnel.
Execute given assignments in a self-directed manner, anticipate issues, and modify procedures as necessary.
Willingness to work on-call & rotating shift schedule to support 24/7 operation.
What you bring to the role:
Degree in computer science or equivalent combination of education and experience required.
5+ years of related Information Technology experience, including a minimum 3-year position specific to general server administration
Knowledge of blade servers, networking, storage technologies (SAN, NAS, DAS, etc.)
Strong SAN design and implementation skills.
Strong VMware ESXi and vCenter design and implementation skills.
Windows PowerShell and/or Linux shell scripting.
Strong experience in Windows and Linux operating systems.
Intermediate knowledge of copper and fiber connectors, types, and speeds.
Hands-on working experience with servers, routers, firewalls, and switches.
Strong leadership qualities and experience working directly with customers to ensure strong customer relationships.
Strong client-facing skills, assertiveness, strong communications skills, leadership, self-starter.
Excellent verbal and written communication skills

#LI-SK1
#LI-On-site
The typical base pay range for this role across the U.S. is USD $94,100.00 - $160,000.00 per year.
The ranges above reflect the potential annual base pay across the U.S. for all roles; the applicable base pay range will depend on the candidate’s primary work location, pay grade, and variable compensation plan. Individual base pay within each range depends on various factors, in addition to primary work location, such as complexity and responsibility of role, job duties/requirements, and relevant experience and skills. Base pay ranges are reviewed and typically updated each year. Offers are made within the base pay range applicable at the time of hire. New hires starting base pay generally falls in the bottom half (between the minimum and midpoint) of a pay range.
At Ingram Micro certain roles are eligible for additional rewards, including merit increases, annual bonus or sales incentives and long-term incentives. These awards are allocated based on position level and individual performance. U.S.-based employees have access to healthcare benefits, paid time off, parental leave, a 401(k) plan and company match, short-term and long-term disability coverage, basic life insurance, and wellbeing benefits, among others.
This is not a complete listing of the job duties. It’s a representation of the things you will be doing, and you may not perform all these duties.
Please be prepared to pass a drug test and successfully pass a pre-employment (post offer) background check.
Ingram Micro believes there is no place in our society for social injustice, discrimination, or racism. As a company we do not – and will not – tolerate these actions.
Ingram Micro Inc. is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, or any other protected category under applicable law.",1979,Computer Hardware Development,$10+ billion (USD),Information Technology,10000+ Employees,Company - Private,False
Azure Data Engineer,"highbrow-tech LLC
","North Chicago, IL",$65.00 - $70.00 Per Hour (Employer est.),3.7,"Role: Azure Data Engineer

New skill set required for GRS modularization and cloud migration: Expertise in Azure cloud, middle tier, back-end, SQL/Synapse

Chicago IL

Job Type: Full-time

Salary: $65.00 - $70.00 per hour

Expected hours: 50 per week

Benefits:

401(k)
Dental insurance
Health insurance
Vision insurance

Compensation package:

Monthly bonus
Quarterly bonus
Signing bonus
Yearly bonus

Experience level:

6 years
7 years

Schedule:

8 hour shift
Monday to Friday

Willingness to travel:

100% (Required)

Work Location: Hybrid remote in North Chicago, IL 60064",-1,-1,Unknown / Non-Applicable,-1,1 to 50 Employees,Company - Private,True
Lead Data Engineer - Agency Analytics,"State Farm
","Bloomington, IL",$109K - $138K (Glassdoor est.),3.6,"Overview:
We are not just offering a job but a meaningful career! Come join our passionate team!
As a Fortune 50 company, we hire the best employees to serve our customers, making us a leader in the insurance and financial services industry. State Farm embraces diversity and inclusion to ensure a workforce that is engaged, builds on the strengths and talents of all associates, and creates a Good Neighbor culture.
We offer competitive benefits and pay with the potential for an annual financial award based on both individual and enterprise performance. Our employees have an opportunity to participate in volunteer events within the community and engage in a learning culture. We offer programs to assist with tuition reimbursement, professional designations, employee development, wellness initiatives, and more!
Visit our Careers page for more information on our benefits, locations and the process of joining the State Farm team!

Office Location(s): Bloomington, Illinois; Dunwoody, Georgia; Richardson, Texas; Tempe, Arizona

Hybrid Work Arrangement: In this position, you should plan to spend time working in the office and remotely (with the ability for same day travel to and from your assigned office location) as part of a hybrid work arrangement. Work arrangements could change over time based on business need. Your manager will share additional information regarding your department’s approach and what it means for you.

**Applicants are required to be eligible to lawfully work in the U.S. immediately;
employer will not sponsor applicants for U.S. work authorization (e.g., H-1B Visa) for this opportunity.**


Responsibilities:
Agency Analytics will be developing data products using emerging AWS technologies and capabilities. Team responsibilities include analyzing detailed enterprise transactional data across product lines to help foster a compliant culture and promote consistency. This is a dynamic process where new metrics and factors for consideration will continue to evolve. The methods of evaluation will also be reviewed to ensure we are identifying and addressing new risks that may emerge. The data needs for this work includes data on-premises and in AWS. There are critical needs to enable more data consumption from AWS to support reporting and analytics needs across multiple enterprise departments.

As Lead Data Engineer you will:
Work closely with Enterprise Technology, other data engineers in other functions, and others on the Agency Analytics team to develop data products to support business needs.
Design analytic data asset solutions to meet Agency and enterprise needs, including focused validation and testing to ensure data is of the highest quality.
Develop, test, implement, execute, validate, and monitor logic to build and maintain data products.
Prepare and manipulate data for use in development of data analytics solutions.
Provide data pipeline solutions for the development, implementation, containerization, execution, validation, monitoring, and improvement of data analytics solutions.
Demonstrate advanced understanding in data engineering practices, including ETL (Extract, Transform, Load), data integration, reusable data pipelines, data management, data analytics, data visualization, and data science.
Conformance with State Farm data management and governance policies.
Establish business domain knowledge for State Farm data sources.
Identify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that support and extend data solutions.
Drive the establishment of this new role within Agency Analytics.


Qualifications:
Bachelor’s degree in computer science, engineering, related data field OR equivalent certifications and experience.
Experience in programming languages such as Python, Spark, Terraform, Angular 7, SAS, React JS, JavaScript, and jQuery.
Experience with AWS services such as (AWS Glue, Lambda, SQS, SNS, Cloud Watch, S3, Event Bridge, RDS, EC2).
Experience with various databases (e.g., AWS Redshift AWS DynamoDB, DB2, Postgres, Oracle, MS SQL, MongoDB).
Experience or ability to rapidly gain enterprise data domain knowledge, including agency and product line data.
Experience designing, developing, and deploying production applications in AWS: S3, DynamoDB, Redshift, API GatewayGlue, Lambda, EKS, SageMaker.
Ability to learn and adopt new technologies and languages.
Critical thinking skills to challenge current thinking and apply the right technology to solve problems.
Experience architecting and engineering data pipelines and optimizing them for performance and scalability.
Experience taking initiative in shaping and driving solutions with teams and business partners.
Prior work with 'legacy' type data structures.
Experience with gathering and creating analytic business requirements, researching potential data sources (both internal and external sources), designing, developing, and maintaining data assets.
Experience in large, complex data environments both highly coupled and highly decoupled.
Comfortable communicating technical content to both technical and non-technical audiences over static documentation and live presentations.

**Applicants are required to be eligible to lawfully work in the U.S. immediately;
employer will not sponsor applicants for U.S. work authorization (e.g., H-1B Visa) for this opportunity.**",1922,Insurance Carriers,$10+ billion (USD),Insurance,10000+ Employees,Company - Private,False
Senior Big Data Engineer,"Integral Ad Science
","Chicago, IL",-1,3.9,"Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for A Senior Data Engineer to join our team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!

What you'll get to do:

Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Work on Big Data technologies such as Spark, Spark streaming, Kafka, NoSql, EMR & other AWS tech stack
Contribute to the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming & batch ETL and RESTful APIs
Provide leadership, work collaboratively, and be a mentor in an awesome team

You should apply if you have most of this:

Bachelors or Masters in Computer Engineering, Computer Science, Electrical Engineering or related field
5+ years of recent hands-on in object oriented language (Java, Scala)
5+ years of experience designing and building data pipelines and data-intensive applications
Experience using Big Data frameworks (e.g.,Spark, Flink) , databases (e.g., NoSql, Delta Tables, Mysql / Postgres) for complex data assembly and transformation
Strong knowledge of collections, multi-threading, JVM memory model, etc.
In-depth understanding of algorithms, performance, scalability, and reliability in a Big Data setting
Solid understanding of OLTP and OLAP systems, database fundamentals
Solid knowledge of SQL
Experience in full software development, Agile, and CI/CD
Experience building production level systems in AWS cloud environment

What puts you over the top:

Experience with Spark streaming or Flink
Familiarity with messaging frameworks like Kafka and / or NoSql databases (Aerospike, Cassandra)
Orchestrating data pipelines using tools such as Airflow



About Integral Ad Science

Integral Ad Science (IAS) is a leading global media measurement and optimization platform that delivers the industry's most actionable data to drive superior results for the world's largest advertisers, publishers, and media platforms. IAS's software provides comprehensive and enriched data that ensures ads are seen by real people in safe and suitable environments, while improving return on ad spend for advertisers and yield for publishers. Our mission is to be the global benchmark for trust and transparency in digital media quality. For more information, visit integralads.com.

Equal Opportunity Employer:

IAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.

California Applicant Pre-Collection Notice:

We collect personal information (PI) from you in connection with your application for employment or engagement with IAS, including the following categories of PI: identifiers, personal records, commercial information, professional or employment or engagement information, non-public education records, and inferences drawn from your PI. We collect your PI for our purposes, including performing services and operations related to your potential employment or engagement. For additional details or if you have questions, contact us at compliance@integralads.com.

To learn more about us, please visit http://integralads.com/ and https://muse.cm/2t8eGlN

Attention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership.

#LI-Hybrid",2009,Internet & Web Services,$100 to $500 million (USD),Information Technology,501 to 1000 Employees,Company - Public,False
Senior Data Engineer,"Gigster
","Chicago, IL",$130K - $187K (Glassdoor est.),3.6,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.

At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.

We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.

Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.

All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.

In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.",2013,Computer Hardware Development,Unknown / Non-Applicable,Information Technology,1 to 50 Employees,Company - Private,False
Field Engineer - Data Center,"Black Box
","Dekalb, IL",$70K - $99K (Glassdoor est.),3.4,"Job Location: DeKalb, IL.

Website: www.blackbox.com

Essential Functions:

Work in partnership with the Construction and Project Managers to ensure delivery and maintenance of all structured cabling and connectivity is on time, within budget, and up to industry leading standards for quality and reliability.
Work with client field engineering team to monitor progress and quality of installation within all phases of construction and/or operational tasks within the data center(s).
Review, comprehend and implement work as shown in contract documents. Review drawings and report on discrepancies to foster proactive quality control.
Serve as the technical lead for the construction team on all structured cabling standards, material selection, testing and installation methods in the field. Assist field installation team in resolving technical issues and partner with client’s design and engineer team to support design reviews, BOM reviews, and as-built drawing validations.
Ensure site as-built design documentation is complete and up-to-date.
Coordinate with procurement lead as materials subject matter expert (SME) for all materials questions and estimates. Verify all required material types, quantities, cable lengths etc. necessary to complete requested work. Ensure correct mapping of all material counts and characteristics in comparison with construction drawings (general notes) and cable schedules prior to and throughout the duration of the project.
Provide oversight to the Run Operations team for the detailed turn-up schedule, moves, adds and changes, and set project priorities. Ensure rapid responses to high priority builds and break-fixes.
Support development of best practices and initiatives, coordinate SOPs/ SIPs with vendors, and participate in global and regional standards. Assist in developing and/or identifying network cable/rack/cabinet installation processes and standards.
Ensure crisp oversight and clear communication across multiple platforms with cross-functional partners and vendors. Proactively communicate progress and project risks and initiatives to key stakeholders.
Manage field activities and implement engineering designs.
Communicate with clients on site to determine needs and explain complex issues.
Diagnose construction and/or machinery issues and resolve malfunctions or other crisis when they arise.
Inspect and test material and machinery to ensure proper maintenance of onsite equipment.
Conduct research and studies on site.
This position may lead and direct on-site technicians.
Additional duties as assigned.

Education Requirements:

Bachelor’s Degree in Engineering, Management Information Systems, IT, Computer Science or related field or equivalent, relevant experience.


Experience Requirements:

5 years of experience in Structured Cabling, Telecommunications, IT or OSP.
Proven direct experience in fiber optic and structured cabling installation required.
Experience with outside and inside plant cabling (OSP & ISP) required.
Experience working in a mission critical environment requiring coordination across multiple disciplines in order to achieve the end goal.
Experience in OSP, ISP and Run Ops environments preferred.
Experience leading people and/or project teams preferred.
Experience reviewing design documents and specifications preferred.


Other Qualifications, Skills & Certifications:

Proficient with Microsoft Office Suite of tools
Proficient with design and construction software such as Procore, Bluebeam, and BIM360.

Competencies

Ability to analyze, understand and interpret data.
Ability to think strategically and act tactically.
Ability to connect with, and influence stakeholders.

#LI-JM1



What's in it for You?



Joining Black Box means you’ll have the opportunity to work on exciting, highly visible projects with our customers. You’ll get to know large and small organizations across the world and work to deliver best in class IT solutions. You’ll be rewarded with a variety of experiences, development opportunities and exposure to some of the world's most admired companies. In addition to these rewarding experiences, you’ll have access to our comprehensive Total Rewards Program. Our program continues to evolve to meet the needs of our dynamic workforce by providing a variety of benefits which are applicable and competitive for each country & region.




About Black Box



Black Box is the trusted global solutions integrator and digital technology partner. With nearly 45 years of experience connecting people and devices, we are an organization of top technical professionals dedicated to delivering solutions and services that help organizations design, build, manage, and secure their communications and IT infrastructure and networks. Technologies include 5G/OnGo, connected buildings, digital workplace, multisite deployments, data centers, and IoT.



We also design and manufacture award-winning products for Pro AV, KVM, cabling, and networking known for their advanced functionality, flawless performance, outstanding reliability and fail-safe security. We deliver high-value technology services and products through our values, such as innovation, ownership, transparency, respect and open-mindedness combined with our global presence and 2,500+ team members in 24 countries and growing. Black Box is a wholly-owned subsidiary of AGC Networks.



Black Box is an equal opportunity employer. Black Box does not discriminate against individuals on the basis of race, color, marital status, sex, sexual orientation, sexual identity, religion, national origin, age, disability, veteran status, genetic information, or any other protected status, and endorses those policies and practices which seek to recruit, hire, train and promote the most qualified persons into available jobs.",1976,Information Technology Support Services,$500 million to $1 billion (USD),Information Technology,1001 to 5000 Employees,Company - Private,False
Sr Informatica Data Engineer - Remote,"Optum
","Alton, IL",$80K - $118K (Glassdoor est.),3.7,"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.




Functions may include database architecture, engineering, design, optimization, security, and administration, data modeling, big data development, Extract, Transform, and Load (ETL) development, storage engineering, data warehousing, data provisioning, and other similar roles.




Responsibilities may include Platform-as-a-Service and Cloud solutions with a focus on data stores and associated ecosystems. Duties may include:

Management of design services
Providing sizing and configuration assistance
Ensuring strict data quality
Performing needs assessments



Position in this function analyzes current business practices, processes, and procedures and identifies future business opportunities for leveraging data storage and retrieval system capabilities. Manages relationships with software and hardware vendors to understand the potential architectural impact of different vendor strategies and data acquisition. May design schemas: write SQL or other data markup scripting and helps to support the development of Analytics and Applications that build on top of data. Selects, develops, and evaluates personnel to ensure the efficient operation of the function.




You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.




Primary Responsibilities:

Developing ETL solutions and supporting data strategy leveraging acquired subject matter expertise to identify/inform opportunities and risks
Creating physical and logical data models supporting new data warehouse development and architecture
Developing/leading complex dashboards, scorecards, reports, and data analysis
Performing peer-reviews and providing technical support/guidance to the project team
Leading the creating/maintaining of Teradata database, ETL, reporting architecture/environment, processes/procedures, coding standards, and data management/security activities
Supporting/Standing up the enterprise data environment for disaster situations or annual disaster recovery drill
Supporting the project manager and leadership in project planning and executing activities, including presentations, risks/issues, and status reporting
Demonstrating deliverables to the team or the customers for approvals and gain buy-in to implement solutions or adapt to new products/services/programs
Working with the DBAs, data modelers, and application/system admins to support maintaining the Teradata and application server hardware and software (including patches, fixes, and upgrades) and leading industry level standards or best practices



You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications:

Bachelor’s degree or higher in engineering, related field, or equivalent experience
7+ years of ETL and enterprise data warehouse technical development

7+ years of experience gathering requirements, exploring data, SQL/DB development
7+ years of experience in reporting, analysis, and/or data management/governance
4+ years of Informatica PowerCenter
Demonstrated experience working with high-performing remote teams and delivering to external customer
Advanced skills in Microsoft Office (Excel, Word, PowerPoint, etc.) with solid communication skills



Preferred Qualifications:

MBA or advanced degree
Certified Business Intelligence Professional (CBIP)
4+ years of experience leading a team as a technical lead or similar capacity
Experience in Medicaid/Managed Care/health insurance industry
Experience with visualization tools (Power B.I., Tableau, SAS, etc.)
Experience using Snowflake
Experience in scripting language experience (Python, R, SAS, T-SQL, etc.)
Familiarity with A.I. and machine learning



All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy



California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California/Colorado/Connecticut/Nevada/New Jersey/New York/Rhode Island/Washington residents is $85,000 to $167,300. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.




At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.




Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.




UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.",-1,Health Care Services & Hospitals,$10+ billion (USD),Healthcare,10000+ Employees,Company - Public,False
Data Engineer IV - Max Digital (Data Engineering),"ACV Auctions
",Illinois,-1,3.7,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.

Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:

Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.

What you will do:

Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:

Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling

At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.

For information on our collection and use of your personal information, please see our Privacy Notice.",2014,Wholesale,Unknown / Non-Applicable,Retail & Wholesale,1001 to 5000 Employees,Company - Public,True
Senior Data Engineer (Snowflake) - Contractor,"EY
",United States,$90.00 - $130.00 Per Hour (Employer est.),3.9,"The primary responsibility for the Sr. Data Engineer / Tech Lead is to ensure data accessibility, quality, and reliability within the client's data platform ecosystem.

RESPONSIBILITIES

Design and develop end-to-end data pipelines and ETL processes using Snowflake, Denodo, DBT Cloud, Fivetran, and AWS Glue, ensuring seamless data integration, transformation, and loading.
Collaborate with cross-functional teams to gather data requirements and implement scalable data models and schemas within Snowflake and Denodo, ensuring optimal performance and data consistency.
Implement and maintain data orchestration workflows using Stonebranch, ensuring efficient and reliable scheduling and automation of data processes.
Work with EnterpriseDataLake for ingestion and storage of data from various sources, ensuring the availability and accessibility of data for analytics and reporting purposes.
Utilize Azure DevOps and Teraform to implement and manage infrastructure as code, ensuring streamlined deployment and scalability of data engineering solutions.
Skills

snowflake

denodo

dbt cloud

fivetran

Qualifications

REQUIREMENTS

Proficiency in Snowflake and Denodo: Strong understanding and hands-on experience in working with Snowflake and Denodo for data integration, transformation, and analysis.
Knowledge of DBT Cloud: Familiarity with DBT Cloud, including experience in building and maintaining efficient data transformation workflows and deploying reliable data pipelines.
Experience with Fivetran: Demonstrated experience in utilizing Fivetran for data ingestion, source connectivity, and ensuring data accuracy and completeness.
Understanding of EnterpriseDataLake: Knowledge of EnterpriseDataLake principles and practices, including data ingestion, storage, and organization for analytics and reporting purposes.
Familiarity with Stonebranch: Proficiency in using Stonebranch for data orchestration and scheduling, ensuring efficient and reliable execution of data pipelines and workflows.
Proficiency in Azure DevOps and Teraform: Experience in leveraging Azure DevOps and Teraform for infrastructure as code, enabling streamlined deployment and scalability of data engineering solutions.

The expected pay rate range for this contract assignment is $90 to $130 per hour. The exact pay rate will vary based on skills, experience, and location. The position is approximately 65-70% remote and 30-35% travel to the client as needed.

#GigNowTechOpportunities

GigNowOpportunities

Equal Employment Opportunity Information
EY provides equal opportunities to applicants, employees, contractors, vendors, and stakeholders without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law. If you are an individual with a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please email gignow.recruiting@ey.com.",-1,-1,Unknown / Non-Applicable,-1,Unknown,Unknown,False
Senior Cloud Data Engineer,"BDO
","Oak Brook, IL",$103K - $138K (Glassdoor est.),3.7,"Job Summary:

This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.

Job Duties:

Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned

Supervisory Responsibilities:

Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product



Qualifications, Knowledge, Skills and Abilities:

Education:

High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred

Experience:

Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred

Software:

Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies

Other Knowledge, Skills & Abilities:

Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel

Keywords: Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL


Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.

California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000




BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.

Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.



BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some examples of our Total Rewards offerings include:
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!",2007,Accounting & Tax,$10+ billion (USD),Financial Services,10000+ Employees,Company - Private,False
Senior Data Engineer,"IFG Companies
",Illinois,$135K - $170K (Employer est.),4.0,"ABOUT IFG COMPANIES
IFG Companies, founded in 1985, is one of America’s largest privately held, property and casualty “A” rated insurance groups in the United States. As a private company, IFG is focused on profitable growth through superior underwriting of a range of products and service to its producers and insureds.

LOCATION
IFG Companies is hiring in Arizona, Connecticut, Florida, Georgia, Illinois, New Jersey, North Carolina and on a case-by-case basis in Ohio, Pennsylvania, Texas and Virginia. Applicants must reside in one of these states to be hired. Remote and flexible work arrangements are provided to all employees based on manager approval.

SALARY RANGE
The expected salary range is $135,000 to $170,000 and subject to change at any time at the discretion of the Company.


POSITION SUMMARY:

The Senior Data Engineer position at IFG calls for an experienced data engineer to have necessary architecture and development skills to help the company design and build enterprise data systems, using data warehouse concepts. The Senior Engineer will take on a critical role in providing long term solutions for the foundation of the data and analytic platforms in accordance with our Enterprise Data Governance Program. This position is the lead for developing solutions that deliver business value and meet our enterprise goals.

RESPONSIBILIES:

Continuous implementation of IFG Companies’ Enterprise Data Governance Program.
Responsible for deploying Master Data Management (MDM) solution at an enterprise.
Work proactively across the organization to identify data sources, data users, and data processes.
Analyze complex system requirements and present findings and recommendations to stakeholders.
Designing and supporting reporting assets, including data warehouse, and analytics.
Work closely with senior management and IT directors to design and develop future data architectures at IFG.
Interact with stakeholders to analyze, explain, design, and develop new data services and capabilities supporting the enable IFG’s business to take data driven decisions.
Assist in developing best BI and data services practices such as Azure Data Factories and Azure CI/CD pipelines, including development, governance and maintenance.
Invent ways to answer key business questions by leveraging existing data assets or assisting in creating new ones
Promote a collaborative team environment and mentor others.
Collaborate with Business Intelligence team leads, development leads and other team members in improving IFG’s data governance practices, data quality and system stability.
Proactive in researching our data quality and working towards ensuring proper controls are in place so quality continues to improve.
Design, maintain, write, and review high-quality code that satisfies customer needs and strives for simplicity, clarity, and testability.
Prepare technical documentation, including data design, test results, and technical manuals.
Support existing systems, including deeply understanding their functionality and how they provide business value to our internal and external customers.


KNOWLEDGE, SKILLS, AND ABILITIES:

Demonstrates being a highly collaborative, team-oriented, and strong consensus builder, working with all levels of employees, including senior management.
Deep understanding of technology solutions (on-premises and cloud) used by the data governance role, including:
Master and reference data management tools
Data dictionary, metadata management
Data quality and lineage
Data security, retention, and implementation of controls
Data storage and operations
Experience with ELT and ELT skills, including SQL Server Integration Services, Azure Data Factory, and Azure Pipelines.
Knowledge of modern data warehouse concepts such as: Data Lakehouse, Parquet files, and Python with Azure Notebooks is a big plus.
Deep understanding of data sources, including structured and unstructured (e.g., applications, logs, files, third-party, etc.) data.
Strong proficiency in one or more data languages and structures (e.g., T-SQL, stored procedures, structured views, triggers, table design, linked servers, synonyms, entity framework, dapper technologies, etc.)
Strong ability to understand the context for the applications and assets and their relation to business objectives.
Strong knowledge of Microsoft SQL Server, experience with NoSQL databases (i.e. Cosmos DB is a plus)
Experience with Microsoft Analysis Services (SSAS), Microsoft Reporting Services (SSRS) and Microsoft SQL Server Integration Services (SSIS).
Experience with Microsoft Master Data Management or Microsoft Profisee is a big plus.
Experience with reporting technologies, including SQL Server Reporting Services, Power BI (on-premises and Power BI premium services in the cloud)
Ability to prioritize and manage multiple tasks while meeting established deadlines.
Excellent problem-solving and analytical skills.
Growth mindset and a willingness to learn new skills, technologies, and frameworks.
Experience with agile development methodologies.
Excellent communication, interpersonal, and analytical skills with a strong customer service orientation, including the ability to clearly communicate in a non-technical manner with stakeholders


EDUCATION/EXPERIENCE/CERTIFICATIONS

Bachelor’s degree in computer science, information systems or equivalent.
Microsoft certifications applicable to data engineering and data architecture are highly desirable:
Microsoft Certified: Azure Data Fundamentals
Microsoft Certified Azure Solutions Architect
8+ years of experience as a data engineer, data developer, application developer, or similar role.
Background in insurance, property and casualty, financial services is preferred.


PHYSICAL DEMANDS:

Physical demands are considered to be that of an office environment, climate controlled, with minimal physical exertion. This position requires prolonged sitting, ability to utilize a computer and interactions with others in meetings or via phone.


BENEFITS
We offer a competitive compensation and benefits package including medical, dental, vision, 401(k), flexible spending, short-term and long-term disability insurance, life insurance, long-term care, education assistance and paid time off, including paid parental leave and a birthday holiday.

IFG Companies is an equal opportunity employer committed to a diverse workforce. M/F/D/V

#LI-JF1
#LI-REMOTE",1985,Insurance Carriers,Unknown / Non-Applicable,Insurance,201 to 500 Employees,Company - Private,False
Senior Data Engineer,"Publicis Sapient
","Chicago, IL",$103K - $154K (Employer est.),3.7,"Senior Data Engineer
Full-time

Company Description

Publicis Sapient is a digital transformation partner helping established organizations get to their future, digitally-enabled state, both in the way they work and the way they serve their customers.We help unlock value through a start-up mindset and modern methods, fusing strategy, consulting and customer experience with agile engineering and problem-solving creativity.United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across truly value

Job Description

Publicis Sapient is looking for a Senior Associate Data Engineer to be part of our team of top-notch technologists. You will lead and deliver technical solutions for large-scale digital transformation projects. Working with the latest data technologies in the industry, you will be instrumental in helping our clients evolve for a more digital future.

Your Impact:

Combine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our client's business
Translate client's requirements to system design and develop a solution that delivers business value
Lead, designed, develop, and deliver large-scale data systems, data processing, and data transformation projects
Automate data platform operations and manage the post-production system and processes
Conduct technical feasibility assessments and provide project estimates for the design and development of the solution
Mentor, help and grow junior team members

Your Technical Skills & Experience:

Demonstrable experience in data platforms involving implementation of end to end data pipelines
Hands-on experience with at least one of the leading public cloud data platforms (Azure, AWS or Google Cloud)
Implementation experience with column-oriented database technologies (i.e., Big Query, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server, Oracle, MySQL)
Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Azure Data Factory, Glue ETL, Lambda, Spark, Spark Streaming, etc.
Ability to handle module or track level responsibilities and contributing to tasks “hands-on”
Experience in data modeling, warehouse design and fact/dimension implementations
Experience working with code repositories and continuous integration
Data modeling, querying, and optimization for relational, NoSQL, timeseries, and graph databases and data warehouses and data lakes
Data processing programming using SQL, DBT, Python, and similar tools
Logical programming in Python, Spark, PySpark, Java, Javascript, and/or Scala
Data ingest, validation, and enrichment pipeline design and implementation
Cloud-native data platform design with a focus on streaming and event-driven architectures
Test programming using automated testing frameworks, data validation and quality frameworks, and data lineage frameworks
Metadata definition and management via data catalogs, service catalogs, and stewardship tools such as OpenMetadata, DataHub, Alation, AWS Glue Catalog, Google Data Catalog, and similar
Code review and mentorship
Bachelor’s degree in Computer Science, Engineering or related field.

Set Yourself Apart With:

Developer certifications for any of the cloud services like AWS, Google Cloud or Azure
Understanding of development and project methodologies
Willingness to travel

Additional Information

Pay Range:$103,000 -$154,000

The range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. Actual range for this position may differ, depending on location and the specific skillset required for the work itself.

Benefits of Working Here:

Flexible vacation policy; time is not limited, allocated, or accrued
16 paid holidays throughout the year
Generous parental leave and new parent transition program
Tuition reimbursement
Corporate gift matching program

As part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to Equal Employment Opportunity without regard for race, color, national origin, ethnicity, gender, protected veteran status, disability, sexual orientation, gender identity, or religion. We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at hirin[email protected] or you may call us at +1-617-621-0200",1990,Business Consulting,Unknown / Non-Applicable,Management & Consulting,10000+ Employees,Company - Public,False
Lead Data Warehouse Engineer (Hybrid),"Enova International
","Chicago, IL",$102K - $151K (Glassdoor est.),3.9,"We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas or take over sponsorship at this time.

About the role:

The Data Engineering, Warehouse, and Operations Team effectively and sustainably builds data strategy and provides data solutions and tools across the organization. We integrate, transform, and improve volumes of data at the project or enterprise level for streamlined processes, greater efficiencies, and smarter, more informed decision-making. This team is high-energy, dynamic and in a business-critical domain space. This role is an opportunity to make a real difference in the data space, and we need confident, experienced people eager to bring in solutions, with a demonstrated ability to learn fast and make that happen.

Requirements:

Experience designing, developing, and working with dimensional models is a must.

10+ years of strong database and SQL experience
7+ years of experience leading projects/teams from conception to completion in fast-paced dynamic environments
5+ years of Data Warehousing experience/methodologies
6+ years of experience in designing and implementing ETL/ELT frameworks for complex data mart projects
Proven experience in architecting, designing, implementing, and maintaining multi-layered SQL and Python processes
Experience working with Relational Database Management Systems, including PostgreSQL, MS SQL Server, MySQL, RDS, and Cloud Data Warehouses such as Snowflake and AWS Redshift
A Bachelor's or Master's degree in Engineering, Computer Science, IT or related study is preferred
Nice to have: AWS and/or Snowflake Certifications

Responsibilities:

Opportunity to lead technical initiatives by designing and architecting solutions that meet the business needs, ensuring data accuracy, reliability, and performance
Collaborating with principals, peers, leadership, and the business to execute solutions
Act diligently to respond to urgent projects and tasks
Troubleshooting discrepancies in existing databases, data pipelines, warehouses, and reporting
Work as a ""full stack"" Data Engineer, contributing to each phase of the SDLC, building a new pipeline between two data sources or working with the business to design and develop a new dashboard
Advise on best practices and innovative designs/solutions
Perform other functions as assigned by management to support the operation of the business

Benefits & Perks:

Flexible work schedule (In-office T/W/Th and remote M/F for hybrid-eligible roles)
Health, dental, and vision insurance including mental health benefits
401(k) matching plus a roth option (U.S. Based employees only)
PTO & paid holidays off
Sabbatical program (for eligible roles)
Summer hours (for eligible roles)
Paid parental leave
DEI groups (B.L.A.C.K. @ Enova, HOLA @ Enova, Women @ Enova, Pride @ Enova, South Asians @ Enova, APEX @ Enova, and Parents @ Enova)
Employee recognition and rewards program
Charitable matching and a paid volunteer day…Plus so much more!

Full-Time Employees working 30+ hours per week are eligible for benefits; interns are not eligible.

About Enova

Enova International is a leading financial technology company that provides online financial services through our AI and machine learning-powered Colossus™platform. We serve non-prime consumers and businesses alike, while offering world-class technology and services to traditional banks—in order to create accessible credit for millions.

Being a values-driven organization is at the core of Enova's success. We live our values by listening to our customers, challenging assumptions, thinking big, setting high expectations, and hiring and developing the best. Through our values and our commitment to making Enova an awesome place to work, we maintain an environment of inclusion and culture where our employees can thrive. You can learn more about Enova's values and culture here.

It is our policy to provide equal employment opportunity for all persons and not discriminate in employment decisions by placing the most qualified person in each job, without regard to any other classification protected by federal, state, or local law. California Applicants: Click here to review our California Privacy Policy for Job Applicants.",2004,Banking & Lending,$1 to $5 billion (USD),Financial Services,1001 to 5000 Employees,Company - Public,False
"Sr Engineer, Data Engineering & Analytics","KeHE Distributors, LLC
","Naperville, IL",$95K - $145K (Employer est.),3.5,"Why Work for KeHE?:
Full-time
Pay Range: $95,000.00/Yr. - $145,000.00/Yr.
Shift Days: , Shift Time:
Benefits on Day 1
Health/Rx
Dental
Vision
Flexible and health spending accounts (FSA/HSA)
Supplemental life insurance
401(k)
Paid time off
Paid sick time
Short term & long term disability coverage (STD/LTD)
Employee stock ownership (ESOP)
Holiday pay for company designated holidays
Overview:
Good people, working with good people, for our common good.

Sound good?
KeHE-a natural, organic, specialty and fresh food distributor – is all about “good” and is growing, so there’s never been a more exciting time to join our team. If you’re enthusiastic about working in an environment with a people-first culture and an organization committed to good living, good food and good service, we’d love to talk to you!
Primary Responsibilities:
As the Senior Data Engineering and Analytics Engineer, you will have the opportunity to make a significant impact on the success and growth of KeHE by engaging with internal business stakeholders of all levels. You will become an enabler by providing support & guidance in defining and delivering their data engineering and analytics needs by turning data into actionable information leveraging on-prem and cloud technologies.
Essential Functions:
Demonstrated ability to partner with internal product stakeholders to help define requirements and outcomes for data-focused initiatives
Ability to decompose large problems and execute smaller, manageable bodies of work to demonstrate continuous architecture delivery
Architecting and managing scalable data pipelines across on-premise and cloud data repositories
Create data flow diagrams, and document source to target mapping
Utilizing their business acumen, analytical mindset, and strong communication/people skills to extract business requirements and translating them into actionable tasks
Maintain the data warehouse performance by optimizing batch processing through parallelization, performance tuning, aggregations etc.
Create user-facing dashboards that provide key insights at a glance for its specific audience
Recognize and adopt best practices and cost-effective solutions in developing analytical insights on prem and in the cloud
Provide guidance and mentor junior team members
Keep current with Business Intelligence data trends and technological innovations
Execute on POC’s with new technologies, drive innovation, and new ideas
Minimum Requirements, Qualifications, Additional Skills, Aptitude:
4 Year College Degree from an accredited university in one of the following: Finance, Computer Science Information Systems, Operations Research, Mathematics, Statistics, or related technical field
7+ years leveraging Microsoft and its associated array of offerings (SQL, PL/SQL, SSIS, SSRS, SSAS, PowerBI, etc.)
5+ years leveraging AWS and its associated array of offerings (Glue, Redshift, Athena, S3, Spectrum, PySpark etc.)
You have deep expertise in Python and C#
You have collaborated with a development team with established source control (git).
Strong understanding data modeling (i.e. conceptual, logical and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Strong experience with business intelligence and data warehousing design principles and industry best practices including multi-dimensional modelling (star schemas, snowflakes, de-normalized models, handling “slow-changing” dimensions)
Ability to work with stakeholders and changing priorities
Ability to learn and apply new technologies to business problems


You would be a great fit if:




You love a challenge and problem solving
Enjoy learning and working with new technologies
You are extremely articulate and concise
You are proactive and resourceful with a drive to match
You have an intrinsic curiosity for tinkering with data products
You recognize the importance of data storytelling
You have excellent written and verbal skills
You are hardworking and have a strong work ethic
You are a quick learner and able to apply new technologies to business problems
Requisition ID: 2023-21574 Equal Employer Opportunity Statement: KeHE Distributors provides equal employment opportunities to all employees and applicants for employment and prohibits all forms of discrimination and harassment on the basis of race, color, religion or faith, sex, gender, age, ancestry, national origin, mental or physical disability or medical condition, sexual orientation, gender identity or expression, marital status, military or veteran status, genetic information, or any other category protected under federal, state, or local law. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training as well as the administration of all Human Resources and Talent Acquisition processes.",1954,Wholesale,Unknown / Non-Applicable,Retail & Wholesale,5001 to 10000 Employees,Company - Private,False
Senior Staff Azure Data and Services Software Engineer,"ServiceNow
","Chicago, IL",$185K - $323K (Employer est.),4.4,"Company Description


At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.

With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™.

Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.

Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.


Job Description


As a Senior Staff Azure Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization's business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.

Key Responsibilities:

Solution Design: Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic cloud technical solutions with Azure, AKS and other Microsoft cloud technologies.
Architecture Planning: Develop and maintain technology roadmaps, ensuring alignment with organizational goals and Azure cloud best practices.
Technical Leadership: Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices for Azure cloud capabilities.
Risk Assessment: Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.
Documentation: Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.
Vendor Evaluation: Assess and recommend third-party tools, products, and services that can enhance our technical solutions.
Prototyping: Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.
Performance Optimization: Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.
Security and Compliance: Ensure that the Azure cloud solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.
Collaboration: Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.

Qualifications


Qualifications:

Bachelor’s degree in computer science, Information Technology, or related field (Master's degree preferred).
Cloud computing expertise (e.g., Azure).
Knowledge of DevOps practices and tools.
Proven experience as a Lead Engineer and Solution Architect or a similar role.
Strong knowledge of enterprise architecture principles and best practices.
Familiarity with microservices architecture.
Familiarity with graph databases.
Proficiency in designing and implementing solutions using various technologies and platforms.
Excellent problem-solving and analytical skills.
Outstanding communication and interpersonal abilities.
Project management skills and experience in managing complex technical projects.
Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus.

Preferred Skills:

Knowledge of DevOps practices and tools.
Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes)
Strong understanding of data architecture and database technologies.
Knowledge of cybersecurity best practices.
Excellent presentation and facilitation skills.

#DTjobs

For positions in the Bay Area, we offer a base pay of $184,700 - $323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.


Additional Information


ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.

At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.

If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.

For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.

Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.




From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license.

Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.",2004,Enterprise Software & Network Solutions,$1 to $5 billion (USD),Information Technology,10000+ Employees,Company - Public,False
"Senior Data Engineer, Public Company","Recruiting From Scratch
","Lakeview, IL",$100K - $200K (Employer est.),4.0,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:

Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.

With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.

What’s so interesting about this role?

Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .

In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.

What’s the job?

Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.

What we’ll love about you

5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.

Salary Range: $165,000-$210,000 USD base. Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/",2019,Staffing & Subcontracting,$1 to $5 million (USD),Human Resources & Staffing,1 to 50 Employees,Company - Private,True
"Senior/Staff Software Engineer, Data Pipelines","EvenUp
",Illinois,-1,3.9,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.

EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.

As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.

The role:

We’re looking to bring on board Senior/Staff Software Engineers focused on our Data Pipelines as we’ve experienced unprecedented growth and need to build & scale out our data pipelines and infrastructure. We’re looking for strong team members to help architect and drive forward the vision of our ideal data infrastructure at EvenUp. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.

What you'll do:

Build fault tolerant data pipelines to process diverse datasets at EvenUp

Design and develop modularized services to increase the capabilities and scope of our data infrastructure

Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability

Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data

Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.

Mentor and coach junior team members

Help grow our engineering team and define a “data first” mentality across our organization.

What we are seeking:

8+ years of industry experience designing and building distributed data systems

Previous experience architecting and scaling event driven architectures

Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch

The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions

Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure

You enjoy owning a project from start to finish and love to drive a project across the finish line.

Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)

Nice to haves:

Fluency in Python, SQL and GraphQL

Previous experience integrating ML models and LLMs into data services

Domain expertise in legal technology, medical records, and working with unstructured data

Benefits & Perks:

We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:

- Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)

Flexible working hours to match your style

- Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!

Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours


EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",-1,Internet & Web Services,Unknown / Non-Applicable,Information Technology,Unknown,Company - Public,True
Senior Staff AI Data Engineer,"Recruiting From Scratch
","Chicago, IL",$160K (Employer est.),4.0,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/

This is a hybrid role based in our Palo Alto or San Francisco offices and will require you to be in office Tuesdays and Thursdays.

What’s so interesting about this role?

We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.

What’s the job?

We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.

In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.

Responsibilities:

Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure

What we’ll love about you

Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)

We’ll really swoon if you have

2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects

What you'll love about us

Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/",2019,Staffing & Subcontracting,$1 to $5 million (USD),Human Resources & Staffing,1 to 50 Employees,Company - Private,True
Data Center Services Electrical Commissioning Engineer II,"CAI
","Chicago, IL",$90K - $115K (Employer est.),4.1,"CAI seeks DC Services Electrical Commissioning Engineers with a minimum of two years' experience in Data Center Commissioning to support development and execution of all electrical aspects of commissioning projects.

Position Description:
This position supports development and execution of all electrical aspects of assigned commissioning projects from initial engagement, design reviews, checklists, safety support, script development, vendor coordination, testing and report development through turn over to the client. The Electrical Commissioning Engineer will support the development of the electrical test schedule, finalize electrical test procedures, review project submittals for consistency with the design intent, basis of design and the owner’s project requirements, and maintain project cadence for electrical systems testing and associated Building Automation Systems. The Electrical Commissioning Engineer is to support the planning and execution of commissioning for the electrical infrastructure of the mission critical facility. They will be expected to execute against the project schedule through the coordination of contractors and/or vendors to complete the desired electrical systems testing.
CAI DC Electrical Commissioning Engineer will be exposed to cutting edge technologies in the Hyperscale and other spaces. You will have an opportunity to work with recognized subject matter experts allowing YOU to be a key player in bringing data technologies to market. As part of our company culture, we invest in YOUR future, and commit to hands on certifications as well as professional training. Our collaborative culture ensures that our customers benefit from exemplary work across our entire range of professional services.

Responsibilities:
Support all aspects of safety for all electrical tests.
Support complete commissioning and performance acceptance testing of the electrical infrastructure systems.
QA/QC of all electrical test procedures.
Provide input and insight to the overall commissioning plan.
Develop reports for the electrical commissioning engineers and contribute to a daily report to the Commissioning Project Manager.
Attend and be an active participant of customer equipment Factory Witness Test
Assist with vendor coordination and management.
Perform equipment inspection to ensure build adherence to vendor submittal.
Provide test documentation that equipment is delivered, installed, and tested correctly and set to function properly for the customer.
Support and perform design specification review, manufacturer submittals, one line drawing sets, and project schedule documentation.
QA/QC of electrical equipment installation\startup
Execute test scripts to confirm equipment and system operation to design specification.
Ensure safe work practices are followed by all on commissioning team and customer site.
Engage with customers to ensure a positive experience, goals achievement, and schedule adherence.
Provide daily reports for electrical commissioning team status.
Conduct facility walk downs, turnover, and punch list reviews.
General understanding of LEED specifications and requirements.
Look for new opportunities for CAI to provide service and value to customer.
Duties may be increased as experience and skill allow.

Requirements include:
Position Requirements:
Bachelor’s degree or equivalent experience
Minimum of 2 years Data Center Commissioning experience.
Knowledge of OSHA and NFPA 70E safety requirements.
Good written and spoken communication skills.
Ability to read and interpret electrical schematics and specifications.
Knowledge of data center design concepts.
Knowledge and commissioning experience with Electrical Distribution Switchgear, Substations, Uninterruptable Power Supplies (UPS), Automatic Transfer Switches (ATS), Batteries, Emergency Diesel Generators & Load Banks.
Knowledge of power quality analysis.
Strong experience with Word, Excel and PowerPoint.
Ability to effectively write electrical commissioning scripts, daily reports, and final commissioning reports.

Other Requirements:
Excellent oral and written English is required
Extensive travel may be required (75%)
Candidates must have a Passport or the ability to immediately get a Passport.
Work under construction site conditions
Able to work in the US without sponsorship now or any time in the future.

About CAI
CAI is a 100% employee-owned company established in 1996, that has grown year over year to more than 800 people worldwide. We provide commissioning, qualification, validation, start-up, project management and consulting services related to operational readiness to FDA regulated and other mission critical industries.

Meeting a Higher Standard
Our approach is simple; we put the client’s interests first, we do not stop until it is right, and we will do whatever it takes to get there.
As owners of CAI, we are committed to living our Foundational Principles, both professionally and personally:
We act with integrity
We serve each other
We serve society
We work for our future

With employee ownership, one person’s success is everyone’s success; we work diligently to accomplish team goals. We place Team Before Self, demonstrate Respect for Others, and possess a can-do attitude. That is how we have grown exponentially.



Benefits
Our full-time positions offer competitive compensation and benefits which include: up to 15% retirement contribution, 24 days PTO and 5 sick days per year, health insurance at extremely low cost to employee, financial support for both internal and external professional education as well as 70% long term disability paid for by the company.
#LI-MR1
Average salary range, not including benefits or compensatory time and possible discretionary bonuses.
We are an equal opportunity employer; we are proud to employ veterans and promote a diverse culture in our workplace. Diversity is a strength for our global company. We pledge that CAI will be operated in a way that is fair and equitable to all – our employees, our customers, and the broader society.
This job description is not all inclusive and you may be asked to do other duties. CAI will also consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the FCO.",1996,Architectural & Engineering Services,$25 to $100 million (USD),"Construction, Repair & Maintenance Services",501 to 1000 Employees,Company - Private,True
