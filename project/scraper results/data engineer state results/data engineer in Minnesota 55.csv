Job Title,Company Name,Location,Salary Estimate,Rating,Job Description,Founded,Industry,Revenue,Sector,Size,Type,Easy Apply
"Sr Network Data Center Engineer (Saint Paul, MN)","Patterson
","Saint Paul, MN",$69K - $97K (Glassdoor est.),3.8,"Patterson isn't just a place to work, it's a partner that cares about your success.

One of the distinguishing marks of our company is the talented people who embrace the people-first, always advancing, and results-driven culture. Professional growth abounds in this motivating environment. We value the diverse talents and experiences our employees bring to Patterson and believe that they build a stronger and successful organization.

Job Description:

The Sr Network Engineer is responsible for driving architectural decisions and detailed design activities for Patterson’s network infrastructure. The Sr Network Engineer collaborates with other engineering team members, technology architects, operations staff, developers, and business leads to develop and deliver strategic solutions for the Patterson enterprise. The areas of focus for this role include all aspects of network infrastructure architecture, implementation, process development, and technology integration. This role collaborates on the creation and maintenance of policies and assists with the development of standards, guidelines, and procedures that are consistent with company goals, industry best practices, and regulatory requirements.


Essential Functions

Responsible for design, implementation, and integration of data center, cloud, and end-user network architectures. Works with leadership, architects, and infrastructure teams to analyze and improve current data center networks, WAN solutions, LAN solutions, and cloud environments.

Drives the development of standards, guidelines, and processes for networks, considering redundant systems, policies, capacity planning and high availability

Works with Project Management, architecture, business, technical, and security teams to achieve project delivery. Collaborates with cross-functional teams to ensure alignment and resolve concerns.

Provides escalation support for data center, cloud and core routing platforms.

Provides processes and guidance for tier 1 and 2 staff. Completes root cause analysis of problems and provides potential solutions in area of expertise.

Acts as a leader and advocate of technology stewardship and management, including mentoring, coaching, and training.

Required Qualifications

Bachelor’s degree in related field or equivalent combination of experience and education

At least 5 years’ experience in a Network Engineer role, working with multiple data center environments, network analysis, packet capture and monitoring tools (Wireshark, Observer, SolarWinds, Gigamon, LiveAction and Cisco Modeling Labs).

At least 3 years’ experience with designing/supporting cloud security solutions (AWS, Azure, GCP).

Experience with Express Route a plus.

Routing Protocol knowledge (BGP, OSPF, EIGRP) and experience with QOS.

Expert understanding of architecture design techniques, theories, principles, and practices.

Performance analysis, tuning and optimization on various hardware platforms in a large enterprise environment. Viavi Gigastor and Gigamon experience a plus. Experience as an escalation engineer with a high level of comfort when debugging complex issues.

Proven analytical and problem-solving abilities.

Excellent written and verbal communication skills.

Ability to present ideas in business-friendly and user-friendly language to all levels of management and staff. Ability work on-call and alternative hour schedules when required.

PCI Audit Knowledge.

Highly self-motivated and directed, with keen attention to detail.

Able to effectively prioritize tasks in a high-pressure environment.

Strong customer service orientation.

Experience working in a team-oriented, collaborative environment.

Preferred Qualifications

Prior experience with common network scripting programming languages.

Prior experience with common automation tools such as Ansible, Puppet, Chef.

Prior experience with API integrations.

Prior experience engineering networks with more than 2000 devices.

Prior experience with Cisco technologies/solutions including Nexus 7000/6000/5000/2000, LAN controllers, fixed-configuration switches, and Cisco ISR and ASR series routers.

Experience with Cisco ACI and SD-WAN/SDN.

Prior experience with the DNS/DHCP.

Experience managing partner and vendor relationships to achieve business outcomes.

Prior experience in a global enterprise.

This role is open to primarily remote work with the ability to travel to data centers or our corporate office in Minnesota when needed.

What's In It For You:

We provide competitive benefits, unique incentive programs and rewards for our eligible employees:

Full Medical, Dental, and Vision benefits and an integrated Wellness Program.

401(k) Match Retirement Savings Plan.

Employee Stock Purchase Plan (ESPP).

Paid Time Off (PTO).

Holiday Pay & Floating Holidays.

Volunteer Time Off (VTO).

Educational Assistance Program (Tuition Reimbursement).

Full Paid Parental and Adoption Leave.

LifeWorks (Employee Assistance Program).

Patterson Perks Program.


The potential compensation range for this role is below. The final offer amount could exceed this range, based on various factors such as candidate location (geographical labor market), experience, and skills.

$93,100.00 - $117,400.00

EEO Statement


As a people-first company, Patterson promotes a culture that embodies and celebrates diversity and inclusivity. We believe our employees’ unique experiences and differences is what strengthens us and drives our success. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status.


We are Patterson. We welcome you.",1877,Wholesale,$5 to $10 billion (USD),Retail & Wholesale,5001 to 10000 Employees,Company - Public,False
"Lead Engineer - Big Data Platform/Infra (Hadoop, Spark Streaming, Druid)","Target
","Brooklyn Park, MN",$109K - $196K (Employer est.),3.5,"Location: 7000 Target Pkwy N, Brooklyn Park, Minnesota, United States, 55445

The pay range is $109,000.00 - $196,200.00

Pay is based on several factors which vary based on position. These include labor markets and in some instances may include education, work experience and certifications. In addition to your pay, Target cares about and invests in you as a team member, so that you can take care of yourself and your family. Target offers eligible team members and their dependents comprehensive health benefits and programs, which may include medical, vision, dental, life insurance and more, to help you and your family take care of your whole selves. Other benefits for eligible team members include 401(k), employee discount, short term disability, long term disability, paid sick leave, paid national holidays, and paid vacation. Find competitive benefits from financial and education to well-being and beyond at https://corporate.target.com/careers/benefits.

JOIN US AS A LEAD ENGINEER – BIG DATA PLATFORM

About us:

As a Fortune 50 company with more than 350,000 team members worldwide, Target is an iconic brand and one of America's leading retailers. Working at Target means the opportunity to help all families discover the joy of everyday life. Caring for our communities is woven into who we are, and we invest in the places we collectively live, work and play. We prioritize relationships, fuel and develop talent by creating growth opportunities, and succeed as one Target team. At our core, our purpose is ingrained in who we are, what we value, and how we work. It’s how we care, grow, and win together.

The Target High Performance Distributed Computing team creates the platforms and tools to enable our business partners to make data-based decisions at Target. This team helps to manage hardware and software for large scale distributed computing, frequently angling towards data analytics and Artificial Intelligence/Machine Learning type applications. We help develop the technology that personalizes the guest experience, from product recommendations to relevant ad content. We’re also the source of the data and analytics behind Target's Supply Chain optimization, fraud detection, demand forecasting (DFE) and metrics to support our stores. We play a key role in identifying the test-and-measure or A/B test opportunities that continuously help Target improve the guest experience, whether they love to shop in stores or Target.com.

As a Lead Engineer, you serve as the technical anchor for the engineering team that supports a product. You create, own and are responsible for the application and platform architecture that best serves the product in its functional and non-functional needs. You'll bring innovative ideas and help set the strategy for the future of our platform. You love keeping abreast of the latest industry trends and use them to help you innovate. You have leadership qualities, good judgment, and clear communication skills. If you’re excited to work on a fast-moving, tightly knit team and build solutions to unsolved problems, we want to meet you.

As a Lead Engineer, you’ll take the lead as you…

Understand Target's business and technical environments and assist teams in resolving complex business challenges via current technical solutions by assessing viability/applicability/cost implication through POCs and prototypes.
Collaborate with technical staff and Enterprise Architecture teams in setting technical direction across platform and drive technology lifecycle management and communication of standards/decisions to the engineering team.
Participate in procurement specifications, installation, and maintenance of Target systems.
Lead designing and building the Target platform API with deep focus on non-functional requirements including scalability, availability, performance, etc. while being a strong advocate of extreme agile and DevOps practices across engineers.

About You:

BS/MA degree in Computer Science or relevant experience
5+ years of experience in developing software applications
Detailed knowledge of GNU/Linux OS experience w/ administration of production grade services running on Linux servers
Proven track record in writing code that is correct, maintainable, testable, expressive, easy to change, efficient and fault-tolerant
Demonstrated proficiency in Java
Demonstrated knowledge of some of the following concepts:
Operating system architecture, memory management, process scheduling, I/O scheduling
Networking, technologies, latency, bandwidth
Benchmarking, performance debugging, performance monitoring
Limiting-resource identification
Have familiarity and experience with some of the following:
Hadoop (multi-node fully distributed Hadoop clusters)
Spark
HDFS
Hive
ZooKeeper
Ozone
Trino/PrestoSQL
Possess a strong understanding of high-performance, large-scale system architecture design and implementation
Experience with distributed and parallel processing, computer architecture, operating systems, synchronization, communication
Experience with modern CI/CD technologies such as Git, Drone, Docker, Artifactory
Understand business fundamentals and how technologies can support business goals along with how to translate business vision into a technical strategy and financial implications
Strong team player who understands concepts of teamwork and team effectiveness.
Have excellent verbal, written, and presentation communication skills to convey complex technical solutions clearly to an organization
Have excellent planning and organizational skills

This position will operate as a Hybrid/Flex for Your Day work arrangement based on Target’s needs. A Hybrid/Flex for Your Day work arrangement means the team member’s core role will need to be performed both onsite at the Target HQ MN location the role is assigned to and virtually, depending upon what your role, team and tasks require for that day. Work duties cannot be performed outside of the country of the primary work location, unless otherwise prescribed by Target. Click here if you are curious to learn more about Minnesota.

Americans with Disabilities Act (ADA)

Target will provide reasonable accommodations with the application process upon your request as required to comply with applicable laws. If you have a disability and require assistance in this application process, please visit your nearest Target store or Supply Chain Facility or reach out to Guest Services at 1-800-440-0680 for additional information.",1962,General Merchandise & Superstores,$10+ billion (USD),Retail & Wholesale,10000+ Employees,Company - Public,False
Data Engineer - 5050304,"Accenture
","Minneapolis, MN",-1,3.9,"Accenture Flex offers you the flexibility of local fixed-duration project-based work powered by Accenture, a leading global professional services company. Accenture is consistently recognized on FORTUNE's 100 Best Companies to Work For and Diversity Inc's Top 50 Companies For Diversity lists.

As an Accenture Flex employee, you will apply your skills and experience to help drive business transformation for leading organizations and communities. In addition to delivering innovative solutions for Accenture's clients, you will work with a highly skilled, diverse network of people across Accenture businesses who are using the latest emerging technologies to address today's biggest business challenges.

You will receive competitive rewards and access to benefits programs and world-class learning resources. Accenture Flex employees work in their local metro area onsite at the project, significantly reducing and/or eliminating the demands to travel.

Utilize strong SQL Python skills to engineer sound data pipelines and conduct routine and ad hoc analysis to assess the performance of legacy products and the saliency of new features.

Build reporting dashboards and visualizations to design, create and track campaign program KPIs.

Perform analyses on large data sets to understand drivers of operational efficiency.

Manage end to end process of analytic tooling feature development, including request intake, requirements evaluation, cross functional team alignment, feature execution, QA testing, and stakeholder communication.

Consult with business analysts, technical data SMEs, and cross functional partners to understand their reporting and data needs, serving as the point of contact for requests, inquiries, and actions.

Interface with other data engineering, product, and data science teams to implement client needs and initiatives.




Basic Qualifications:

Minimum 2 years experience with Python.

Minimum 3 years experience SQL.

Minimum 3 years experience Big Data Analytics.

High School Diploma or GED.

Preferred Qualifications:

Experience with large enterprise data sets.

Bachelors Degree

Compensation at Accenture varies depending on a wide array of factors, which may include but are not limited to the specific office location, role, skill set and level of experience. As required by local law, Accenture provides a reasonable range of compensation for roles that may be hired in California, Colorado, New York, or Washington as set forth below.

Information on benefits is here.

Role Location

California: $61.53 to $71.53/hour

Colorado: $61.53 to $71.53/hour

New York: $61.53 to $71.53/hour

Washington: $61.53 to $71.53/hour


What We Believe


We have an unwavering commitment to diversity with the aim that every one of our people has a full sense of belonging within our organization. As a business imperative, every person at Accenture has the responsibility to create and sustain an inclusive environment.


Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and more creative, which helps us better serve our clients and our communities. Read more here


Equal Employment Opportunity Statement


Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation.


All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.


Accenture is committed to providing veteran employment opportunities to our service men and women.


For details, view a copy of the Accenture Equal Employment Opportunity and Affirmative Action Policy Statement.


Requesting An Accommodation


Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired.


If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter.


Other Employment Statements


Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.


Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.


Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.


The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information.",1989,Business Consulting,$10+ billion (USD),Management & Consulting,10000+ Employees,Company - Public,False
Data Quality Assurance Engineer (Hybrid),"Farm Credit Network
","Lakeville, MN",$59K - $81K (Glassdoor est.),4.7,"Compeer Financial is seeking collaborative, innovative and dynamic professionals to be a part of our Top Workplace culture!

Tell me more about this opportunity.

Position Overview:
This position evaluates and performs functional, integration and regression testing for various data-related engagements. Supports Business Data Analysts, ETL Developers and other stakeholders in analysis of business needs, process improvements and new technologies. Develops procedures to efficiently create and execute test plans, track defects and resolve identified issues. Develops automated test scripts and maintains an automation framework using QA software tools and processes. Provides subject matter expertise and mentorship across the organization. Proactively identify problems with acceptance criteria (lack of clarity, inconsistencies, technical limitations) and communicates these issues early.

Essential Functions:

Collaborates with Business Systems Analysts, ETL Developers, and other stakeholders to review project requests, participate in design sessions and determine testing goals.
Estimates, prioritizes, plans and coordinates testing activities.
Utilizes various testing methods to ensure quality, including black box, white box, regression, and automated testing.
Writes, executes and maintains both manual and automated test cases for functional, integration and regression testing which includes translating both simple and complex business rules to SQL queries and executing those queries to validate data quality and accuracy and ensure that it meets agreed-upon requirements and applicable QA standards.
Provides testing status updates to the project team.
Manages and reports test results which will require clear and concise documentation and an ability to conduct root cause analysis and communicate complex information to both technical and non-technical personnel.
Tracks data issu es and works with team leads from discovery to resolution.
Collaborates with reporting and analytics teams to conduct data quality investigations.
Maintains test environments to include managing test data, configuring systems for specific test needs, ensuring compliance with standards.
Leads efforts to develop, implement and review quality processes and methodologies.
Participates with cross-functional Data and Business Technology teams to formulate best practices.
Researches new testing tools, technologies and strategies.
Facilitates meetings with Data, Project Delivery and/or business unit team members.

Minimum Qualifications & Required Knowledge, Skills and Abilities:

Bachelor’s degree in math, computer science, management information systems, or related field or an equivalent combination of education and experience sufficient to perform the essential functions of the job.
Minimum of 3 years of experience in a software or data development environment as an application/ETL tester, business analyst, technical writer or developer.
Knowledge of software testing methodologies, tools and best practices.
Knowledge of business applications, process improvement, project management systems, and agile methodology.
Solid knowledge and experience in writing clear, concise and comprehensive test plans and test cases.
Solid knowledge in writing complex SQL queries on large tables (millions of records) and ensure data integrity is maintained throughout the ETL lifecycle.
Specialized experience with data warehouses, APIs, ETL testing and SQL scripting.
Knowledge of queries, scripting and coding frequently used in QA Automation tools.
Experience creating automated test scripts using QA software tools and processes.
Knowledge of effective automation test design and development methods.
Effective communication skills t o clearly and concisely convey technical issues to customers and to share ideas, solutions and feedback with team.
Effective interpersonal, analytical, organizational and planning skills.
Ability to apply a methodical and logical approach to problem solving.
Critical thinking skills with attention to detail and accuracy.
Quick learner who thrives in a collaborative team environment.
Ability to coordinate tasks to meet established deadlines.
Ability to foster collaborative working relationships across the organization.
Ability to train and mentor others.
Demonstrates initiative to remain technically competent by keeping abreast of industry best practices.
Ability to work independently and collaboratively with other teams to achieve goals and represent the business.

Who is Compeer Financial?
Compeer Financial exists to champion the hopes and dreams of rural America. By joining our team, you will help empower those in agriculture and rural communities to achieve their goals and expand their possibilities. We embrace business agility and innovative approaches to serving our clients and communities.

Why join our team?

Amazing team members who are passionate about serving agriculture and rural America.
Investment in our team members’ education, growth and development.
Engagement in our communities through giving back and volunteerism.
Flexible, collaborative and dynamic work environment.
Great benefits:

Medical, Dental, Vision insurance
401K (3% Compeer contribution & up to an additional 6% match)
Paid time off (vacation, sick leave, holidays, volunteer time)

Find out why our team members choose Compeer Financial by watching this video.

How do I apply?
Qualified candidates, please apply online at www.compeer.com/careers.

Compeer Financial is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.",-1,Banking & Lending,$25 to $100 million (USD),Financial Services,51 to 200 Employees,Company - Public,False
Data Engineer,Tail Wind Informatics,Minnesota,$90K - $110K (Employer est.),-1.0,"About Us:
Tail Wind Informatics Corporation -Microsoft Solutions Partner- is a dynamic and innovative IT consulting services company that specializes in delivering Data Architecture and Business Intelligence solutions. We are currently seeking a talented and experienced Data Engineer to join our team and play a crucial role in managing and optimizing data infrastructure. If you are passionate about data, have a strong background in Microsoft technologies, and enjoy solving complex data challenges, we want to hear from you!

Position Overview:
As a Data Engineer at Tail Wind, you will be responsible for designing, developing, and maintaining data pipelines and systems on the Azure cloud platform. You will work closely with cross-functional teams to ensure that data is accurate, accessible, and supports data-driven decision-making. The ideal candidate has hands-on experience with Azure Data Factory, SQL, Stored Procedures, Python, Databricks, Snowflake, Azure Synapse Analytics, and Power BI.
Key Responsibilities:
Design, build, and maintain data pipelines and ETL processes using Azure Data Factory.
Develop and optimize SQL queries, stored procedures, and scripts for data transformation and extraction.
Understand data requirements and ensure data availability.
Implement data quality checks and data validation processes to ensure data accuracy and consistency.
Leverage Python for data manipulation, automation, and data integration tasks.
Utilize Databricks for advanced data processing, transformation, and analytics.
Manage and optimize data storage.
Work with Azure Synapse Analytics or Snowflake to build and maintain data warehouses and analytics solutions.
Create interactive reports and dashboards using Power BI for data visualization and insights.
Stay up-to-date with the latest data technologies and best practices.
Qualifications:
Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience).
3+ years of experience as a Data Engineer with a focus on Microsoft technologies.
Strong proficiency in Azure Data Factory, or related product, for ETL processes and data orchestration.
Proficiency in SQL, including the ability to write complex queries and stored procedures.
Experience with Python for data manipulation and automation.
Knowledge of Databricks for big data processing and analytics.
Familiarity with Snowflake or Azure Synapse Analytics for data warehousing.
Proficiency in Power BI for data visualization and reporting.
Strong problem-solving skills and attention to detail.
Excellent communication and teamwork abilities.
Why Join Tail Wind?
Competitive salary and benefits package.
Opportunity to work with cutting-edge technologies and solve challenging data problems.
Collaborative and innovative work environment.
Professional development opportunities and support for various certifications.
Application Process:
To apply for this position, please click ""apply for this job"" at the top of the page, then upload and submit a current copy of your resume.
*Salary will be determined based on the results of a Technical Interview*
Tail Wind Informatics Corp., is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive, healthy environment for all employees. We look for people that love what they do, want to learn, earn and enjoy life to the fullest.

No agencies please.",-1,-1,Unknown / Non-Applicable,-1,1 to 50 Employees,Company - Public,True
Data Quality Assurance Engineer,"Compeer Financial
","Lakeville, MN",$61K - $86K (Glassdoor est.),4.3,"Data Quality Assurance Engineer (Hybrid)
Job Category: Technology
Employment Type: Full-Time Regular
Location: MN - Lakeville ; IL - Bloomington ; MN - Mankato ; WI - Sun Prairie





Email a Friend

Save

Apply Now

Compeer Financial is seeking collaborative, innovative and dynamic professionals to be a part of our Top Workplace culture!

Tell me more about this opportunity.

Position Overview:
This position evaluates and performs functional, integration and regression testing for various data-related engagements. Supports Business Data Analysts, ETL Developers and other stakeholders in analysis of business needs, process improvements and new technologies. Develops procedures to efficiently create and execute test plans, track defects and resolve identified issues. Develops automated test scripts and maintains an automation framework using QA software tools and processes. Provides subject matter expertise and mentorship across the organization. Proactively identify problems with acceptance criteria (lack of clarity, inconsistencies, technical limitations) and communicates these issues early.

Essential Functions:

Collaborates with Business Systems Analysts, ETL Developers, and other stakeholders to review project requests, participate in design sessions and determine testing goals.
Estimates, prioritizes, plans and coordinates testing activities.
Utilizes various testing methods to ensure quality, including black box, white box, regression, and automated testing.
Writes, executes and maintains both manual and automated test cases for functional, integration and regression testing which includes translating both simple and complex business rules to SQL queries and executing those queries to validate data quality and accuracy and ensure that it meets agreed-upon requirements and applicable QA standards.
Provides testing status updates to the project team.
Manages and reports test results which will require clear and concise documentation and an ability to conduct root cause analysis and communicate complex information to both technical and non-technical personnel.
Tracks data issues and works with team leads from discovery to resolution.
Collaborates with reporting and analytics teams to conduct data quality investigations.
Maintains test environments to include managing test data, configuring systems for specific test needs, ensuring compliance with standards.
Leads efforts to develop, implement and review quality processes and methodologies.
Participates with cross-functional Data and Business Technology teams to formulate best practices.
Researches new testing tools, technologies and strategies.
Facilitates meetings with Data, Project Delivery and/or business unit team members.

Minimum Qualifications & Required Knowledge, Skills and Abilities:

Bachelor’s degree in math, computer science, management information systems, or related field or an equivalent combination of education and experience sufficient to perform the essential functions of the job.
Minimum of 3 years of experience in a software or data development environment as an application/ETL tester, business analyst, technical writer or developer.
Knowledge of software testing methodologies, tools and best practices.
Knowledge of business applications, process improvement, project management systems, and agile methodology.
Solid knowledge and experience in writing clear, concise and comprehensive test plans and test cases.
Solid knowledge in writing complex SQL queries on large tables (millions of records) and ensure data integrity is maintained throughout the ETL lifecycle.
Specialized experience with data warehouses, APIs, ETL testing and SQL scripting.
Knowledge of queries, scripting and coding frequently used in QA Automation tools.
Experience creating automated test scripts using QA software tools and processes.
Knowledge of effective automation test design and development methods.
Effective communication skills to clearly and concisely convey technical issues to customers and to share ideas, solutions and feedback with team.
Effective interpersonal, analytical, organizational and planning skills.
Ability to apply a methodical and logical approach to problem solving.
Critical thinking skills with attention to detail and accuracy.
Quick learner who thrives in a collaborative team environment.
Ability to coordinate tasks to meet established deadlines.
Ability to foster collaborative working relationships across the organization.
Ability to train and mentor others.
Demonstrates initiative to remain technically competent by keeping abreast of industry best practices.
Ability to work independently and collaboratively with other teams to achieve goals and represent the business.

Who is Compeer Financial?
Compeer Financial exists to champion the hopes and dreams of rural America. By joining our team, you will help empower those in agriculture and rural communities to achieve their goals and expand their possibilities. We embrace business agility and innovative approaches to serving our clients and communities.

Why join our team?



Amazing team members who are passionate about serving agriculture and rural America.
Investment in our team members’ education, growth and development.
Engagement in our communities through giving back and volunteerism.
Flexible, collaborative and dynamic work environment.
Great benefits:
Medical, Dental, Vision insurance
401K (3% Compeer contribution & up to an additional 6% match)
Paid time off (vacation, sick leave, holidays, volunteer time)

Find out why our team members choose Compeer Financial by watching this video.

How do I apply?
Qualified candidates, please apply online at www.compeer.com/careers.

Compeer Financial is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.",1916,Banking & Lending,$10+ billion (USD),Financial Services,1001 to 5000 Employees,Company - Private,False
Data Engineer,"Brightree
","Bloomington, MN",$84K - $120K (Glassdoor est.),3.4,"ResMed has always applied the best of technology to improve people's lives. Now our SaaS technology is fueling a new era in the healthcare industry, with dynamic systems that change the way people receive care in settings outside of the hospital–and tools that work every day to help people stay well, longer. We have one of the largest actionable datasets in the industry, creating a complete view of people as they move between care settings. This is how we empower providers–with vital insight to deliver the care people need, right when they need it.
We're also ensuring that our health solutions connect to other companies' networks. Because when objectives align, everyone wins. And as we work today to drive better care and lower costs, we're developing more personalized solutions for tomorrow, utilizing machine learning, intelligent care paths, and predictive protocols. If you are an innovator who wants to make an impact we want to talk to you! We have exciting opportunities supporting Brightree by ResMed and MatrixCare by ResMed!
We are looking for a savvy Data Engineer to join our growing team of platform experts. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.
You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The goal is to support our software developers, database architects, data analysts and data scientists on data initiatives to ensure optimal data delivery architecture is consistent throughout ongoing projects.
You are self-directed and comfortable supporting the needs of multiple teams, systems and products. You are excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. You are a team player who lifts the entire team through collaboration, mentoring and sharing your experience
Location - Open for ""Remote"" across US
Let's talk about the role
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and AWS ‘big data’ technologies like Glue, Lambda, EMR etc
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions
Work on data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems
Let's talk about you
Must have previous experience creating/running Python
Excellent Python coding knowledge or Pyspark jobs working within AWS Glue
You will have 4+ years of total experience in a full cycle Data Engineer role, who has attained a graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field or equivalent working experience
Experience building and working with AWS Data Lakes
Experience working on GitHUB CI/CD processes
You will also be strong in
Experience with big data tools: Snowflake, Hadoop, Spark, Kafka, etc
Experience with data pipeline and workflow management tools: Luigi, Airflow, AWS Step etc
Experience with AWS cloud services: EMR, RDS, Redshift
Experience with DBT \ Coalesce or other ELT tools
Experience with stream-processing systems: Kinesis, Spark-Streaming, etc
Experience with Docker Containers and Kubernetes
Familiarity with a variety of datasets, structured, semi structured and unstructured etc
Experience building and optimizing ‘big data’ data pipelines, architecture and data sets
Nice to have Experience with MS SQL Stack and SSIS
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Experience supporting and working with cross-functional teams in a dynamic environment
#LI - Tech
#LI - Remote
Joining us is more than saying “yes” to making the world a healthier place. It’s discovering a career that’s challenging, supportive and inspiring. Where a culture driven by excellence helps you not only meet your goals, but also create new ones. We focus on creating a diverse and inclusive culture, encouraging individual expression in the workplace and thrive on the innovative ideas this generates. If this sounds like the workplace for you, apply now!",2002,Enterprise Software & Network Solutions,$100 to $500 million (USD),Information Technology,501 to 1000 Employees,Company - Public,False
"AWS Data Engineer--Minneapolis, MN(Onsite)",i2vision,"Minneapolis, MN",$50.00 Per Hour (Employer est.),-1.0,"Position: AWS Data Engineer

Location: Minneapolis, MN(Onsite)

Duration: 12+ Months Contract

JD:

Data Architecture, Data Modeling, AWS Glue, Dynamo DB, Aurora, DMS, RedShift

Job Type: Contract

Salary: $50.00 per hour

Expected hours: 40 per week

Experience level:

6 years

Schedule:

Monday to Friday

Experience:

Dynamo DB: 6 years (Preferred)
Data Architecture: 5 years (Preferred)
Data modeling: 6 years (Preferred)
AWS Glue: 6 years (Preferred)

Work Location: In person",2014,-1,$1 to $5 million (USD),-1,1 to 50 Employees,Company - Public,True
ERP Data Engineer,"University of Minnesota
","Minneapolis, MN",$85K - $115K (Glassdoor est.),4.2,"Bachelor’s degree in Computer Science, IT, Engineering (any), or related
60 months of experience as a software professional.

ERP Data Engineer needed to conduct Data Pipeline Development and PeopleSoft integration. Design, develop and deploy new APIs, event streams, and file based integrations using Dell Boomi, Oracle SQL, Apache Kafka, Amazon SQS, and other data integration technologies. Support and leverage change data capture techniques on thePeopleSoft Oracle database to feed ERP data into downstream systems. Create Azure DataPipelines using the Microsoft Azure toolkit. Model system agnostic data structures based to abstract away PeopleSoft specific source data structures. Implement data structures (SQL and no-SQL) on Oracle and Microsoft Azure data platforms. Implement and maintain data securityactivities including role, account, and secret management as well as regular access review. Monitor and maintain production integrations and data flows. Evaluate and follow through on issues and problems until resolved. Integrate third-party applications with the PeopleSoft system using PeopleSoft integration technologies such as Integration Broker, Web Services, Application Engine, Component Interface, File Layout and File Parser. Perform technical configuration in PeopleSoft related to technologies or frameworks like Admission Application Web Services (AAWS), Integration Broker, File Parser and Approval Framework. Deployintegration related customizations to PeopleSoft application environments. Maintain PeopleSoft security in development instances using User Profile, Roles, Permission List, Query Trees and row level security assignment.

Office of Information Technology:

Interested in a career with one of the nation’s largest universities? The University of Minnesota is an institution dedicated to changing lives through research, education, and outreach. The Office of Information Technology (OIT) - the University’s central IT department - works to support and advance this mission and to support overall academic advancement. Our dedicated IT professionals connect students, faculty, and staff with innovative services to meet their teaching, learning, research, and administrative needs. The Office of Information Technology offers an environment of trust, collaboration, and mission-focused work. Join us and support innovation and engagement through technology!


Applications must be submitted online. To be considered for this position, please click the Apply button and follow the instructions. You will be given the opportunity to complete an online application for the position and attach a cover letter and resume.

Additional documents may be attached after application by accessing your ""My Job Applications"" page and uploading documents in the ""My Cover Letters and Attachments"" section.

To request an accommodation during the application process, please e-mail employ@umn.edu or call (612) 624-UOHR (8647).

The University recognizes and values the importance of diversity and inclusion in enriching the employment experience of its employees and in supporting the academic mission. The University is committed to attracting and retaining employees with varying identities and backgrounds.

The University of Minnesota provides equal access to and opportunity in its programs, facilities, and employment without regard to race, color, creed, religion, national origin, gender, age, marital status, disability, public assistance status, veteran status, sexual orientation, gender identity, or gender expression. To learn more about diversity at the U: http://diversity.umn.edu

The University of Minnesota, Twin Cities (UMTC)

The University of Minnesota, Twin Cities (UMTC), is among the largest public research universities in the country, offering undergraduate, graduate, and professional students a multitude of opportunities for study and research. Located at the heart of one of the nation's most vibrant, diverse metropolitan communities, students on the campuses in Minneapolis and St. Paul benefit from extensive partnerships with world-renowned health centers, international corporations, government agencies, and arts, nonprofit, and public service organizations.

At the University of Minnesota, we are proud to be recognized by the Star Tribune as a Top Workplace for 2021, as well as by Forbes as Best Employers for Women and one of America’s Best Employers (2015, 2018, 2019, 2023), Best Employer for Diversity (2019, 2020), Best Employer for New Grads (2018, 2019), and Best Employer by State (2019, 2022).",1851,Colleges & Universities,Unknown / Non-Applicable,Education,Unknown,College / University,False
Senior Data & Analytics Engineer,"Cargill
","Wayzata, MN",$102K - $134K (Glassdoor est.),4.0,"Want to build a stronger, more sustainable future and cultivate your career? Join Cargill's global team of 160,000 employees who are committed to safe, responsible and sustainable ways to nourish the world. This position is in Cargill’s food ingredients and bio-industrial business, where we anticipate trends around taste, nutrition and safety to innovate and provide solutions to manufacturers, retailers and foodservice companies.
Job Purpose and Impact

This role falls within the Cargill Bioindustrial business where we are unleashing nature, sustainable for a better tomorrow.

Join us as we build a global strategic pricing team in a high-growth business working to create a more sustainable future.

The Senior Data and Analytics Engineer will prepare business relevant information and reduce time to insight by leading technical activities that enable information capture, business intelligence and analytics competency. In this role, you will ensure teams have the data needed to make timely, accurate and actionable insights by investing in the core capabilities of data management, data engineering and key decision support applications that consume data. You will be a key partner between business needs and data engineering and will deliver the final information product including the dashboards that enable business value. Key Accountabilities
Support the deployment of the strategic pricing capabilities for the Cargill Bioindustrial Group aligned with strategies, market dynamics and analytical reporting needs. Design and implement internal process improvements, optimize data delivery and create scalable data sets. Build existing data assets and prepare data to meet specific reporting and analytics needs. Build data solutions to enable automated and self service data consumption. Model, design, develop, test and implement complex backend and front end structures to meet business visualization, reporting and analytics requirements. Evaluate the performance of processes, services and outcomes and recommend performance improvement. Partner collaboratively with data engineers and data scientists on complex infrastructure projects and advocate for and emphasize the business value of applications and offer mentorship and technical solutions to solve business problems. Independently handle complex issues with minimal supervision, while escalating only the most complex issues to appropriate staff. Other duties as assigned Minimum Qualifications
Bachelor’s degree in a related field or equivalent experience Confirmed skills of enabling data content for reporting or analytics solutions Minimum of four years of related work experience Preferred Qualifications
Certification in programing language/s Data and BI technology experience: Tableau, Snowflake, SQL, Power BI, relational databases, data warehousing Confirmed skills in querying relational databases with query language Confirmed experience working with large data stores

Equal Opportunity Employer, including Disability/Vet",1865,Crop Production,Unknown / Non-Applicable,Agriculture,10000+ Employees,Company - Private,False
Cloud Data Engineer,"Solution Design Group
","Minneapolis, MN",$80K - $114K (Glassdoor est.),4.5,"SDG is a high-performance software community. We are a team of collaborative and creative consultants who build and deliver custom software for some of the most recognizable local and national brands. In this role, you will be asked to leverage your current skills while also learning new ones. Working in over 500 different technologies and continually learning together, you can expect a one-of-a-kind and award-winning work experience. Our team at SDG has exceptional integrity and the desire to create the best possible customer solutions. We are proud to partner with our customers to consistently provide a successful working relationship as a high-performance team.

We are adding Data Engineers to influence the direction of data modernization for our customers. We are looking for engineers, senior engineers and leaders in data ecosystems, to join our growing Data Practice. This person will work closely with our customers to manage, transfer, manipulate, or integrate data to ensure exceptional performance for their business. You will be a part of a tight-knit technical community, giving you the opportunity to constantly grow your development amongst top technical talent. SDG is looking for a hard worker with a team-oriented mindset, that has at least 3 years of experience and an in-depth understanding of cloud data integration tools and cloud data warehousing. A successful Data Engineer at SDG is a lifelong learner who is motivated to continually improve their craft and thrives amongst their fellow employee-owners.
SDG’s Data Developers have proven experience with the following responsibilities. If you do too, we want to hear from you:
Design and development of relational and non-relational databases, data warehouses and data lakes using databases such as SQL Server, PostgreSQL, MySQL, AWS Aurora, AWS RDS, AWS DynamoDB, Azure SQL, Azure CosmosDB, AWS Redshift, AWS Lake Formation and Azure Synapse Analytics
Development, monitoring, and maintenance of data integration and processing tools and pipelines using tools such as SSIS, AWS Glue, AWS Dataspider and Azure Data Factory
Design and testing of machine learning and data science tools, including the integrating and deployment of data models using languages such as Python and R and tools including AWS Jupyter Notebooks
Implementation of high-capacity, real-time data streaming technologies using tools such as AWS Kinesis and Azure Event Hub
Data analysis and visualization tools including tools such as Tableau, PowerBI, AWS Athena, Azure Data Lake Analytics
Leveraging non-database data storage alternatives to maximize performance, costs and data quality including storage solutions such as AWS S3
Familiarity with security and security policies within AWS and Azure
Requirements:
3+ years of professional data development experience
Experience delivering solutions in large scale enterprise environments
Experience with Public Cloud Providers such as AWS and Microsoft Azure
Prior consulting or professional services experience is a bonus
Education degree or certification in Computer Science, or the equivalent related work experience
What's in it for you?
Full-time salaried consultant position
A true stake in success. SDG is an ESOP - 100% employee owned
Star Tribune Top Workplace winner the last 6 consecutive years
National Top Workplace in 2022
Engaged teammates who care about quality solutions
Challenging and rewarding work with great customers
Various opportunities to give back to the community
Be amongst some of the best technologists in the industry
Dedicated to our core values of superior customer service, exceptional employee experience, and responsible corporate citizenship
Opportunities to connect with other SDGer’s via internal communities, committees, and events – virtually and in person
Applicants must be authorized to work for ANY employer in the United States. We are unable to sponsor or take over sponsorship of an employment Visa at this time.

#LI-AS1
#LI-Hybrid",1989,Information Technology Support Services,$25 to $100 million (USD),Information Technology,201 to 500 Employees,Company - Private,True
Data Engineer,"Legrand AV
",United States,-1,3.7,"Thank you for your interest in becoming part of the team at Legrand!
GENERAL PURPOSE

Business Intelligence & Performance team at Legrand AV manages and delivers insights about company data so our executive-level stakeholders and businesses to make informed decisions. We are seeking a mid-senior level highly skilled and experienced Data Engineer to join our team. As a Data Engineer, you will play a critical role in developing and maintaining our data analytics infrastructure and data warehouse architecture. You will collaborate with cross-functional teams to design, build, and optimize data pipelines, ensuring the availability, reliability, and efficiency of our data ecosystem.

RESPONSIBILITIES
Act as the primary contact for everything related to the data integration and data warehouse.
Design, develop, and maintain scalable and efficient data pipelines and ETL processes to extract, transform, and load data from various sources into our reporting database/data warehouse.
Build a framework of repeatable solutions and playbooks enabling efficient and predictable data pipelines.
Have hands-on development experience in the implementation of an agile, cloud centric data warehousing and reporting platform
Collaborate with stakeholders to understand data requirements and translate them into technical specifications and solutions.
Implement data models, database schemas, and data transformation logic to support efficient data storage, retrieval, and analysis with proper data warehouse model and architecture.
Perform data cleansing, data quality assurance, and data validation to ensure accuracy and reliability of the data and outcomes.
Optimize data infrastructure, query performance, and data processing workflows to improve overall system efficiency and speed.
Work closely with data analysts and BI developers to provide them with reliable data for ingestion, analysis, and reporting purposes.
Ensure data security, privacy, and compliance with relevant regulations and best practices.
Monitor and troubleshoot data pipeline issues, resolve data-related anomalies, and perform root cause analysis.
Stay updated with industry trends, emerging technologies, and best practices in data engineering, data analytics, and data warehouse architecture.
Maintain query performance and tuning to ensure cost optimization.
REQUIREMENTS
Proven experience as a Data Engineer, working with large-scale data pipelines, data warehouse architecture, and ETL processes.
Solid understanding of data warehouse concepts, dimensional modeling, and schema design principles.
Experience with Microsoft Azure cloud computing platforms and services.
Experience with Azure data integration and orchestration tools, such as Data Factory, Databricks, or Synapse Analytics, for building end-to-end data pipelines or similar tools and technologies.
Knowledge of Azure analytics services like Azure Analysis Services, Azure Machine Learning, or Azure Data Explorer for advanced data analytics and insights or similar tools and technologies.
Familiarity with Azure DevOps or similar CI/CD tools for managing and deploying data integration code.
Strong analytical and problem-solving skills with attention to detail.
Excellent communication and collaboration skills to work effectively within cross-functional teams.

Minimum Education and Experience Required:
Bachelor's degree in Computer Science, Information Systems, or related field. Equivalent work experience will be considered.
5+ years developing Data Pipelines / Flows using ETL/ELT tools and technologies.
3+ years’ experience with a cloud data lake/warehouse solution (SQL Server, Snowflake, Databricks, Azure Data Warehouse, etc.). Hands-on experience working with data integration tools like SSIS, Data Factory, Matillion, and Informatica)
3+ years building complex Analytics and Reporting solutions such as Power BI, Tableau, Looker, etc.

Special Job Requirements:
Must be available for extended, varied work hours at times

WORKING CONDITIONS/PHYSICAL DEMANDS
While performing the duties of this job, the employee is regularly required to sit and make coordinated movements of the fingers for data entry on a keyboard.
General office environment
May require regular ground travel to other company facilities within local metropolitan area
Long-distance or air travel as needed – approximately 5% travel

Note: Nothing in this job description restricts management’s right to assign or reassign duties and responsibilities to this job at any time.

Legrand is proud to be an Equal Opportunity Employer. You will be considered for this position based upon your experience and education, without regard to race, color, religion, age, sex, national origin, sexual orientation, ancestry; marital, disabled or veteran status. We are committed to creating and maintaining a workforce environment that is free from any form of discrimination or harassment.
If you'd like to work in a fun, creative, business-casual environment that offers a comprehensive benefit package, we encourage you to apply!
Legrand is an equal employment opportunity employer.
For California residents, please see the link for the
Privacy Notice for Candidat
es
. California law requires that we provide you this notice about the collection and use of your personal Information.",1904,Electronics Manufacturing,$5 to $10 billion (USD),Manufacturing,10000+ Employees,Company - Public,False
Data Engineer/Data Analyst,High5Hire,Minnesota,$55.00 - $60.00 Per Hour (Employer est.),-1.0,"This is an onsite contractual position for 1 month in Minneapolis, MN, US. Please only apply if you are a US Citizen or a Greencard holder.

The pay rate for this position is $55-$60 per hour.

Required Experience: 5-10 years

We are seeking a Data Engineer who is eager to tackle the challenges of processing vast amounts of EHR data originating from multiple sources. You will need to develop a deep understanding of the data and drive efforts to maintain and improve data quality and usability. You should understand the importance and value of writing maintainable, documented, and well-tested code throughout the entire product lifecycle. Above all, you should be curious about what is possible in healthcare with the right tools and infrastructure.

Data Engineer to Build Advanced Search Capability using the Elasticsearch stack Applicant MUST have the following skills:
Elasticsearch stack knowledge (demonstrable experience in building search capability tooling using Elasticsearch).
Python programming knowledge and experience. Apache Spark, in particular with PySpark API, knowledge and experience.
Data pipeline experience. Excellent communication skills What Project/Projects will the candidate be working on while on assignment?
This data engineer will primarily be building/architecting and upgrading/augmenting a search tool using the Elasticsearch stack to search a big data volume of text documents. This person will be tasked to build a production level search application.
We need to build and keep up-to-date Elastic indices to allow users external to our group to be able to search the notes.

Primary Responsibilities:

Design and develop production level search application for easy searching of content within a big data medical free text data asset.
Work with EHR data across teams with ETL, NLP engineers and data scientists, researchers and clinicians to provide searching services with a high data quality control standard

Team Description: You would be part of a small core NLP Team with 15 core team members (data scientists, project manager, medical informaticists, data analysts) with support from 12 clinical annotators integrated into the team via a 3rd party vendor

What are the top 5-10 responsibilities for this position?

Demonstrable senior proficiency level and knowledge of the Elasticsearch stack.
Programming experience, including solid Python experience, following software engineering best practices.
Experience building and maintaining data pipelines and data assets.
Experience Building dashboards and user interfaces using Kibana or other visualization tools. Experience with distributed data processing frameworks such as Spark or MapReduce.
Experience as an individual contributor, hands-on developer, non-manager role executing on engineering projects as a primary job responsibility.

Demonstrated knowledge of data management best practices Main Technologies:

Currently the main technologies we are using are Apache Spark, Hadoop, Hive, Luigi, Python (and a little bit of Scala) and the platform we use is the on-prem Hadoop cluster.

Candidates should be solid with at least some of these technologies, and follow good engineering practices, such as testing, code reviews and putting in place monitoring systems like dashboards or alerts.

Preferred Qualifications:

Experience with dashboard development in Elasticsearch
Experience with data pipeline frameworks such as Airflow, Luigi or Oozie Experience with cloud-based computing (AWS or Azure) Familiarity with EHR data and standards (HL7 or FHIR)
Experience with non-relational data bases Experience with code and process documentation
Experience with explaining, educating, presenting and/or training non-engineers on engineering concepts and processes
Experience with continuous integration and delivery

Job Type: Contract

Salary: $55.00 - $60.00 per hour

Compensation package:

Hourly pay

Experience level:

5 years
6 years
7 years
8 years

Schedule:

Monday to Friday

Ability to commute/relocate:

MN, US: Reliably commute or planning to relocate before starting work (Required)

Application Question(s):

Are you a US Citizen or a Greencard Holder? (Yes/No) - Please answer this question as we will not be able to move forward with your profile if it is blank - thank you.

Experience:

Spark: 5 years (Required)
SQL: 5 years (Required)
Hadoop: 5 years (Required)

Work Location: In person",-1,-1,-1,-1,-1,-1,True
Azure Data Engineer,"Wipro Limited
","Minneapolis, MN",$88K - $123K (Glassdoor est.),3.1,"Overview:

Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading global information technology, consulting and business process services company. We harness the power of cognitive computing, hyper-automation, robotics, cloud, analytics and emerging technologies to help our clients adapt to the digital world and make them successful. A company recognized globally for its comprehensive portfolio of services, strong commitment to sustainability and good corporate citizenship, we have over 220,000 dedicated employees serving clients across six continents. Together, we discover ideas and connect the dots to build a better and a bold new future.

Role: Azure Data Engineer

Responsibilities:
Understand business requirements.
Understand source systems, source data and source data formats that are available on-prem / cloud.
Design and build data ingestion pipeline.
Design and build complex data processing pipelines.
Work with relevant stakeholders to assist with data-related technical issues and support their data needs.
Build programs for data quality checks.
Provide operational support.
Work with data architecture, data governance and data analytics. teams to ensure pipelines adhere to enterprise standards, usability, and performance.
Involve in System Testing, UAT, code deployment activities.
Coordinate with offshore team on regular basis

Experience Level
4-6 years of working experience in primary skills
Overall 8-10 years in ETL/Data Engineering

Primary Skills:
Azure Databricks
PySpark,
Scala

Secondary Skills
ADF
CICD
Airflow
SQL
Cloud Databases
Understanding of Agile methodologies

Wipro is an Equal Employment Opportunity employer and makes all employment and employment-related decisions without regard to a person's race, sex, national origin, ancestry, disability, sexual orientation, or any other status protected by applicable law.",1945,Information Technology Support Services,$5 to $10 billion (USD),Information Technology,10000+ Employees,Company - Public,False
"Manager, Analytics and Data Engineer - Remote","Optum
","Plymouth, MN",$101K - $184K (Employer est.),3.7,"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.

This position will help to develop Optum’s cloud based actuarial analytics platforms, ingesting, conforming, cleansing, and enriching health payer data to support advanced actuarial and data science analytics. Our actuarial analytics platforms are built using Databricks, Snowflake and Power BI and other cloud based analytic and engineering tools.

This position will design, develop, implement, test, deploy, monitor, and maintain the delivery of data and analytics pipelines to support actuarial reporting and analytics. Additionally, this position will be involved in building actuarial enrichment data processes, productionizing data science algorithms and performing application POCs to support client service engagements.

You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.




Primary Responsibilities:

Lead team of data engineers and collaborate with actuaries and data scientists to design and deliver cloud health care analytic solutions
Responsible for data engineering lifecycle including research, proof of concepts, design, development, testing, deployment, and maintenance
Build automated, scalable, flexible, secure, and cross domain data and analytic pipelines with supporting monitors, listeners, notifications, and process state reporting
Lead monthly and quarterly production activities
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Work with other Optum and customer staff to develop pipelines to move data in and out of our domain

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Years of post-high school education can be substituted/is equivalent to years of experience




Required Qualifications:

4+ years of experience in data engineering, specifically coding ETL and building data pipelines
2+ years of experience writing code in Spark, Python, and SQL
Experience leading a technology team
Experience using orchestration tools such as ADF, Airflow, etc.
Experience with CICD tools such as Jenkins, GitHub, Maven etc.



Preferred Qualifications:

Cloud experience (Azure/AWS/GCP)
Snowflake experience
Power BI development experience
Experience using dbt to build data models
Experience analyzing and building models using health care claims data
Work as an actuary or with actuaries
Knowledge of health care concepts - benefits, pricing, underwriting, stop loss reinsurance, reserves etc.

California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $101,200 to $184,000 per year. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.



All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.




Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.




UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.",-1,Health Care Services & Hospitals,$10+ billion (USD),Healthcare,10000+ Employees,Company - Public,False
Big Data Engineer,Sky Consulting Inc,"Minneapolis, MN",-1,-1.0,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",-1,-1,Unknown / Non-Applicable,-1,1 to 50 Employees,Company - Public,True
"Senior Data Engineer, Talent Analytics","RVO Health
","Minneapolis, MN",$100K - $170K (Employer est.),3.5,"AT A GLANCE

RVO Health is looking to grow our Talent Analytics team by adding a Senior Data Engineer. In this role, you'll be challenged to help shape our Talent Analytics strategy while working on high-priority data efforts – all in line with our broader mission of attracting diverse talent and giving them an experience that will bring out their very best. You will be responsible for scoping, executing, and delivering technical projects to stakeholders across the Human Capital organization, and producing data engineering & analytical solutions that connect them to the data they need.

Where You'll Be

To prioritize togetherness, culture, and accountability, RVO Health operates on a hybrid in-office work schedule. We expect employees to work from our Minneapolis office Tuesday, Wednesday and Thursday each week. You are welcome to work remotely Mondays and Fridays if you wish.

11000 Optum Cir Eden Prairie, MN 55344

What You'll Do
Develop/maintain data pipelines from various data sources (ADP WFN, Greenhouse Recruiting/Onboarding, CultureAmp, Docebo, etc) to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.
Conduct hands-on, advanced data engineering & analytics using multiple data sources originating from different applications and systems.
Collaborate with the data science team to identify new opportunities for deep analytics within the Human Capital organization.
Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.
Execute on proof of concepts, where appropriate, to help improve our technical processes.
Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.
What We're Looking For
5+ years of Data Engineering experience
3+ years of writing SQL experience against complex databases for data extraction using AWS Athena (Presto), Databricks Delta Lake along with Data Modeling & Data warehousing experience.
3+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines and experience data processing using Parquet and Avro
3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, requests, json, csv, os
3+ years of experience working on AWS services including Glue, Athena, Lambda, S3, SNS, SQS, Cloud formation, Step Functions, Serverless architecture.
Experience with GitHub, Code check-in, versioning, Git commands
Introduce and drive adoption of CI/CD framework within the team and build/deploy CI/CD Pipelines using Terraform or AWS Cloud Formation
Experience with visualization tools such as Tableau, Looker or PowerBI to build dynamic/scalable dashboards and reports.
Strong analytical and interpersonal skills
Knowledge or experience within Talent/People analytics is a plus
Enthusiastic, highly motivated and ability to learn quickly.
Able to work through ambiguity in a fast-paced, dynamically changing business environment.
Ability to manage multiple tasks at the same time with minimal supervision.

Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.

Starting Salary: $100,000 - $170,000

Note actual salary is based on geographic location, qualifications and experience
Access to a Free Udemy for Business subscription—thousands of hours of learning content on hundreds of different subjects at your fingertips
Health Insurance Coverage (medical, dental, and vision)
Life Insurance
Short and Long-Term Disability Insurance
Flexible Spending Accounts
Paid Time Off
Holiday Pay
401(k) with match
Employee Assistance Program
Paid Parental Bonding Benefit Program

This position may occasionally require travel for training and other work-related duties.

Who We Are:

Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group's Optum Health. Together we're focused on delivering on our vision of a stronger and healthier world.

RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.

We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.

RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications.

We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.

#LI-Hybrid

RVO Health Privacy Policy: https://rvohealth.com/legal/privacy",2022,Hospitals & Health Clinics,Unknown / Non-Applicable,Healthcare,1001 to 5000 Employees,Company - Private,False
Software Engineer II - Data Science,"Infinite Campus
",United States,-1,3.1,"Job Description

The Software Engineer II is responsible for design, development, and testing of select product area(s) of Infinite Campus software systems. Engineering leadership skills to mentor and coach engineers on technical implementation. Able to produce long term sustainable solutions. Must be legally authorized to work within country of employment without sponsorship for employment visa status (e.g., H1B) now or in the future. Must be legally authorized to work within country of employment without sponsorship for employment visa status (e.g., H1B) now and/or in the future.


Job Responsibilities
· Demonstrate mastery of standard concepts, practices, and procedures in technologies used at Infinite Campus (e.g., Java, Groovy, SQL, Spock, Grails, JavaScript).
· DevOps knowledge/skills such as build and deploy processes, Devops tools such as Jenkins, docker, tomcat, Kubernetes.
· Research, investigate, and fix a wide range of technical issues.
· Assume overall responsibility for design, development, and quality of a project or product area
· Mentor other software engineers
· Provide technical leadership through coding knowledge, technical guidance on solution design and implementation, engaging in pair programming sessions and providing oversight between engineering team to ensure dependencies are managed
· Improve code based upon technical experience and product knowledge. Stay abreast of latest technology
· Develop software to fulfill complex business requirements
· Facilitate planning, estimation, and communication
· Demonstrate ability to meet aggressive timelines
· Evaluate the quality of requirements, designs, and tests
· Multi-task among different development activities
· Improve code based upon technical experience and product knowledge
· Debug and test code
· Create unit and other automated code validation tests
· Research, investigate, and fix a wide range of technical issues
· Use agile planning, estimation, communication, and release processes
· Write system documentation in support of the code being developed
· Access Infinite Campus production customer data and production data in public test adhering to the practices and procedures outlined in the master security guidelines
· Fulfill other related duties as assigned


Desired Background
· BA/BS in Computer Science or related field and three years software development experience; OR five years software development experience
· Strong communication and presentation skills
· Experience with technologies such as Java, Groovy, SQL, XSLT, Spock, Grails, JavaScript, or Angular
· DevOps knowledge/skills such as build and deploy processes, Devops tools such as Jenkins, docker, tomcat, Kubernetes.


Performance Expectations
· Perform job responsibilities as directed achieving desired results within expected time frames and with a high degree of quality and professionalism
· Establish and maintain positive and productive work relationships with all staff, customers and business partners
· Protect confidentiality of student and district data
· Must be a self-starter
· High attention to detail
· Must have good analytical and organizational skills
· Possess a positive attitude in stressful situations
· Ability to work in a fast-paced, agile environment with frequent software releases
· Ability to adapt to and work with numerous functional organizations
· Demonstrate commitment to quality
· Communicate well and proactively with internal stakeholders



Position must adhere to the practices and procedures outlined in the master security guidelines.

*This position may perform job responsibilities directly for a customer contract, which may be subject to a more thorough criminal background check to include, but not limited to, fingerprints, outside the standard screening. If the position performs job responsibilities directly for said customer(s), employee is required to pass the specified criminal background check; meeting the requirements of the customer contract.

Infinite Campus, Inc. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Infinite Campus complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

Infinite Campus, Inc. expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Infinite Campus’ employees to perform their job duties may result in discipline up to and including discharge.",1997,Computer Hardware Development,$100 to $500 million (USD),Information Technology,201 to 500 Employees,Company - Private,False
AI Engineer/Data Scientist,TMPI,"Minneapolis, MN",$130K - $160K (Employer est.),-1.0,"Benefits/Perks

Competitive Compensation
Flexible Scheduling
Career Growth Opportunities
Job Summary
We are seeking a detail-oriented Data Scientist to join our team. As a Data Scientist, you will analyze large amounts of raw data to discover patterns and insights that will help to improve our company. We will rely on you to collect and analyze data, build models, and present your findings in an easy-to-understand manner. The ideal candidate has an analytical mind and a passion for data.

Responsibilities

Analyze large amounts of raw data to discover trends and patterns
Discover valuable business insights and propose new approaches to challenges
Automate the data collection process
Identify valuable data sources
Build machine learning algorithms and predictive models
Present information in an easily digestible format
Collaborate with other teams within the organization
Work between multiple projects and flexibility to shift between projects
Code development and integration
integrate with database and knowledge on SQL
Code versioning and maintaining
Qualifications

A bachelor’s degree in Computer Science or a related field is preferred
Strong understanding of data collection and analysis
Strong troubleshooting and analytical skills
Ability to work well as part of a team
Strong written and verbal communication skills
Experience with data visualization tools such as Tableau and PowerBI
Minimum of 3 years working experience in AI field
Minimum of 3 years experience in programming languages
Understand Agile Scrum and Waterfall project management methodologies
Knowledge of R, SQL, Python, Java, Unix and Linux, C++",-1,General Repair & Maintenance,Unknown / Non-Applicable,"Construction, Repair & Maintenance Services",Unknown,Company - Private,True
Data Engineer,"ABOUT HEALTHCARE INC
","Saint Paul, MN",$87K - $121K (Glassdoor est.),3.2,"ABOUT offers a flexible, purpose-built solution that empowers hospitals and health systems to operate as one connected network of care. We enable easy access for clinicians to move patients into and out of the acute care setting - getting them to the next, best care setting faster and easier. Complemented by our clinical experts and best practices, we provide health systems the necessary controls and insights to grow with resilience, drive clinician effectiveness, and improve patient outcomes.

SUMMARY

THIS POSITION IS ON-SITE 2-3 DAYS/WEEK in OUR DOWNTOWN ST PAUL OFFICE. PLEASE DO NOT APPLY IF YOU ARE NOT LOCAL TO MN

Services as a technical expert on our data layer. Discusses customer needs, maintains and updates databases, warehouse or marts. Tunes and ensures optimal database and server performance. Assists with reporting and analytics to meet business intelligence requirements and ensures overall integrity, quality and related system maintenance.

ESSENTIAL FUNCTIONS:
Discusses needs, operations and data requirements with staff or customers and outlines system capabilities and approaches to pull, maintain and analyze system data.
Work with other data engineers to design and maintain scalable data models and ETL pipelines
Help design, build, and improve the infrastructure for ingesting, storing, securing and transforming data at a scale
Help design and build systems to monitor and analyze data
Provide technology ownership for data solutions for projects that the team has been tasked with.
Work with a cross functional team of business analysts, architects, engineers, data analysts to formulate technical requirements.
Design and build data pipelines from various data sources to a target data warehouse using batch data load strategies utilizing cloud technologies.
Conceptualizing and generating infrastructure that allows data to be accessed and analyzed effectively.
Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.
Perform periodic code reviews and test plans to ensure data quality and integrity.
Provide input into strategies that drive the team forward with delivery of business value and technical acumen.
Execute proof of concepts, where appropriate, to help improve our technical processes.
Provides analysis, interpretation and counsel regarding the application and usage of systems, business intelligence and reporting to improve policies, programs, and practices.
Provides research and feedback to resolve management and customer questions and requirements; assists with receiving customer feedback and coordinating resources and responses as required.
Analyzes and reviews operations, results, feedback, and related information on an ongoing to as needed basis to determine trends, draw conclusions, interpret findings, and presents results, proposals, and recommendations to management.
Ensures the accuracy of operational databases, reports, and related details through audits, queries, and operational reviews; works with teams to resolve discrepancies.
Interprets and applies department policies and procedures and assists with applicable laws, rules, and regulations; receives guidance within these areas as needed.
Contributes to the efficiency and effectiveness of the department's service to its customers by offering suggestions and directing or participating as an active member of a work team.
Performs other duties as assigned.

QUALIFICATIONS:
Need to Have:
Due to the need for this role to be credentialed with the VA, we can only consider US citizens for this role.
CURRENTLY local to Minneapolis / St Paul area and able to work on site 2-3 days/week
Bachelor’s Degree in Information Technology or related field and 5 years of related experience; or equivalent education and experience.
Ability to work on site in our St Paul office 2-3 business days/week; parking reimbursement provided
4+ years of experience in managing data/databases (Proficient in SQL)
4+ years of experience in translating business requirements into technical data solutions on a large scale.
Nice to Have:
Prior experience working in a private equity-funded organization.
Healthcare technology experience.
Experience with Mirth or a similar integration or ETL tools

Required Knowledge and Skills
Required Knowledge:
Research and troubleshoot potential issues presented by stakeholders within the data ecosystem.
Experience with Data Modeling, Data warehousing
Strong analytical and interpersonal skills.
Enthusiastic, highly motivated and ability to learn quickly.
Able to work through ambiguity in a fast-paced, dynamically changing business environment.
Ability to manage multiple tasks at the same time with minimal supervision.
Advanced principles, practices and techniques of managing data , databases and system analytics.
Specialized understanding of data engineering, reporting and management.
Understanding of the administration and oversight of data, system and business intelligence programs, policies, and procedures.
Various methods to identify and resolve analytical problems, questions, and concerns.
Basic methods and approaches to analyze and improve business operations.
Understanding of applicable laws, codes, and regulations.
Computer applications and systems related to the work.
Principles and practices to serving as an effective project team member.
Methods to communicate with staff, coworkers, and customers to ensure safe, effective, and appropriate operations.
Correct business English, including spelling, grammar, and punctuation.
Required Skills:
Performing advanced data engineering duties in a variety of assigned areas.
Overseeing and administering business intelligence and data analytical systems.
Using standard, customized, and complex data analytics tools.
Training others in policies and procedures related to the work.
Identifying, documenting, and reporting on system and data administration.
Serving as a team member and the development and management of projects.
Operating in both a team and individual contributor environment.
Using initiative and independent judgment within established department guidelines.
Contributing effectively to the accomplishment of team or work unit goals, objectives, and activities.
Establishing and maintaining effective working relationships with a variety of individuals.
We also value the employee experience, so we offer a full benefits package, including medical, dental, vision, supplemental insurances, parking reimbursement, and a flexible time off policy.

PHYSICAL/MENTAL REQUIREMENTS:
Mobility to work in an office setting, use standard office equipment and stamina to sit for extended periods of time; strength to lift and carry up to 10 pounds; vision to read printed materials and computer screens; and hearing and speech to communicate in person or over the telephone.

Travel as needed to support company and customer initiatives. Work on Site at our St Paul office is required and ABOUT reserves the right to change the location of the role at any time.
This role may involve work on federal government contracts that require additional federal background investigations, training, and federal badging. As such, you will required to submit personal information to the government and be fingerprinted and photographed at your local Department of Veteran Affairs Medical Center.

ABOUT is also an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.",-1,Enterprise Software & Network Solutions,Unknown / Non-Applicable,Information Technology,51 to 200 Employees,Company - Private,True
Senior Data Engineer,"General Mills
","Minneapolis, MN",$133K - $175K (Employer est.),4.1,"Job Description:
Employer: General Mills, Inc.

Job Title: Senior Data Engineer (multiple positions)
Job Requisition: #25002 | 20330.305.6
Job Location: 1 General Mills Blvd. Minneapolis, MN 55426 | Telecommuting 100% of time is permitted.
Job Type: Full Time
Rate of Pay: $133,385 - $174,600 per year

Duties: Work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions that will generate insights from our connected data, enabling General Mills to advance the data-driven decision-making capabilities of our enterprise. Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data products for analytics and data scientist team members to improve their productivity. Advise, consult, mentor and coach other data and analytic professionals on data standards and practices. Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions. Lead evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team. Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes. Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.

Telecommuting 100% of time is permitted.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Management Information Systems, Engineering, or related field and 5 years of post-baccalaureate, progressively responsible experience in job offered or 5 years of post-baccalaureate, progressively responsible experience in data engineering or architecture.

Must have experience in each of the following:
1. 4 years of experience working with data analysis
2. 4 years of experience with SQL or Hive QL
3. 4 years of experience developing and maintaining data warehouses in big data solutions
4. 4 years of experience with Big Data development using Hadoop
5. 4 years of experience with Hive, BigQuery , Impala OR Spark
6. 4 years of experience automating the data pipelines/processes
7. 4 years of experience with Hadoop ecosystems (HDFS, YARN, Hive, HBase, Sqoop, Spark, and or Hue, )
8. 3 years of experience utilizing Agile Development Methodology
9. 3 years of experience with Git Repositories

Telecommuting 100% of time is permitted. Background check and drug testing required.
Contact: Apply online at https://careers.generalmills.com/careers/ Please refer to job requisition number- #25002

The salary range for this position $133,385-$174,600 per year. At General Mills we strive for each employee’s pay at any point in their career to reflect their experiences, performance and skills for their current role. The salary range for this role represents the numerous factors considered in the hiring decision including, but not limited to, education, skills, work experience, certifications, etc. As such, pay for the successful candidate(s) could fall anywhere within the stated range. Beyond base salary, General Mills offers a competitive Total Rewards package focusing on your overall well-being. We are proud to offer a foundation of health benefits, retirement and financial wellbeing, time off programs, wellbeing support and perks. Benefits may vary by role, country, region, union status, and other employment status factors. You may also be eligible to participate in an annual incentive program. An incentive award, if any, depends on various factors, including, individual and organizational performance.
.

Company Overview:
We exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best — bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what’s next.",1866,Food & Beverage Manufacturing,$10+ billion (USD),Manufacturing,10000+ Employees,Company - Public,False
Senior Data Engineer,"When I Work
","Minneapolis, MN",$96K - $130K (Glassdoor est.),4.0,"When I Work is a remote first company. We are open to hiring candidates in the continental US and Ontario, Canada. If an onsite location is important to you in your search, you are welcome to work from our Minneapolis HQ office.
Who We Are

We help hourly teams get shift done.

At When I Work, everything we do starts with a mission to make shift work awesome. We deliver on that mission by making every piece of hourly workforce management - scheduling, time tracking, shift trading, team messaging, and more - easy and straightforward for managers and employees alike.

The Data and RevOps team at When I Work is a group of inquisitive and driven individuals who love solving problems using data. We have built a best-in-class data environment and fuel insights throughout the organization on our product and customers. We work collaboratively together, invest in our processes and tooling, and move slow to move fast. We focus on projects that will have a big impact to the company and work to enable anyone to be a savvy data user.

What You'll Do

Over the last few years we have been building out a best-in-class data environment that we've used to transform When I Work into a data-driven company. You will be a key contributor in continuing to grow and mature this environment as well as develop and sustain data projects that will have significant impact on our company and our users.

Proactively identify opportunities to improve and update data platform infrastructure and research new technologies and strategies
Design, build and implement tools aimed at allowing business users to collect and analyze data in an efficient and effective way
Design, build and maintain integrations between our internal data platform and 3rd party tools utilized throughout the company
Develop and manage ETLs and data pipelines
Create data products for consumption by internal When I Work team members
Be part of a team that owns all aspects of its service delivery
- from cloud infrastructure, to application code, to operations
Our Technology Stack

We use a lot of different technologies to get the job done, and each member of our team brings their own mix of technology experience. If you have familiarity with even a few of these (or equivalents), you could make a valuable contribution: Python, Go, SQL, Terraform, Jupyter, Git, GitLab, Spark, Flink, Presto, Kafka, MySQL, NoSQL, Kubernetes, DBT, Prefect, Airflow, lots of AWS(EC2, EKS, Lambda, S3, RDS, DynamoDB, Aurora, Redshift, Athena, EMR, CloudSearch, Kinesis, API Gateway).

Who You Are

You are a programmer who is excited by data and its endless possible use cases. Someone that enjoys creating tools and infrastructure to empower your peers. Collaboration and teamwork are a must, but you also have the ability to work independently when needed to get things done. Above all, you are driven to learn and a motivated problem solver who wants to help tackle the new and interesting challenges that we encounter as a fast-growing startup.

Experience and Skills Needed
3+ years of experience in data engineering
You have strong programming fundamentals
You are comfortable with agile software processes
You have experience with multiple programming languages (Python, SQL, etc.)
Comfortable working with APIs/Webhooks
You have significant experience working with structured and unstructured data
You have significant experience with cloud computing environments and infrastructure
You are a proponent of DevOps and enthusiastic about DataOps
You practice empathy and kindness, and you look to help others
What Would Be Awesome To Have
Advanced Python and data package (Numpy, Pandas, etc.) skills
You are comfortable with different data modeling techniques and have experience with a data warehouse platform (Redshift, Snowflake, etc.)
You have experience working with message queues and event buses to collect and process data in near real time
Understanding & perspective on data catalogs and schema registries
What's In It For You
Professional development allowance
Paid parental leave
Medical benefits - employee premiums paid 100% by When I Work
Dental benefits- employee premiums paid 100% by When I Work
Paid vacation and holidays
Flexible work environment
401K Match
Remote first culture including home office set-up stipend and ongoing telecommuter stipend
Casual dress code
Dynamic and dedicated team


We believe actions speak louder than words. Every encounter with our people and products should be memorable and helpful. Challenges are exciting, failure is how we learn, and we all have an entrepreneurial spirit. Building an inclusive and equitable workplace isn't lip service. We invest our time and our money in organizations that are not only working to diversify the current jobscape, but also investing in the future of talent. We're motivated by a strong, innovative, and passionate work culture and we're constantly searching for ways to improve and get shift done.

Whether you're a perfect match or not, if it sounds like a good fit, we encourage you to apply.

The tech industry is notorious for its lack of diverse representation, and we're aware of the research showing that historically underrepresented groups are less likely to apply to a job if they don't believe that they meet all of the criteria. Are you hesitant to submit an application because you're not sure if you check every box? Apply anyway! We would love to hear from you and figure out what you can add to the culture here at When I Work.

We'd love to talk to you! Please submit the following to apply:

Resume (including months/years of employment for each position).
Cover letter including:
an overview of your existing experience
a convincing reason why you'd like to work at When I Work.
Must already be authorized to work in the United States or Canada on a full-time basis for any employer.",2010,Software Development,$25 to $100 million (USD),Information Technology,51 to 200 Employees,Company - Private,True
Data Engineer (Cloud) – Minneapolis,"Object Partners
","Minneapolis, MN",$90K - $129K (Glassdoor est.),4.4,"Why consider OPI, and why do people dig working here?

Variety of consulting; new technologies, projects, and people on a regular basis.
Stability; we’ve been around since 1996 and have a diverse mix of clients and technologies to keep us busy, very busy. And we keep a bench. If you’re not on a project, you’re writing software for our internal business functions or you’re learning new technologies. It’s beneficial to make our consultants as marketable as possible. That’s good for your career.
No politics or management; we don’t get in the way. Why sit in meetings all day when you can code and be productive?
Awesome benefits; robust healthcare plan, 28 days of PTO, semi-annual profit sharing bonuses, you get paid OT, company trips, various quarterly company events, new MacBook Pro’s, free beer/soda, chips, candy, and so much more.
You work with the best. Do an Object Partners search on LinkedIn and see the types of talent we hire. You truly get to work with intelligent, passionate engineers that share the same goal of building great software the right way.
Low company overhead. It all means more money back into our consultants pockets (profit sharing) or company trips and events to share in the financial success.
Data Engineer

As a Data Engineer, you’ll be working with the latest cloud and technology stacks to help clients implement and mature their modern data architecture. You will work with tools/platforms like Kafka, Snowflake, and Databricks to help clients get the most out of their data that may be in systems like Salesforce, SAP, SQL Server, or file storage. With a variety of projects, technologies, and clients, you will constantly be growing, and never bored.


Qualifications
At least 4 years of experience as a hands-on software or data engineer
At least 1-2 years building production-grade data solutions (Example: ETL/ELT, Spark, Azure Data Factory, AWS Data Migration Services, streaming systems)
Demonstrated aptitude for problem-solving and creativity
Ability to learn new technologies and apply learnings to production-grade solutions
Experience with at least one prominent cloud provider (e.g.: AWS, Azure, GCP)
Strong working knowledge of a querying language like SQL
Understanding of CI/CD, automated testing, and the DevOps culture
Effectively communicate complex technical solutions to a variety of audiences through oral and written mediums
Preferred Skills
Production experience with at least one distributed data system like Snowflake, Databricks, Cassandra, DynamoDb, Elastic, or Hadoop
Production experience with at least one messaging technology like Kafka, Kinesis, Pulsar, or RabbitMQ
Certification on at least one relevant platform/tool (AWS, Azure, GCP, Snowflake, Databricks, Spark)
Can translate business needs into optimized and efficient data models in SQL or NoSQL
Service frameworks such as Spring Boot, Ratpack, Vert.x, or Play
Knowledge of data analytics, visualization and governance
Experience working in an agile development framework like Scrum or Kanban",1996,Business Consulting,$5 to $25 million (USD),Management & Consulting,51 to 200 Employees,Company - Private,False
Data Analyst/Engineer,"Beacon Specialized Living Services
",Minneapolis-Saint Paul,$64K - $86K (Glassdoor est.),3.0,"Applicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.

Primary Responsibilities/Essential Functions:

· Assist in selecting and building Data Warehouse

· Define and Build Tabular Data Model

· Improving observability, discoverability, governance, and implementing a common data integrity and data quality testing framework.

· Constructing reliable and performant high-volume ETL or ELT pipelines for sensitive healthcare data.

· Contributing to and maintaining legacy ETL and ELT data pipelines.

· Proactively monitoring data pipelines for potential problems and debugging issues if they arise.

· Helping to model data at various stages of refinement, curation, and enrichment to best suit different downstream targets and marts.

· Partnering with leadership to identify data objectives, targets, and bringing data insights to life.

· Other duties as assigned.

Education & Qualifications:

· A Bachelor's degree in computer science, data science, or information systems.

· 3 years of proven data and performance engineering.

· Expert in SQL.

· Experience using data warehouses and databases like Azure, SQLAAS.

· Experience developing custom-built data/analytics solutions.

· Experience with Azure, Data Factory, API’s.

· A strong understanding of healthcare.

· Established project management skills.

· Advanced training certifications may be advantageous.

· Excellent verbal and written communication skills, interpersonal, and teaching skills.

· Good anticipation, analytical, and problem-solving skills.

· The ability to remain current on the latest technology and best practices in information security.

· Valid Driver’s License with acceptable driving record as determined by Motor Vehicle Report and insurance guidelines.

Job Type: Full-time

Benefits:

401(k)
401(k) 4% Match
Dental insurance
Health insurance
Health savings account
Life insurance
Paid holidays
Paid time off
Vision insurance

Compensation package:

Yearly bonus

Experience level:

3 years

Schedule:

Monday to Friday

Application Question(s):

Are you willing to travel to Nashville, TN as needed? (not frequent)
Do you require work sponsorship now or in the future? (Answer Required)

Education:

Bachelor's (Required)

Experience:

Healthcare IT: 3 years (Required)

Work Location: In person",1964,Health Care Services & Hospitals,$5 to $25 million (USD),Healthcare,1001 to 5000 Employees,Company - Private,True
Data Engineer,"OneOme Holdings LLC
","Minneapolis, MN",$84K - $123K (Glassdoor est.),3.0,"ABOUT OneOme

We are a precision medicine leader, providing evidence-based pharmacogenomic solutions that facilitate more personalized prescriptions across the globe. Our RightMed Solution helps healthcare organizations, providers, and payers optimize patient outcomes and reduce costs by facilitating more personalized prescriptions. Paired with an in-house CLIA-certified, CAP-accredited lab, OneOme’s RightMed Test provides powerful genetic insights that doctors, pharmacists, and other healthcare providers can use to help inform medication decisions for their patients. Our mission is to enable healthcare teams with evidence-based pharmacogenomic testing and analytics that help improve outcomes and reduce costs through more personalized medication decisions. OneOme was named one of Fast Company’s 50 Most Innovative Companies of 2018.


ROLE DESCRIPTION

We are seeking a Data Engineer to join an existing team of software developers and data scientists that are building and supporting OneOme’s PGx solution. The ideal candidate will be well versed in best practices for storing and processing genomic and health claims data including ETL processes, relation mapping, and warehousing of data in both local and cloud environments.

This position requires excellent attention to detail, effective communication skills, and the ability to work both independently and within a team environment.


RESPONSIBILITIES

Build and deploy cloud based solutions
Make data driven decisions to improve stability and performance
Work with 3rd party clients to design and implement data transfer workflows
Design and implement data quality control workflows to validate data flows in and out of the organization
Work with stakeholders to build ETL and reporting workflows


REQUIREMENTS

B.S. in Computer Science or related discipline
Minimum of 4 years experience with health claims data
Knowledge of Azure resources (Data Factory and Data Bricks)
Experience working autonomously with limited direction in a fast-paced environment


REQUIRED EXPERIENCE/SKILLS

Prior experience with HIPAA or similar security practices
Experience in healthcare
Python (Pandas and Numpy)
SQL
Spark


DESIRED QUALIFICATIONS

Salesforce
Data Bricks
Data Factory


POSITION LOCATION

OneOme Headquarters, 807 Broadway St NE, Suite 100, Minneapolis, MN 55413


SCHEDULE AND HOURS

This position requires the ability to work the core company business hours of Monday - Friday 8AM to 5PM Central Time. Work outside of normal company work hours including weekends may also be required depending upon business needs. Some travel (up to 10%) may be required from time to time.


WORK ENVIRONMENT

OSHA RISK FACTOR CATEGORY 1. The employee is regularly required to talk or hear. The employee frequently is required to stand, walk; sit; and use hands to finger, handle, or feel, and work with a computer. The employee is occasionally required to reach with hands and arms; climb or balance; stoop, kneel, crouch, or crawl; and smell. The employee must occasionally lift up to 50 pounds and/or carry objects weighing up to 25 pounds. The employee is required to perform repetitive motions, including reaching above the head, and typing. Specific vision abilities required by this job include peripheral vision, depth perception, and ability to adjust focus. Color-blindness testing is required for those with job-specific duties requiring color discrimination.",2014,Biotech & Pharmaceuticals,Unknown / Non-Applicable,Pharmaceutical & Biotechnology,1 to 50 Employees,Company - Private,True
Data Engineer II - Max Digital (Data Operations),"ACV Auctions
","Minneapolis, MN",$81K - $108K (Glassdoor est.),3.7,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.

Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:

Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
MAX Digital is looking for a Data Operations Engineer to join our growing team and boost our data quality and data control efforts to meet customer and market demand. You will be working with the Product, Customer Success, Customer Support, Account Management, Engineering & Data Operations teams as well as a group of dedicated industry professionals to support some of the most advanced tools in the automotive retail industry.

What you will do:

Actively and consistently support all efforts to simplify and enhance the customer experience.
Analyze and address data customization requests & quality issues reported by our customers.
Configure, manage and test data integrations for our customers.
Triage and resolve all data-related customer cases escalated by the Help Desk (L1) team
Work with product managers & engineering to determine & track meaningful metrics and KPIs.
Build data visualizations / dashboards to communicate key insights
Translate ad-hoc internal and external data requests into reusable data tools that can scale to answer broader questions.
Work with complex data to solve a wide variety of challenges using statistical approaches such as exploratory data analysis to provide insights and recommendations.
Quality assurance and data validation of new data products and services developed by engineering teams.
Identify and leverage data pipelines across the data fabric of ACV to conduct qualitative and quantitative analysis, identify trends, and provide solutions to business partners.
Work with product managers and operation teams to understand the business pipeline, review new products and identify related data sources for analysis.
Develop and analyze large data sets using all required tools from data gathering, data quality check, query data, and BI.
Communicate and follow up with stakeholders to understand their needs and questions converting business challenges into technical requirements and present analysis to stakeholders.
Perform additional duties as assigned.
What you will need:

Bachelor’s degree in Computer Science, Data Science, Information Systems, or related field(s) involving complex data analysis (or relevant experience)
3 years’ professional experience using SQL to write complex queries and transformations to analyze data across large and complex data sets.
Ability to read, write, speak and understand English.
Experience optimizing SQL queries for performance.
Proficient in analyzing data across various platforms with multiple tools like Microsoft SQL Server, BigQuery, Excel, Google Sheets, MongoDB, etc.
Expert working knowledge of Excel / Google Sheets
Experience working with Python Notebooks for data analysis (strong emphasis on pandas, numpy, etc)
Comfortable working with APIs to query & integrate output with other related datasets for analysis.
Experience working in AWS and/or Google Cloud Platform environments.
Strong analytical and problem-solving skills
Internal and external “Customer-first” mentality to meet or exceed needs of the organization and its customers.
Strong ability to effectively summarize, document and communicate both internally to technical and non-technical teammates and externally to customers at all levels.
Intellectually curious and lifelong learner.
Nice to Have
Advanced proficiency in Python, R, TypeScript, or other scripting languages
Experience with data visualization / dashboarding tools like Tableau, Looker Studio, etc.
Comfortable working with other types of APIs (e.g. GraphQL)
Experience with data engineering workflow management systems (e.g. Apache Airflow)
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling

At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.

For information on our collection and use of your personal information, please see our Privacy Notice.",2014,Wholesale,Unknown / Non-Applicable,Retail & Wholesale,1001 to 5000 Employees,Company - Public,True
Senior Data Engineer,"Hormel Foods
","Austin, MN",$102K - $143K (Glassdoor est.),3.7,"Senior Data Engineer

Hormel Foods Corporation

ABOUT HORMEL FOODS - Inspired People. Inspired Food.™

Hormel Foods Corporation, based in Austin, Minn., is a global branded food company with over $12 billion in annual revenue across more than 80 countries worldwide. Its brands include Planters®, SKIPPY®, SPAM®, Hormel® Natural Choice®, Applegate®, Justin's®, Wholly®, Hormel® Black Label®, Columbus® and more than 30 other beloved brands. The company is a member of the S&P 500 Index and the S&P 500 Dividend Aristocrats, was named on the ""Global 2000 World's Best Employers"" list by Forbes magazine for three years, is one of Fortune magazine's most admired companies, has appeared on Corporate Responsibility Magazine's ""The 100 Best Corporate Citizens"" list for 12 years, and has received numerous other awards and accolades for its corporate responsibility and community service efforts. The company lives by its purpose statement - Inspired People. Inspired Food.™ - to bring some of the world's most trusted and iconic brands to tables across the globe. For more information, visit www.hormelfoods.com and http://csr.hormelfoods.com/.

Summary:
We are looking for a Senior Data Engineer in our IT Analytics team. This is an exciting opportunity to help grow and modernize analytics at Hormel Foods! Individuals interested in this position will need strong communication skills, communicating up, down and across the organization. You will be responsible for managing simultaneous initiatives that require innovative problem solving.

You will use tools such as SQL, Oracle Business Intelligence, Tableau, Google Cloud Platform, Python and Informatica ETL to engineer data pipelines and data models to enhance enterprise reporting and analytics. Additionally, you will engineer reports, dashboards and visualizations using enterprise business intelligence tools (Oracle and Tableau).

Specific competencies include:

Data Structures and Models - Designs, develops and scales the overall database/data warehouse structure based on functional and technical requirements. Designs, develops and scales data collection frameworks for structured and unstructured data.

Data Pipelines and ELT - Designs, applies and scales data extraction, loading and transformation techniques in order to connect large data sets from a variety of sources.

Data Performance - In complete autonomy, troubleshoots and fixes for data performance issues that come with querying and combining large volumes of data. Accounts for scaled performance in initial design.

Visualizations and Dashboards - Gathers requirements, designs and develops reports, dashboards and visualizations with multiple sources that meet business needs. Understands data and ideates ways for business to leverage data in innovative ways.

Responsibilities:

Works directly with the assigned business units to understand their analytics needs and gather requirements for analytics solutions
Engineers advanced physical and logical data models for dimensions and facts within the staging, warehouse and semantic layer of our enterprise data warehouse or data lake
Engineers and performance tunes Python or Informatica ETLs as well as Google BigQuery Dataprocs to move data from a variety of source systems and file types to fit into dimensional data models
Utilizes advanced SQL within Google BigQuery, Informatica ETLs or Oracle SQL Views when necessary to achieve proper metric calculations or derive dimension attributes
Engineers schedule and orchestration for batch and near-real time data loads into the enterprise data warehouse
Provide issue resolution and maintenance for a large variety of business unit solutions already existing in the enterprise data warehouse
Engineers dashboards within the enterprise business intelligence platform containing reports and visualization that have intelligent user interface design and flow for the business including adequate performance

Required Qualifications:

A bachelor's degree in Computer Science, MIS, or related area and significant experience with business intelligence design and development.
7+ years of experience with reading and writing SQL.
7+ years of experience designing and developing within a business intelligence/reporting tool like Oracle Business Intelligence, Tableau or Google Cloud Platform.
5+ years of experience engineering within a data warehouse or related experience with dimensional data modeling.
5+ years of experience designing and developing ETLs/pipelines in Python, Google BigQuery Dataprocs and/or Informatica ETL.
Excellent written and verbal communication skills.
Proven ability to gather detailed technical requirements to design and develop business intelligence report solutions from beginning to end.
Excellent organizational and time management skills.
Tested problem-solving and decision-making skills.
A strong pattern of initiative.
Highly developed interpersonal and leadership skills.
Must be a Citizen or National of the United States, a lawful, permanent resident, or have authorization to work in the United States.
Applicants must not now, or in the future, require sponsorship for an employment visa.

Preferred Qualifications:

Experience with Finance and/or Supply Chain data analytics
Experience working within Google Cloud Platform with services like Dataflow, Datafusion, Pub/Sub, Cloud SQL, Cloud Storage
Experience with Oracle SQL including advanced functions like analytical functions.
Experience tuning SQL and ETLs.
Experience within a core metadata model (RPD) including the physical, logical and presentation layers for the enterprise business intelligence platform (OBIEE - Oracle Business Intelligence Enterprise Edition.)

LOCATION:

Austin, MN - Global Headquarters (Preferred). Secondary location options include locations with a Hormel satellite office including Minneapolis, MN, Willmar, MN, Bentonville, AR, Bridgewater, NJ, Chicago/Naperville, IL.

A comprehensive relocation package to the Austin, MN area will be provided. Relocation assistance will not be provided to other location options.

BENEFITS:

Hormel Foods offers an excellent benefits package. Competitive base salary plus bonus, annual merit increase performance reviews, medical, dental, vision, non-contributory pension, profit sharing, 401(k), stock purchase plan, relocation assistance, paid vacation.

TRAVEL REQUIREMENTS: Travel may be necessary 10% of the time.

For immediate consideration, send apply online at: www.hormelfoods.com/careers

At Hormel we invite difference and diversity in all aspects. We offer a space of support, understanding, and community. We are committed to the journey! Learn more about our progress here: https://www.hormelfoods.com/about/diversity-and-inclusion/

Hormel Foods Corporation is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, gender, gender orientation, gender identity, national origin, disability, or veteran status.",1891,Food & Beverage Manufacturing,$10+ billion (USD),Manufacturing,10000+ Employees,Company - Public,False
Data Engineer,"MatrixCare
","Bloomington, MN",$86K - $118K (Glassdoor est.),3.3,"ResMed has always applied the best of technology to improve people's lives. Now our SaaS technology is fueling a new era in the healthcare industry, with dynamic systems that change the way people receive care in settings outside of the hospital–and tools that work every day to help people stay well, longer. We have one of the largest actionable datasets in the industry, creating a complete view of people as they move between care settings. This is how we empower providers–with vital insight to deliver the care people need, right when they need it.
We're also ensuring that our health solutions connect to other companies' networks. Because when objectives align, everyone wins. And as we work today to drive better care and lower costs, we're developing more personalized solutions for tomorrow, utilizing machine learning, intelligent care paths, and predictive protocols. If you are an innovator who wants to make an impact we want to talk to you! We have exciting opportunities supporting Brightree by ResMed and MatrixCare by ResMed!
We are looking for a savvy Data Engineer to join our growing team of platform experts. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.
You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The goal is to support our software developers, database architects, data analysts and data scientists on data initiatives to ensure optimal data delivery architecture is consistent throughout ongoing projects.
You are self-directed and comfortable supporting the needs of multiple teams, systems and products. You are excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. You are a team player who lifts the entire team through collaboration, mentoring and sharing your experience
Location - Open for ""Remote"" across US
Let's talk about the role
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and AWS ‘big data’ technologies like Glue, Lambda, EMR etc
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions
Work on data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems
Let's talk about you
Must have previous experience creating/running Python
Excellent Python coding knowledge or Pyspark jobs working within AWS Glue
You will have 4+ years of total experience in a full cycle Data Engineer role, who has attained a graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field or equivalent working experience
Experience building and working with AWS Data Lakes
Experience working on GitHUB CI/CD processes
You will also be strong in
Experience with big data tools: Snowflake, Hadoop, Spark, Kafka, etc
Experience with data pipeline and workflow management tools: Luigi, Airflow, AWS Step etc
Experience with AWS cloud services: EMR, RDS, Redshift
Experience with DBT \ Coalesce or other ELT tools
Experience with stream-processing systems: Kinesis, Spark-Streaming, etc
Experience with Docker Containers and Kubernetes
Familiarity with a variety of datasets, structured, semi structured and unstructured etc
Experience building and optimizing ‘big data’ data pipelines, architecture and data sets
Nice to have Experience with MS SQL Stack and SSIS
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Experience supporting and working with cross-functional teams in a dynamic environment
#LI - Tech
#LI - Remote
Joining us is more than saying “yes” to making the world a healthier place. It’s discovering a career that’s challenging, supportive and inspiring. Where a culture driven by excellence helps you not only meet your goals, but also create new ones. We focus on creating a diverse and inclusive culture, encouraging individual expression in the workplace and thrive on the innovative ideas this generates. If this sounds like the workplace for you, apply now!",1982,Enterprise Software & Network Solutions,$100 to $500 million (USD),Information Technology,1001 to 5000 Employees,Subsidiary or Business Segment,False
Sr. Cloud Data Engineer,"Medtronic
","Minneapolis, MN",-1,3.9,"Position Description:
Sr. Cloud Data Engineer for Medtronic, Inc. Minneapolis, MN. Multiple positions available. Designs, develops and maintains secure platforms for analytics & data science algorithms that support diabetes applications. Optimize security, robustness, scalability, performance, and cost to build and develop data driven products with prediction and forecasting capabilities. Develop on-premise and cloud-based software architecture systems. Provide critical support for AWS cloud environment and its associated services. Perform relational & NoSQL database design & optimization. Coordinate machine learning and statistical modelling for production systems using Python and Java. Develop ETL tools and logic to connect data pipelines to centralized datalake. Utilize real-time streaming data & processing using Apache Streams & Kafka. Develop analytical algorithms, reporting tools & dashboards to aid research, product strategy & business intelligence including attrition prediction & customer retention. Understand real-world and clinical diabetes solutions and navigates and uses data from continuous glucose monitoring systems and insulin pumps. Design, develop, debug and deploy Devops pipeline components related to technical infrastructure and ML/AIOps software programs. Create designs for multilayer infrastructure access management, security and system health. Monitor application performance, develop correlations for the existing automations and provide technical software solutions to isolate and resolve issues with the code, network and infrastructure. Coordinate SDLC (Software Development Life Cycle) processes and tool chains. Position is eligible for telecommuting status from anywhere in the United States. Multiple Positions available. #LI-DNI.

Basic Qualifications:
Requires a Master’s degree in Computer Science, Computer Information Systems, Statistics, Biomedical Engineering, Applied Mathematics, or related engineering or technical field and 2 years of experience as a cloud engineer or any occupation in data engineering or data science; or a Bachelor’s degree in Computer Science, Computer Information Systems, Statistics, Biomedical Engineering, Applied Mathematics, or related engineering or technical field and 5 years of experience as a cloud engineer or any occupation in data engineering or data science. Must possess 2 years of experience with each of the following: developing of on-premise and cloud-based software architecture systems; relational and NoSQL database design and optimization; development for production grade analytical systems using Python and Java; developing ETL tools and logic to connect data pipelines; real-time streaming data & processing; developing analytical algorithms, reporting tools & dashboards for research, product strategy and business intelligence; developing, designing, debugging and deploying Devops pipeline components related to technical infrastructure; developing designs for multilayer infrastructure access management, security and system health; monitor application performance and provide technical software solutions to isolate and resolve issues with the code, network and infrastructure; and SDLC (Software Development Life Cycle) processes and tool chains.",1949,Health Care Products Manufacturing,$10+ billion (USD),Manufacturing,10000+ Employees,Company - Public,False
Senior Data Center Engineer,"Marco Technologies LLC
","Rochester, MN",$72K - $103K (Glassdoor est.),4.0,"Marco is a high-performing technology services company with a track record of creating rewarding careers. Whether you are early in your career ready for an exciting opportunity, or an experienced professional eager to elevate your skills with a Print & IT services and consulting industry leader, then you have come to the right spot.

More about us. We make business technology Simple. Secure. Better. We do it all – from hosted/cloud services, copiers/printers, managed services, phone systems, document management, business IT services and audio/video systems. We are an organization with strong partnerships between sales and service. We have hundreds of sales professionals, engineers, and technicians ready to fix any and all customer problems. With over 50 years in the industry, Marco has a Gold Standard Culture with a focus on employee engagement, client satisfaction, vendor partnerships, and community partnership.

Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.


ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",1973,Information Technology Support Services,$100 to $500 million (USD),Information Technology,1001 to 5000 Employees,Company - Private,True
Data Catalog Engineer (Upcoming),"The Judge Group
","Inver Grove Heights, MN",$47K - $71K (Glassdoor est.),3.6,"Our client is currently seeking a Data Catalog Engineer to join their growing team! Please note that this job is open to fully remote candidates, but would prefer a resource who is local to or willing to travel 2-3 times a month to their campus in the Twin Cities area!

Project notes:

?Data quality center of excellence
?acquired meta data catalog tool - alation data catalog
?Interacting with a lot of different people
?Data platform team is in charge of getting tool connected to data platform - they will own the data
?Catalog tool ? designed to be data owners and data tool

Must haves:
?Alation experience
?Strong experience as a catalog admin ? very specific (alation experience)
?~3 yrs experience",1970,HR Consulting,$100 to $500 million (USD),Human Resources & Staffing,1001 to 5000 Employees,Company - Private,False
Senior Data Engineer (Snowflake) - Contractor,"EY
",United States,$90.00 - $130.00 Per Hour (Employer est.),3.9,"The primary responsibility for the Sr. Data Engineer / Tech Lead is to ensure data accessibility, quality, and reliability within the client's data platform ecosystem.

RESPONSIBILITIES

Design and develop end-to-end data pipelines and ETL processes using Snowflake, Denodo, DBT Cloud, Fivetran, and AWS Glue, ensuring seamless data integration, transformation, and loading.
Collaborate with cross-functional teams to gather data requirements and implement scalable data models and schemas within Snowflake and Denodo, ensuring optimal performance and data consistency.
Implement and maintain data orchestration workflows using Stonebranch, ensuring efficient and reliable scheduling and automation of data processes.
Work with EnterpriseDataLake for ingestion and storage of data from various sources, ensuring the availability and accessibility of data for analytics and reporting purposes.
Utilize Azure DevOps and Teraform to implement and manage infrastructure as code, ensuring streamlined deployment and scalability of data engineering solutions.
Skills

snowflake

denodo

dbt cloud

fivetran

Qualifications

REQUIREMENTS

Proficiency in Snowflake and Denodo: Strong understanding and hands-on experience in working with Snowflake and Denodo for data integration, transformation, and analysis.
Knowledge of DBT Cloud: Familiarity with DBT Cloud, including experience in building and maintaining efficient data transformation workflows and deploying reliable data pipelines.
Experience with Fivetran: Demonstrated experience in utilizing Fivetran for data ingestion, source connectivity, and ensuring data accuracy and completeness.
Understanding of EnterpriseDataLake: Knowledge of EnterpriseDataLake principles and practices, including data ingestion, storage, and organization for analytics and reporting purposes.
Familiarity with Stonebranch: Proficiency in using Stonebranch for data orchestration and scheduling, ensuring efficient and reliable execution of data pipelines and workflows.
Proficiency in Azure DevOps and Teraform: Experience in leveraging Azure DevOps and Teraform for infrastructure as code, enabling streamlined deployment and scalability of data engineering solutions.

The expected pay rate range for this contract assignment is $90 to $130 per hour. The exact pay rate will vary based on skills, experience, and location. The position is approximately 65-70% remote and 30-35% travel to the client as needed.

#GigNowTechOpportunities

GigNowOpportunities

Equal Employment Opportunity Information
EY provides equal opportunities to applicants, employees, contractors, vendors, and stakeholders without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law. If you are an individual with a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please email gignow.recruiting@ey.com.",-1,-1,Unknown / Non-Applicable,-1,Unknown,Unknown,False
Data & Software Engineer,"Ovative Group, LLC
","Minneapolis, MN",$62K - $93K (Employer est.),4.1,"About Ovative Group

Ovative Group is the premier independent media and measurement firm in the United States. We help change-makers, in fast-growing, customer-centric organizations across industries reinvent their marketing and measurement programs. We leverage our media, measurement, and consulting capabilities to help brands like Coach, Kate Spade, Stuart Weitzman, Facebook, The Home Depot, CVS, Disney, and UnitedHealth Group transform their media and marketing programs. Our proprietary approach to measuring and optimizing marketing investment decisions, Enterprise Marketing Return (EMR), is disrupting the industry and setting the gold standard for customer and marketing strategy, activation, and measurement.

Recognized eight consecutive years on Star Tribune’s list of Top 150 Workplaces and five years on Inc. 5000’s list of the fastest-growing private companies in America, we pride ourselves in always overdelivering for our clients, our teams, and our communities.

About the Role

We are seeking a Data & Software Engineer to join our rapidly growing product and engineering development team. Our company specializes in enterprise-level media measurement and optimization across various industries. In this role, you will be an integral part of a cross-functional team responsible for creating, optimizing, and maintaining scalable software and data solutions to enhance performance, stability, and scalability. You will work under the guidance of experienced team members and closely collaborate with stakeholders throughout the entire software development lifecycle, from concept to deployment.

The ideal candidate will have a strong foundation in iterative development practices, familiarity with version control systems like GitHub, and a passion for developing both conceptual and pragmatic problem-solving skills. Your attention to detail and communication skills, both written and oral, will enable you to work directly with a variety of users, understand their objectives, and contribute to translating them into technical requirements and solutions. As a valuable team member, you will be encouraged to learn and grow in a supportive environment, actively participate in team activities, and set the foundation for your professional development.

Responsibilities:

Assist in designing, developing, testing, and deploying software solutions that align with business and technical requirements.
Contribute to the effort of identifying opportunities for automation with a focus on the operational stability of software applications and systems
Collaborate with your team to support translating business goals and user requirements into detailed, actionable technical requirements
Contribute to documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports
Contribute to the creation and maintenance of robust, performant solutions that provide high customer impact at scale
Provide technical assistance to teammates as needed through collaboration and by sharing your expertise
Follow established software and product engineering best practices, including code quality, documentation, deployment, and testing.
Explore new technologies and tools under guidance to enhance software development.
Troubleshoot tier 1 software issues with support from senior team members as needed.
Participate in technical knowledge sharing with other team members.
Stay current with emerging technology and software engineering innovations and continuously enhance skills.

Requirements:

3+ years of relevant data & software engineering development experience
Proficient working with ETL/ELT tooling for large data sets (e.g., Airbyte)
Proficient utilizing SQL, Python, and command line
Familiarity with cloud-based platforms (i.e. GCP, AWS)

Preferred:

Experience working with APIs for data retrieval
Experience working with data warehouses and big data tools (e.g., BigQuery, Databricks)
Experience creating data/table architecture
Experience implementing QA processes and QA automation
Experience integrating data models within software
Experience working with marketing, analytics and customer data

Pay Transparency

At Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus. Actual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.

For our Sr. Analyst positions, our compensation ranges from $62,000 to $93,000, which is inclusive of a 15% bonus.

Benefits of Working at Ovative Group

We provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.

Culture:

Culture matters and we’ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We’re open in communication and floor plan. We’re flat – our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.

Compensation and Insurance:

We strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.

We’re rewarded fairly and when the company performs well, we all benefit.

Tangible amenities we enjoy:

Access to all office spaces in MSP, NYC, and CHI
Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams.
Flexible paid vacation policy
401k match program
Top-notch health insurance options
Monthly stipend for your mobile phone and data plan
Sabbatical program
Charitable giving via our time and a financial match program
Shenanigan’s Day

Working at Ovative won’t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it’ll be the most rewarding job you’ll ever have. If you think you can make us better, we want to hear from you!",2009,Advertising & Public Relations,$25 to $100 million (USD),Media & Communication,501 to 1000 Employees,Company - Private,True
Data Engineer,"Centriam
","Minneapolis, MN",$69K - $105K (Glassdoor est.),5.0,"As a Data Engineer, you will play a key role in designing and building reliable, scalable, and performant data solutions in support of our growing analytics practice. If you are passionate about the data analytics space and are excited by the opportunity to build innovative solutions in a fast-paced environment, this may be the role for you!

Key Responsibilities:
Build high quality data processing solutions using a variety of technologies
Implement complex data projects with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights
Solve problems that span multiple interconnected systems
Automate infrastructure while accounting for a large number of interdependent processes
Translate business requirements into technical requirements, and refine existing technical requirements as needed
Collaborate with cross-functional teams on various initiatives
Mentor, pair, and delegate work to encourage the growth of those around you
Design and implement scalable, low-latency, high-availability, and performant applications
Assist in other analytics and/or development work outside of the traditional data engineering role
Experience & Skills:
Proficient in Python
Strong SQL skills
Experience with cloud engineering on AWS
Expertise with Linux command line tools
Experience with RESTful APIs
Strong commitment to well designed and well maintained unit tests, BDD, TDD
Strong interest in building big data applications and solutions
Strong interest in learning new technologies, languages and skills
Excellent communication skills and technical knowledge-sharing habits
2+ years of relevant experience
Bonus Skills:
Data analysis
Solution architecture
Database performance tuning and optimization
Data visualization and charting frameworks experience
Bonus Technology Skills
Luigi
AWS Redshift, Lambda, API Gateway, ECS, Glue
Docker
Pandas
Document database technology (CouchBase, MongoDB, DynamoDB)
Why Work at Centriam?
Small analytics company focused on delivering excellence and value to our clients by providing high quality analytical and data-driven solutions.
Fun & collaborative culture; you will wear many hats and play an integral part in projects.
Exposure to a wide variety of data analytics, cloud, and open source technologies.
“Take what you need” vacation policy.
401k with generous match, vested immediately.
Free drinks, coffee, beer, and snacks.
Downtown Minneapolis office, connected to skyway.",2012,Advertising & Public Relations,Unknown / Non-Applicable,Media & Communication,1 to 50 Employees,Company - Private,False
Senior Network Engineer - Data Center,"Wells Fargo
","Minneapolis, MN",-1,3.7,"About this role:

Wells Fargo is seeking a Senior Network Engineer to join our Internal Hosting implementation team. This team provides the foundational global network infrastructure (data center routers and switches) which includes building capacity and managing the lifecycle of our internal network modules.

In this role, you will:
Lead or participate in implementing network security policies across routers and firewalls
Manage production networks including Internet Protocol backbone, data centers and edge pops across the globe
Ensure the continuous availability of all data network services
Identify gaps, risks and issues and navigate organizational structure to resolve them
Perform quarterly proactive network testing to ensure proper functioning and reliability of the network
Investigate and remediate network capacity related issues
Apply knowledge of security and regulatory policies to design and implement foolproof secured network solutions
Provide resolution information and work with other teams to complete impact analysis
Deliver comprehensive and maintenance plans for change management review and approval
Mentor and train network operations team in the installation, configuration, and maintenance
Partner cross-functionally with other Product Infrastructure teams in order to continuously improve and apply standards and policies relevant to operational excellence
Required Qualifications:
4+ years of Network Engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education
4+ years of experience with configuration of routing and switching latest platforms and solutions - Cisco Nexus and/or Arista product line
4+ years of experience designing LAN/WAN/Datacenter solutions for large enterprises
4+ years of experience with IP routing protocols (BGP, OSPF, RIP) in a large enterprise environment
Desired Qualifications:
Excellent documentation and verbal communication skills
Demonstrated skill with creating and/or updating technical design documentation used by engineering teams
Experience with Microsoft Office, Visio Professional
Experience working in an agile environment utilizing Atlassian Jira products
Strong understanding of the following routing protocols: OSPF, BGP, EIGRP
Bachelor's degree in Computers or Electronics with 7-9 Years of Networking Experience
CCNA, CCNP/DP or CCIE Certification preferred - Datacenter
Strong understanding of QoS configurations to support enterprise standards.
Strong experience in configuring Cisco and Arista Routers/Switches
Strong understanding of the following layer 2 switching protocols: Spanning Tree, Trunking, Etherchannel
Strong understanding of the following: HSRP, CBWFQ, DSCP, NAT/SNAT, TCP/IP, Multicast, Ethernet, EVPN, MLAG, CVP
Strong understanding of DNS/domain services
Python/Ansible/GITHUB experience a plus
Experience and familiarity with Change control processes - Service Now
Job Expectations:
Flexibility to frequently be on call beyond normal working hours
Ability to travel up to 10% of the time
Telecommuting is not an option for this position
This position offers a hybrid work schedule
Relocation assistance in not available for this position
This position is not eligible for visa sponsorship
Execute changes into the above environments via the Wells Fargo change control process, assist on mitigating risk for the enterprise by proactively addressing capacity problems or system related issues, participate in enterprise level projects from an engineering perspective and responsible for Design, reviewing, implementing, testing/validating, and researching industry best practices
This team works under a product model with dedicated teams supporting our products
We Value Diversity

At Wells Fargo, we believe in diversity, equity and inclusion in the workplace; accordingly, we welcome applications for employment from all qualified candidates, regardless of race, color, gender, national origin, religion, age, sexual orientation, gender identity, gender expression, genetic information, individuals with disabilities, pregnancy, marital status, status as a protected veteran or any other status protected by applicable law.

Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.

Candidates applying to job openings posted in US: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.

Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.

Drug and Alcohol Policy

Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.",1852,Banking & Lending,$10+ billion (USD),Financial Services,10000+ Employees,Company - Public,False
Data Engineer,"Piper Sandler
","Minneapolis, MN",$96K - $132K (Glassdoor est.),4.1,"At Piper Sandler, we connect capital with opportunity to build a better future.

We believe diverse teams with unique backgrounds, skills and experiences yield more innovative solutions. Our mission is to recruit, develop, retain and engage a diverse, high-performing team. Our business enables bright, committed people working in partnership within an environment that allows each person to achieve at a high level. We commit to encouraging and valuing inclusivity because every partner brings unique perspectives that help us better serve our clients.

We are currently looking for a Data Engineer in our Minneapolis, MN office.

Responsibilities include:

Prepare, cleanse and format datasets for processing and analysis
Build and maintain complex ETL pipelines
Interface with data scientists, software developers, and other analytics operations staff as needed
Contribute to software code reusability and knowledge management
Build complex ETL pipelines which are easy to maintain and enhance
Perform Managed File Transfer automation tasks supporting different protocols like SFTP, FTP, HTTPS etc.
Provide production support during business hours and non-business hours as needed
Liaise with other technology specialists and software engineers/developers to assist with quality assurance, code logic, data processing, system design, and disaster recovery/resiliency
Monitor production jobs, maintain and enhance ETL pipelines
Schedule the jobs using enterprise supported scheduling tool (Autosys)
Extract and ingest data using variety of methods including SOAP and REST APIs
Manage Tasks using Kanban/Azure Boards to collaborate with the team and to track progress

Requirements:

Four-year college diploma or university degree in computer systems or computer science
Certifications in relevant technology(ies) is a plus
5+ years of experience working in Data Engineering
5+ or more years of experience with relational databases like Oracle and MSSQL server
Experience working with managed file transfer products like Cleo Harmony or IBM Streling File Gateway
Proficient in linux/shell scripting
Proficient in Python
Experience in data integration and building complex ETL pipelines using Python
Experience working in ETL Tools like IBM Datastage, Informatica, Talend, SSIS
Experience working in Linux environments
Experience in the financial services industry
Experience working with large data sets
Experience working with different data formats like XML/JSON/CSV/BAI/FIX
Strong follow-through skills, keen attention to deadlines, and the ability to drive tasks to completion
Strong written and oral communication skills as well as strong listening and interpersonal skills
Highly self-motivated and ability to complete tasks with minimal supervision
Ability to multi-task, exercise excellent time management, and meet multiple deadlines
Keen attention to detail and ability to document information accurately
Willingness to learn new technologies
Desirable but not required - Knowledge/experience in cloud technologies like Kafka/Azure Synapse/Snowflake

As a leading investment bank, we enable growth and success for our clients through deep sector expertise, candid advice and a differentiated, highly productive culture.

Our human capital, technology, marketing and other corporate support teams work with our business partners to maximize each employee. We understand the dynamic nature of the industry and work alongside our company strategies. Learn more about our firm here .

Piper Sandler values a strong culture dedicated to the emotional and physical well-being of our employees. Learn more about our commitment to our employee’s health and well-being. Learn more about our benefits program and how we are here for our employees and their families today, tomorrow and beyond.

All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity, national origin, disability, age, marital status, status as a protected veteran or status with regard to public assistance.

LI-BSL",1895,Investment & Asset Management,$500 million to $1 billion (USD),Financial Services,1001 to 5000 Employees,Company - Public,False
Azure Data Engineer,"Inspire Medical Systems I
","Golden Valley, MN",$89K - $121K (Glassdoor est.),3.9,"Inspire Medical Systems has developed the only FDA-approved neurostimulation technology that transforms the lives of people with moderate to severe sleep apnea. We are a ground-breaking, fast-growing company where the patient’s outcome is first and foremost our top priority.

If you want to become part of a purpose-driven company and directly help to transform lives, this is the perfect career opportunity for you!

Position Summary:

The Azure Data Engineer should be experienced building in the Big Data space, using traditional, new, and emerging technologies. A good understanding of data modeling and SQL coding best practices is expected.

We are looking for a highly energetic and collaborative Azure Data Engineer with experience leading enterprise data modeling projects with Business and IT operations using Azure Analytics, Azure Data Warehousing and Azure Big Data products.


MAIN DUTIES/RESPONSIBILITIES:




You will produce high-quality, secure, and maintainable code in an agile environment
Learn and understand business processes with limited guidance
Work collaboratively as a member of the development team to build best-in-class software solutions in an agile environment
Support and research issues across all application layers and database
You will identify areas to improve and scale our Azure architecture and application design
Ensure code can be deployed using Azure DevOps
Design and query database tables, views, functions, stored procedures and batch processes
Develop, implement, and support interfaces that connect our websites, back-end systems, and various 3rd party cloud solutions

Required Qualifications:

Bachelor’s degree from an accredited college or University
Experience with private and public cloud architectures, pros/cons, and migration considerations
Minimum of 5 years of RDBMS experience
Experience with JSON, JSON-LD, XML data structures
Experience implementing data pipelines using latest technologies and techniques
Experience with SDLC products (JIRA, Confluence, Github, etc) or similar agile project management tools
5+ years of hands-on experience in programming languages such as Java 8, c#, node.js, python, SQL, Unix shell/Perl scripting etc.
At least 5 years of consulting or client service delivery experience on Azure
Bachelor’s or higher degree in Computer Science or related discipline
Experience handling structured and unstructured datasets
Expert in USQL, Java, Python, Hive SQL, Spark SQL, DataBricks or Snowflake
Strong t-SQL skills with experience in Azure SQL DW
Experience in Data Modeling and Advanced SQL techniques
Cloud migration methodologies and processes including tools like Azure Data Factory, Azure Synapse, Event Hub, etc.
Excellent problem solving, analytical, and critical thinking skills

Preferred Qualifications:

Master’s degree from an accredited college or University
Experience ingesting data from MS Dynamics CRM a big plus
Microsoft Azure certifications are a plus


Inspire Medical Systems provides equal employment opportunity (EEO) to all employees and applicants without regard to race, color, religion, creed, sex, national origin, age, disability, marital status, familial status, sexual orientation, status with regard to public assistance, membership or activity in a local commission, military or veteran status, genetic information, or any other status protected by applicable federal, state and local laws. This policy applies to all aspects of the employment relationship, including recruitment, hiring, compensation, promotion, transfer, disciplinary action, layoff, return from layoff, training and social and recreational programs. Inspire Medical Systems complies with applicable laws governing non-discrimination in employment in every location in which Inspire Medical Systems has facilities. All such employment decisions will be made without unlawfully discriminating on any prohibited basis.

Inspire Medical Systems is an equal opportunity employer with recruitment efforts focused on ensuring a diverse workforce. Applicants with a disability that are in need of accommodations to complete the Inspire Medical Systems application process should contact Human Resources at 844-672-4357 or email careers@inspiresleep dot com.

Inspire Medical Systems participates in E-Verify.",2007,Health Care Services & Hospitals,$5 to $25 million (USD),Healthcare,51 to 200 Employees,Company - Public,True
Sr AI/Data Science Engineer,"Medtronic
","Minneapolis, MN",-1,3.9,"Careers that Change Lives

Engineers and Scientists create our market-leading portfolio of innovations. Join us to make a lasting impact. Help bring the next generation of life-changing medical technology to patients worldwide. Together, we can change healthcare worldwide. At Medtronic, we push the limits of what technology can do to help alleviate pain, restore health and extend life. We challenge ourselves and each other to make tomorrow better than yesterday. Join us to make a lasting impact.

Medtronic is looking for an Sr. AI Data Science Engineer to join the Manufacturing Process Optimization Team. As a part of Global Operations and Supply Chain (GO&SC) - Operations Innovation, the Process Optimization team will deliver strategic manufacturing process performance improvements across the Medtronic enterprise. The Sr. AI/Data Science Engineer will work between the strategic programs in Process Optimization, the Digital Technology Team, and the Information Technology (IT) Team to develop, coordinate and deliver advanced analytics, data process solutions, and data insights to help steer our strategic work.



A Day in the Life

Responsibilities may include the following and other duties may be assigned.

§ Supports key analytics for OEE, SPC, Yield, and Capacity Modeling. Work with program leaders to identify business problems and propose data analytics solutions.

§ Help identify and develop cutting edge methods for data mining to develop new insights.

§ Liaison between various business, functional and/or technical development teams. Specifically, works between the Digital Technology team, the Information Technology (IT) team, and the Analytics team to help enable and grow existing data architecture platform(s).

§ Work to understand manufacturing process and equipment, understand machine requirements for data communication. Develop value stream, process, machine data hierarchy to enable data reporting requirements.

§ Develop and manage automated data subscriptions within the analytics process.

§ Enable standard digital capabilities for asset intelligence. Drive continuous improvements in operations through digitizing data, creating analytics and visualization, and providing data insights.

§ Ensure data hand-off between Process Optimization and Digital, IT teams are seamless with well-defined and standardized schema to ensure successful data visualization.

§ Focus on data accuracy and data governance for key analytics. Be the go-to person on the Process Optimization team, to work between the program leaders and the Digital, IT, Analytics teams.

§ Trouble shooting – Lead and/or assist various trouble shooting activities in Digital Technology and Analytics. Work between key stake holders in IT, OT, and Projects to ensure timely resolution of issues.

§ Develops and communicates descriptive, diagnostic, predictive and prescriptive insights/algorithms.

§ Manage workflow within and between multiple domain environments including testing development and production.

§ Experience managing teams for programming and implements efficiencies, performs testing and debugging.

§ Completes documentation and procedures for requirements, training, installation, and maintenance.

§ After data set and dashboard development: Adapts machine learning to areas such as virtual reality, augmented reality, artificial intelligence, robotics, and other products that allow users to have an interactive experience (or possible in sites with SPC tool in place)

§ Can work with large scale agnostic frameworks, data analysis systems and modeling environments.

§ Work within existing Quality System requirements to ensure data compliance.

There is no relocation assistance available for this role

Must Have: Minimum Requirements

Bachelors degree required
Minimum of 4 years of relevant experience, or advanced degree with a minimum of 2 years relevant experience
Nice to Have

§ Experience in developing Digital Analytics, Statistical Methods, Visualization Tools, and Analytics Insights.

§ Experience in relevant data-oriented technology solutions, for example Ignition, Spark, Python, SQL, PowerBI

§ Experience in Controls Engineering, Machine Learning, or similar.

§ Experience in Project Management, Governance, and Change Management.

§ Strong problem solver, strong analytical skills. Working understanding of industrial statistics.

§ Requirements Development - Able to formulate clear requirements contingencies, identify and manage risk and execute program objective autonomously.

§ Familiar with IT architecture, infrastructure, data engineering to empower data driven decision making.

§ Experience Driving business strategy through digital tools and dashboards.

§ Working knowledge of Industry 4.0 operational technology applications (MES/SCADA/IIOT/PLC/Controls)

TECHNICAL SPECIALIST CAREER STREAM

An individual contributor with responsibility in our technical functions to advance existing technology or introduce new technology and therapies. Formulates, delivers and/or manages projects assigned and works with other stakeholders to achieve desired results. May act as a mentor to colleagues or may direct the work of other lower-level professionals. The majority of time is spent delivering R&D, systems or initiatives related to new technologies or therapies – from design to implementation - while adhering to policies, using specialized knowledge and skills.


DIFFERENTIATING FACTORS

§ Autonomy: Seasoned individual contributor.

§ Works independently under limited supervision to determine and develop approach to solutions.

§ Coaches and reviews the work of lower-level specialists; may manage projects / processes.

§ Organizational Impact: May be responsible for entire projects or processes within job area.

§ Contributes to the completion of work group objectives, through building relationships and consensus to reach agreements on assignments.

§ Innovation and Complexity: Problems and issues faced are difficult, and may require understanding of multiple issues, job areas or specialties.

§ Makes improvements of processes, systems, or products to enhance performance of the job area.

§ Analysis provided is in-depth in nature and often provides recommendations on process improvements.

§ Communication and Influence: Communicates with senior internal and external customers and vendors.

§ Exchange information of facts, statuses, ideas, and issues to achieve objective, and influence decision-making.

§ Leadership and Talent Management: May provide guidance, coaching and training to other employees within job area.

§ Required Knowledge and Experience: Requires advanced knowledge of job area combining breadth and depth, typically obtained through advanced education combined with experience.


About Medtronic




Together, we can change healthcare worldwide. At Medtronic, we push the limits of what technology, therapies and services can do to help alleviate pain, restore health and extend life. We challenge ourselves and each other to make tomorrow better than yesterday. It is what makes this an exciting and rewarding place to be.

We want to accelerate and advance our ability to create meaningful innovations - but we will only succeed with the right people on our team. Let’s work together to address universal healthcare needs and improve patients’ lives. Help us shape the future.


A commitment to our employees lives at the core of our values. We recognize their contributions. They share in the success they help to create. We offer a wide range of benefits, resources, and competitive compensation plans designed to support you at every career and life stage. Learn more about our benefits here .

This position is eligible for a short-term incentive plan. Learn more about Medtronic Incentive Plan (MIP) on page 6 here .

The provided base salary range is used nationally (except in certain CA locations). The rate offered is compliant with federal/local regulations and may vary by experience, certification/education, market conditions, location, etc.




Physical Job Requirements

The physical demands described within the Responsibilities section of this job description are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. For Office Roles: While performing the duties of this job, the employee is regularly required to be independently mobile. The employee is also required to interact with a computer, and communicate with peers and co-workers. Contact your manager or local HR to understand the Work Conditions and Physical requirements that may be specific to each role. (ADA-United States of America)",1949,Health Care Products Manufacturing,$10+ billion (USD),Manufacturing,10000+ Employees,Company - Public,False
Sr. Data and Software Engineer,"Ovative Group, LLC
","Minneapolis, MN",$79K - $132K (Employer est.),4.1,"About Ovative Group

Ovative Group is the premier independent media and measurement firm in the United States. We help change-makers, in fast-growing, customer-centric organizations across industries reinvent their marketing and measurement programs. We leverage our media, measurement, and consulting capabilities to help brands like Coach, Kate Spade, Stuart Weitzman, Facebook, The Home Depot, CVS, Disney, and UnitedHealth Group transform their media and marketing programs. Our proprietary approach to measuring and optimizing marketing investment decisions, Enterprise Marketing Return (EMR), is disrupting the industry and setting the gold standard for customer and marketing strategy, activation, and measurement.

Recognized eight consecutive years on Star Tribune’s list of Top 150 Workplaces and five years on Inc. 5000’s list of the fastest-growing private companies in America, we pride ourselves in always overdelivering for our clients, our teams, and our communities.

About the Role

We are seeking a Sr. Data & Software Engineer to join our rapidly growing product and engineering development team. Our company specializes in enterprise-level media measurement and optimization across various industries. In this role, you will take a leadership position within a cross-functional team responsible for creating, optimizing, and maintaining scalable software and data solutions to enhance performance, stability, and scalability. You will play a pivotal role in guiding the team and working closely with stakeholders throughout the entire software development lifecycle, from concept to deployment.

The ideal candidate will bring extensive experience in iterative development practices, deep knowledge of version control (e.g., GitHub), and both conceptual and pragmatic problem-solving skills. Your outstanding attention to detail and strong written and oral communication skills will enable you to work directly with a variety of users, understanding their objectives and translating them into technical requirements and solutions. You will have the opportunity to mentor junior team members, contribute to innovative ideas, and drive the adoption of cutting-edge technologies within the team and foster your own professional growth.

Responsibilities:

Lead the design, development, testing, and deployment of robust software solutions that meet business and technical goals.
Lead effort in identifying opportunities for automation with a focus on the operational stability of software applications and systems
Engage directly with Product Managers and stakeholders to understand their business goals, gather requirements, and translate into detailed, actionable technical requirements
Research, write and edit documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports
Evaluate trade-offs between correctness, robustness, performance and customer impact to ensure we build the right solutions that scale
Mentor and level up the skills of your teammates through collaboration and by sharing your expertise
Ensure adherence to and advancement of software and product engineering best practices, including code quality, documentation, testing, security, deployment, and performance.
Evaluate and integrate new technologies, tools, and frameworks to optimize the software development process.
Troubleshoot complex software issues and provide effective solutions.
Provide guidance, mentorship, and contribute to architectural decisions, code reviews, and project direction.
Foster a culture of continuous learning to keep your team current with emerging technology and software engineering trends.

Requirements:

5+ years of relevant data & software engineering development experience
Highly proficient working with ETL/ELT tooling for large data sets (e.g., Airbyte)
Highly proficient utilizing SQL, Python, and command line
Experience with cloud-based platforms (i.e. GCP, AWS)

Preferred:

Experience working with marketing, analytics and customer data
Experience working with APIs for data retrieval
Experience working with data warehouses and big data tools (e.g., BigQuery, Databricks)
Experience creating data/table architecture
Experience implementing QA processes and QA automation
Experience integrating data models within software

Pay Transparency

At Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus. Actual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.

For our Manager positions, our compensation ranges from $79,000 to $132,000, which is inclusive of a 20% bonus.

Benefits of Working at Ovative Group

We provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.

Culture:

Culture matters and we’ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We’re open in communication and floor plan. We’re flat – our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.

Compensation and Insurance:

We strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.

We’re rewarded fairly and when the company performs well, we all benefit.

Tangible amenities we enjoy:

Access to all office spaces in MSP, NYC, and CHI
Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams.
Flexible paid vacation policy
401k match program
Top-notch health insurance options
Monthly stipend for your mobile phone and data plan
Sabbatical program
Charitable giving via our time and a financial match program
Shenanigan’s Day

Working at Ovative won’t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it’ll be the most rewarding job you’ll ever have. If you think you can make us better, we want to hear from you!",2009,Advertising & Public Relations,$25 to $100 million (USD),Media & Communication,501 to 1000 Employees,Company - Private,True
Sr. Data Engineer,"SPS COMMERCE
","Minneapolis, MN",$80K - $109K (Glassdoor est.),4.2,"Description:
SPS Commerce is hiring a Sr. Data Engineer to join our team of highly experienced software development experts. The Sr. Data Engineer will develop, maintain, test and evaluate big data solutions within a canonical and standards ecosystem. Drive data quality based on means of data entry, storage, and management. With attention to detail, shorten the implementation life cycle, and improve understanding of the data for internal and external analysis. In the process, discover meaning in the data itself that drives value for our customers.

Based in our office in downtown Minneapolis, our hybrid work model provides the best of both worlds. We #succeedtogether through in person collaboration, balanced with remote work to provide flexibility.

Why join SPS Commerce?

We solve retail supply chain problems by cutting through inefficiency with innovation and automation. At SPS we empower retailers, suppliers, distributors, grocers, and logistics partners to work better together with our people, our process, and our tech products. We have the world’s largest retail network, and we don’t just lead the industry, we are the industry.

At SPS, we believe every employee makes a difference. We ensure employees have the tools, resources, and training to explore new ideas and execute them. Our success comes from playing as a team and always playing to win. Careers don’t just grow here, they’re made here.


Does this sound like you?

You enjoy troubleshooting and fixing problems
You are Interested in databases and data structures
You are Interested in infrastructure-as-code and infrastructure automation
You want to directly influence the success of a winning organization
What is the day to day like?


Acquire datasets for analytics use and supply appropriate documentation for team use
Extract, transform, load and model data with the purpose of understanding or drawing conclusions to drive decisions making within the RSX community and for external customers
Perform data profiling tasks (query, mining, etc.) to determine risks to proposed analytics solution based on quality of, and anomalies within, existing data sources
Interface internally and externally to understand business rules and identify gaps across data definitions. Perform data cleaning and data reduction using knowledge of the client’s business rules
Create source to target maps, data audit, or other reference documents and deliver as input into next workflow. Data dictionaries will have concise, consistent, and unambiguous definitions
Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations
Collaborate on design & development with programmers and business analysts

What experience is required?




Bachelor’s Degree in CS, CE, MIS, Mathematics, related field or equivalent years of experience
5+ years of experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development
2+ years’ experience developing or using controlled vocabularies.
Extensive experience using document database stores
Proficient in one or more of the following or their equivalent. Spark, Hadoop, Hive and Pig
Proficient in one or more of the following: PHP, Java, or Python and a familiarity with Node.js
Knowledge of the use of statistical methods, programming logic, No SQL databases such as HBase, Cassandra, MongoDB, familiarity with SQL
Experience in metadata related field or work and/or a masters’ degree in information science, library and information science (specifically metadata or a related field), or a related degree.
Extensive experience or certificate/training in using W3C standards related to linked and canonical data and ontologies. In particular JSON, XML, DITA, RDF, RDFS, OWL, and/or SKOS or relevant subsets of those standards
Curious, resourceful, strong attention to detail and productive working independently and collaboratively
Nice to haves


Professional certification as Certified Data Management Professional (CDMP) credential, Google’s Certified Professional in data engineering, IBM Certified Data Engineer in big data, the CCP Data Engineer from Cloudera, and the Microsoft Certified Solutions Expert credential in data management and analytics.
Prior experience in change control / change management role
Multilingual terminologies, taxonomies, ontologies, or scholarly articles.

SPS Commerce offers a comprehensive package of benefits including health, dental, vision, disability and life insurance, paid time-off, 401(k), health and flexible spending accounts, stock purchase plan and more.

EOE including disability/ veteran",1982,Computer Hardware Development,$100 to $500 million (USD),Information Technology,1001 to 5000 Employees,Company - Public,False
Data Platform Engineer Lead,"Bremer Bank
","Lake Elmo, MN",$96K - $138K (Glassdoor est.),3.8,"Why Work at Bremer?



Are we a bank? Absolutely. But when it comes to careers, you'll find that there's a lot more to our business than you might expect. We’re passionate about helping customers throughout Minnesota, Wisconsin and North Dakota succeed and grow. Our commitment to authentic and enduring relationships starts by taking time to listen and learn about our customers’ goals, and innovate to solve their challenges, not only through financial resources but also deep knowledge, insights and ideas.

We expect, encourage and reward entrepreneurial thinking from our employees. We have a shared purpose of cultivating thriving communities, inspired by our founder, Otto Bremer, who spent his lifetime supporting the communities that his banks served in new and unexpected ways. That commitment to community and creative problem-solving contributes to Bremer’s culture.

If you want the next chapter in your story to be one where you make a real difference for our customers, communities and economies, we’re the bank to join. We foster diversity, equity and inclusion, building on the legacy of our immigrant founder. If you’re curious, enjoy collaborating, and start each day with the customer in mind, we invite you to join our team!

Job Description



Are you passionate about solving complex data challenges using cutting-edge technologies? Do you love working with technology in a fast-paced, dynamic environment where you contribute to innovative solutions? If so, we want you to join our team as a Data Engineer Lead!


You should have strong leadership and communication skills. You must be able to collaborate with other teams, effectively manage projects, and communicate technical information to non-technical stakeholders.

The Data Engineer Lead will develop and maintain data integration processes to ensure complete, accurate and timely data to the company. This position is part of the Data Services team enabling Data-as-a-Service support based on varying levels of Data Service Level Agreements. The Data engineer Lead will be part of a dynamic team adopting modern data architecture to improve the data management processes and workflows as well as data presentation capabilities. The Data engineer Lead will develop and maintain processes to support data feeds and batch data integrations needs across the company. They provide support to the Analytics, Data Warehouse, Business Intelligence and Cloud Engineering teams as well as several business applications with high priority data needs.




The ideal candidate will have extensive experience building data pipelines, extensive experience data wrangling and is someone who enjoys building and optimizing data systems. They must be self-directed and comfortable leveraging modern data architectures to support the data needs of multiple teams, systems, and products.

The ideal candidate will have a B.S. degree in Computer Science / Engineering with a minimum of 10 years of experience or High school diploma or GED and 13 years of experience with data management platforms, data warehousing technologies, business intelligence technologies, advanced analytics technologies, reporting tools, and experience with one or more ETL tools. They must demonstrate knowledge of data warehouse design, source to target specifications, prototyping data sets, process analysis, and critical thinking skills. Excellent communication skills and the ability to communicate in a clear and concise manner.


The position requires performing in depth ad-hoc data analysis and the ability to organize, integrate, and analyze large amounts of data both for operational support as well as for quality assurance and solution design purposes. This position requires the ability to work in a collaborative environment, follow pre-defined patterns and know when exceptions need to be made, the ability to follow, define, communicate, and improve development patterns. Understanding of “impact analysis” and how to properly identify potential consequences of a change.




Experience:

Strong expertise Microsoft SQL Server and TSQL
Strong expertise Microsoft SQL Server Integration Services
Experience with extract-transform-load (ETL) Tools like Informatica or Wherescape Red
Experience working with and supporting Data Virtualization and Data Visualization tools.
Knowledge of NoSQL technologies such as Elastic Search or Cassandra.
Experience working with large and complex data sets with multi-terabyte scale.
Knowledge of Agile and DataOps



Bremer Values & Benefits



Our Values

Collaboration: We work elbow-to-elbow with our customers, making a difference for them in good times and bad. We share our skills, knowledge and experience to support each other every day.

Commitment: We’re deeply invested in the places we live and work, and we’re always ready to do whatever it takes to help our neighbors and communities thrive and grow.

Creativity: Imagination and innovation are essential to helping our customers and Bremer succeed. If there’s a better way to do something, we’ll find it or create it.


Our Benefits

At Bremer, we are committed to supporting and growing the communities we call home. Our people understand that they represent our name and reputation in every conversation, and that they’re backed by an organization that values finding a better way to get things done. Let’s talk about benefits. Not just the 401(k), pension plan, healthcare, and vacation kind (we’ve got those), but the kind of everyday benefits that get you excited for what each day holds.

Equal Employment Opportunity



Bremer Bank is committed to a diverse and inclusive workforce. We’ve built an organization of experienced, intelligent and collaborative people from a range of all backgrounds, and all of them contribute something essential to helping our customers thrive and grow.

All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by applicable law.

For your reference, here are additional details for your awareness:
Know Your Rights: Workplace Discrimination is Illegal (dol.gov)
Family and Medical Leave Act (dol.gov)
Employee Polygraph Protection Act rights (dol.gov)

Miscellaneous Items to Note

Must be legally authorized to work in the U.S. without Bremer sponsorship for employment visa status (e.g., H1B status, 0-1, TN, EB-2)
Notice to California Applicants - for your awareness, please see Bremer Bank California Notice at Collection and Privacy Policy
Accessibility Commitment - Bremer is committed to accessibility, diversity and inclusion for all our applicants. We believe everyone should be able to have equal and effective access to our websites and our services. We have worked very hard to ensure that our website and digital content meet the needs of our applicants and visitors. Click here for more information on our accessibility and how to get in touch with us.
Notice to Third Party Recruiters - Bremer Bank, National Association, including its affiliates, does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed agreement for recruiting or placement services, Bremer will have no obligation and will not agree to pay any referral or recruiting fee. In the event a recruiter or agency submits a candidate for consideration, Bremer explicitly reserves the right to pursue and/or hire any such candidate(s) without any financial or other obligation to the recruiter or agency. Any unsolicited resumes, including those submitted to hiring managers, will be deemed to be the property of Bremer.",1943,Banking & Lending,$10+ billion (USD),Financial Services,1001 to 5000 Employees,Company - Private,False
Data and Development Engineer,"Bold Orange
","Minneapolis, MN",$62K - $93K (Glassdoor est.),4.4,"Role Location: Greater Minneapolis/St. Paul area (partial in-office requirement)
Bold Orange is a customer experience company. We believe authentic human connections are the single most important driver of business and societal progress. We exist to create these connections across the customer lifecycle, from acquisition to engagement to retention. Our culture is one of curiosity, collaboration, proactivity and always bringing the outside perspective to our customers.
Position Summary:
Bold Orange is seeking a Data and Development Engineer with a passion for developing technical solutions related to various aspects of data lineage. This person will design and execute data acquisition, process automation, and support analytics & insights data service capabilities by managing large behavioral datasets.
The ideal candidate will blend software development experience with a data architect’s mindset to enable data analytics, support web development projects and help create technical connections across customer experiences. Ideally, logical, organized, pragmatic, and methodical to their core. To be successful they will collaborate closely with technology services, analytics, our emerging SFMC practice, and other cross-functional partners to accelerate data collection, transformation, and automation of processes that require behavioral marketing and digital analytics data.
Position Duties and Responsibilities:
Design, develop, and execute technical solutions to automate data processes and increase the efficiency of analytic deliverables
Support data acquisition efforts from digital execution platforms both ad-hoc in nature, as well as at a system level where feasible.
Develop data and analytic views to support capabilities around modeling, analysis, testing & optimization, and insight development
Collaborate on data acquisition and data transformation needs for analytics, personalization, and customer data platforms
Understand, monitor, and enhance data flow(s) to enable reporting and insights development, including tracking data quality
Articulate requirements and present solutions to less-technical stakeholders
Maintain knowledge of technical changes impacting data availability, new technologies and tools, privacy practices, and relevant tools
Work with front-end web developers to create and support back-end web solutions
Develop, maintain, and deploy new and existing REST API’s
Continuously inform business partners of work in process and potential roadblocks to timelines
Required Qualifications & Experience:
3+ years of professional experience in data engineering, data management, data development, software development or related field
Bachelor’s degree in computer science, or related discipline
Advanced proficiency with MSSQL, Amazon Redshift, and MongoDB
Demonstrated expertise/applied knowledge of C#, Python, and PowerShell
Experience consuming and developing REST API’s and related software interfaces
Demonstrated experience with back-end web development, data architecture, structured and unstructured data management, and data transformation
Experience with Visual Studio, C#, PHP scripting, general development patterns, and related frameworks
Ability to thrive in a highly collaborative, fast-paced environment where the nature of the work can be variable
Innately curious and happy to dig in whenever called upon
Who We Are:
Our tone is professionally sassy. We embrace meat raffles, hot seats, and the occasional Jell-O shot. We like staff meetings that are informative, educational, and at times, damn funny. We believe in no hierarchy, no bullshit, and no politics. Just honest, hard work, and great fun.

Equal Opportunity Employer:
We are an equal opportunity employer, dedicated to a policy of nondiscrimination in employment on any basis including race, color, creed, gender, sexual orientation, age, disability, religion, national origin, marital status, familial status, ancestry, status as a veteran, status with regard to public assistance and any other characteristic protected by law. Bold Orange does not and will not discriminate against employees, prospective employees, clients, or vendors.",2018,Advertising & Public Relations,Unknown / Non-Applicable,Media & Communication,51 to 200 Employees,Company - Private,False
"AWS Data Engineer ( Hybrid at Eagan, Minnesota)","Cognizant Technology Solutions
","Eagan, MN",$72K - $103K (Glassdoor est.),3.8,"We are Cognizant Artificial Intelligence

Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. However, clients need new business models built from analyzing customers and business operations at every angle to really understand them.

With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate, and scale the most desirable products and delivery models to enterprise scale within weeks.




*You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future *




AWS Data Engineer (Hybrid at Eagan , Minnesota)




2 to 4 years




Technical Skills

AWS
SQL
ETL



Nice to have skills.




AWS Glue



Job Summary

Strong Healthcare experience with Data Model and ETL experience
Experience with a variety of AWS Cloud platform services that include but not limited to: Lambda NodeJS Dynamo DB SNS SQS RDS S3 CloudWatch and API Gateway
Experience with developing Microservices and building APIs
Strong experience in Java Scala and at least one JavaScript framework like Angular.js or Node.js React.js Vue.js or Ember.js
Responsible for building data solutions on the AWS cloud technology



Roles/Responsibilities:

ETL experience
Cloud native database experience – RDS Redshift DynamoDB
Knowledge of the health care/health insurance industry is a plus
Big Data and NoSQL experience
Automation experience
Experience driving and developing medium complex data integration solutions using two or more of data management software tools.
Experienced knowledge in data analysis using SQL user story development data warehouse design and modeling (3rd normal form or dimensional modeling) data integration design and development and/or performance tuning Teradata SQL.
Knowledgeable in data warehouse market best practices development standards and methodologies.
Demonstrated ability and willingness to play multiple roles for different projects e.g. planning/architecting and overseeing development for some projects being a hands-on technical resource for others.
Not only leading and working on strategic and tactical projects but also understand the importance of details and remaining hands-on to data applications/solution business enablement.
Experienced in work estimation design review and code review.
Strong skills in cross team coordination for project execution and issue resolution as well as working with contracting vendor onsite and offshore.
Demonstrated oral/written communication and presentation skills to effectively communicate medium complexity technical information.
Demonstrated ability to influence motivate and lead teams with diverse cultural and professional background.
Experience in mentoring and coaching junior developers in problem solving and critical thinking efforts.



Primary Location

Eagan, Minnesota, Hybrid 2 days per week.




Salary and Other Compensation:

The annual salary for this position is depending on experience and other qualifications of the successful candidate.

This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.




Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:

Medical/Dental/Vision/Life Insurance
Paid holidays plus Paid Time Off
401(k) plan and contributions
Long-term/Short-term Disability
Paid Parental Leave
Employee Stock Purchase Plan



Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.




LI-MA1 #CB #Ind123

Employee Status : Full Time Employee

Shift : Day Job

Travel : No

Job Posting : Nov 21 2023

About Cognizant
Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.

Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.

Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.

If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.",1994,Information Technology Support Services,$10+ billion (USD),Information Technology,1001 to 5000 Employees,Company - Public,False
Senior Data Engineer,Erstom,"Minneapolis, MN",$109K - $145K (Glassdoor est.),-1.0,"Job Description:

You will be part of a team building the next generation data warehouse platform and will design, develop, and maintain complex extract, transform, and load (ETL) data pipelines using large heterogeneous datasets. You will also build data engineering solutions for complex data models that express business processes. Your expertise with leading technologies and tools such as Oracle, Postgres, Python, etc. will result in a valuable modern data warehouse that supports critical business decisions and data analysis processes. Your collaboration and communication skills will help to establish stakeholder relationships and ensure that your work products are in alignment with project goals. Most importantly, you will be passionate about working with data and will be a significant contributor.

Responsibilities:
Design, develop, and automate scalable data engineering solutions by leveraging cloud infrastructure. Extend or migrate existing data pipelines to new cloud environment.
Lead technical projects involving design and development of data pipelines for complex datasets. Document project plans, outline tasks and milestones, provide estimation of effort.
Work closely with business partners to devise and manage data pipelines, load frequency, data delivery mechanisms, and performance tuning.
Identify and implement best practices for data engineering and software development to ensure quality delivery of enterprise solutions.
Help enable team alignment by participating in code reviews, change management and team meetings.
Develop and maintain detailed technical documentation of data engineering solutions.
Collaborate with key stakeholders, both internal and external, including enterprise data architect, data modelers, and subject matter experts (SMEs).
Qualifications:
Five or more years of professional experience as data engineer. Bachelor’s degree in Computer Science or equivalent experience.
Demonstrated experience in data warehousing and ETL development.
Experience building complex data pipelines using large, disparate data sources.
Demonstrated expert knowledge in SQL.
Demonstrated experience working with relational databases such as Oracle, Postgres and other modern database technologies.
Proficiency in modern programming languages such as Python, R, Java.
Thorough understanding of data movement and transformation tools, such as Informatica, Datastage or equivalent.
Demonstrated experience in selecting tools, methods, techniques, and evaluation criteria for designing optimal data engineering solutions.
Demonstrated experience in leading complex technical projects, including assigning tasks and selecting team members.
Ability to make technical presentations to teams, focus groups, management, and governance committees.
Excellent customer service, communication and collaboration skills.
Preferred Qualifications:
Five years or more experience as data engineer designing and implementing complex data pipelines.
Master’s degree in Computer Science, Information Technology or related field.
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.).
Experience with AWS technologies.",-1,-1,Unknown / Non-Applicable,-1,1 to 50 Employees,Company - Private,False
Principal Data Engineer,"C.H. Robinson
","Eden Prairie, MN",$96K - $130K (Glassdoor est.),3.6,"Are you passionate about shaping the future of logistics through data-driven insights and cutting-edge cloud technologies? Join our Fortune 500 Global Logistics Company on our NAST Data Estate Team as a Principal Data Engineer and play a crucial role in transforming our data ecosystem into a modern, scalable, and high-performance cloud-based infrastructure. As part of our dynamic data engineering team, you will have the opportunity to collaborate with leadership and other professionals, leverage state-of-the-art cloud tools, and drive innovation that shapes the logistics industry worldwide.

This role may be located in Minneapolis-Saint Paul, Kansas City, Seattle, or Chicago.

Responsibilities:

Collaborative Teamwork: Collaborate with other senior level engineers/architects, business stakeholders, and analytics teams to help shape and influence our modern data strategy leveraging cloud-first technologies by eliciting requirements and delivering results
Cloud Data Architecture: Design, architect, and with partnership among your team develop data architectures to support our business needs and objectives. Using modern cloud data warehouses and supporting technologies to enable efficient costs and value driven products
Data Engineering: Create ELT/ETL pipelines that allow us to retrieve valuable data from both internal and external platforms, transform it into a value-added product, and make it available for consumers. Establish and implement best practices to ensure data quality, reliability, and cost responsibility
Data Modeling and Optimization: Design and implement data models that align with business requirements and enhance data accessibility for reporting and analytics. Optimize data structures and queries to improve system performance and reduce latency.
Continuous Improvement: Proactively identify technologies or solutions that differ from current technology stack or represent innovative uses of existing technologies. Construct and deliver proposed solution strategies for potential new technologies
Mentoring: Provide technical guidance and mentorship to engineering teams through technical POCs, shared frameworks, and desired best practices and patterns for software development, delivery, and management

Required Qualifications:

8+ years of experience in data engineering (4+ years cloud), utilizing modern platforms (AWS, Azure, Google Cloud Platform)
2+ years of experience with cloud data warehouses (Snowflake, Redshift, BigQuery)
Experience building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development, and performance optimization
Proficiency in SQL and Python and hands on experience with big data frameworks (Spark, Hadoop), NoSQL technologies (ElasticSearch, MongoDB), and streaming technologies (Kafka, Azure Event Hub)
Experience with cloud data replication and integration tools (Matillion, Fivetran, dbt, Airflow/Astronomer, LookML)
Bachelor’s degree from an accredited college or university in Computer Science, Software Engineering, IS, MIS, or other technology degree or minimum 4 years of equivalent work experience and high school diploma/GED

Preferred Qualifications:

Proven record of strong decision-making, analytical skills, conflict resolution, follow through, and building technical relationships with senior executives and business stakeholders
Experience with containerization and orchestration tools (Docker, Kubernetes)
Experience establishing testing patterns, acceptance testing criteria, and reviewing others' automated tests
Familiarity with data governance principles, data security, and compliance with global data protection regulations (ALTR, Alation, Collibra, Atlan)
Experience with Business Intelligence tools (PowerBI, Sigma, Tableau, Alteryx, Thoughtspot)

Questioning if you meet the mark? Studies have shown that women and people of color may be less likely to apply unless they match the job description exactly. Here at C.H. Robinson, we’re building a diverse and inclusive workplace where all employees feel they belong. If this position excites you, we welcome you to apply whether you check all the preferred qualifications or just a few. You may just be our next great fit!

Equal Opportunity and Affirmative Action Employer

C.H. Robinson is proud to be an Equal Opportunity and Affirmative Action employer. We believe in equality for all and celebrate the diversity of our employees, customers and communities. We believe this increases creativity and innovation, drives business growth and enables engaged and thriving teams. We’re committed to providing an inclusive environment, free from harassment and discrimination, where all employees feel welcomed, valued and respected.

Affirmative Action Employer/EOE/M/F/Disabled/Veteran

Benefits

Your Health, Wealth and Self

Your total wellbeing is the foundation of our business, and our benefits support your financial, family and personal goals. We provide the top-tier benefits that matter to you most, including:

Two medical plans (including a High Deductible Health Plan)

Prescription drug coverage

Enhanced Fertility benefits

Flexible Spending Accounts

Health Savings Account (including employer contribution)

Dental and Vision

Basic and Supplemental Life Insurance

Short-Term and Long-Term Disability

Paid and floating holidays

Paid time off (PTO)

Paid parental leave

Paid time off to volunteer in your community

Charitable Giving Match Program

401(k) with 6% company matching

Employee Stock Purchase Plan

Plus a broad range of career development, networking, and team-building opportunities

Dig in to our full list of benefits on OUR CULTURE page.


Why Do You Belong at C.H. Robinson?




Standing out among the world’s largest logistics platforms, C.H. Robinson solves logistics problems for companies across the globe and across industries, from the simple to the most complex. For 100+ years, our global suite of services has innovated trade to seamlessly deliver the products and goods that drive the world’s economy. With 20 million shipments annually for 100,000 customers, and millions of dollars contributed to support causes that matter to us, our people and technology literally move the world.

As a FORTUNE 200 company, FORTUNE has also named C.H. Robinson one of the World’s Most Admired Companies 2022. Headquartered in Eden Prairie, Minnesota, we are proud to be recognized as one of LinkedIn’s Top Companies in Minneapolis-St. Paul 2021. And we’re not stopping there… Join us as we collaborate, innovate, and work as one global team to make life better and more sustainable for our customers, communities, and world.",1905,Shipping & Trucking,$10+ billion (USD),Transportation & Logistics,10000+ Employees,Company - Public,False
Senior Data Engineer,"Self Esteem Brands
",United States,-1,4.2,"Self Esteem Brands, LLC, parent company of Anytime Fitness, Waxing the City, Basecamp Fitness, The Bar Method, and Stronger U, is seeking an experienced and passionate professional to join our talented team as a Senior Data Engineer. At Self Esteem Brands, we offer a fun, fast growing, inspirational culture that incorporates a flexible, hybrid work schedule.

Job Summary

Self Esteem Brands is at the forefront of a digital transformation. We're not just evolving; we're revolutionizing our data infrastructure. Our goal? A modernized data platform that is more than ""the data warehouse"" and embodies principles of a data mesh culture to better serve our internal and global customers. We are looking for a visionary Senior Data Engineer, someone well-versed in data platforms such as Databricks or Snowflake, cloud environments (preferably azure) and and keen to lead this transformative journey as we look to enhance our capabilities to support our multi-brand, global organization that incorporates many different sources, velocities and volumes of data

Purpose/Impact: (Duties & Essential Functions)

Spearhead and evolve the design, execution and technology selection of our data platform to enable simple onramp and consumption of our data assets.
Champion the transition to a data mesh culture, promoting domain-oriented design and decentralized data ownership.
Lead initiatives to evolve our Data Operations/Observability, Discovery, and Lineage practices, becoming a champion for data transparency and accuracy.
Drive exploration and potential implementation of data lakehouse architectures, focusing on simplifying management and addressing core data concerns.
Be a advocate for the data platform and a conduit with analytic engineers, analysts, bi developers.
Mentor and nurture junior data engineers, guiding them towards best practices.
Develop bespoke SQL-based data solutions, tailored for a decentralized data environment.
Engage with cross-functional domain teams, ensuring their needs are central to our data strategies.
Keep abreast of industry shifts and emerging technologies, ensuring our data platform remains cutting-edge.

Strengths and Background

Bachelor's Degree in Computer Science, Engineering, or related experience.
5+ years of experience in a data engineering role, with history of implementing data platform modernization.
Solid experience and understanding of architecting, designing, and operationalization of large-scale data and analytics solutions on Cloud Data Warehouses such as Snowflake, Databricks, Google BigQuery, or AWS Redshift is a must.
Experience with data integration, transformation and orchestration frameworks such as Airbyte, dbt, Fivetran, Segment or similar
Experience with Microsoft data stack (azure sql, azure data factory, synapse, power bi)
Advanced proficiency in SQL and familiarity with domain-oriented data design.
Expertise in programming languages such as Python, or Scala.
Use of tools such as Git, GitHub Actions, Agile, Jira and Confluence or similar
Familiarity with BI tools such as Tableau, PowerBI
Demonstrable expertise in data observability, discovery, and lineage methodologies.
Proven leadership capabilities and an evangelist for data mesh culture.
Experience with AI and ML platforms is a bonus.

What’s in it for you:

Medical, Dental and Vision Coverage
401(K) Savings Plan
Paid Parental Leave
Coaching & Therapy Sessions
Brand Discounts & Reimbursements
Professional Development Opportunities",-1,Beauty & Wellness,$25 to $100 million (USD),Personal Consumer Services,201 to 500 Employees,Company - Private,True
Sr. Azure Data Engineer,"Iron Systems
","Minneapolis, MN",$102K - $139K (Glassdoor est.),3.3,"Date Posted:
11/9/2023


Job Function:
Software Development


Location:
Minneapolis MN - USA


Offered Salary:
Competitive






Iron Systems is an innovative, customer-focused provider of custom-built computing infrastructure platforms such as network servers, storage, OEM/ODM appliances & embedded systems. For more than 15 years, customer have trusted us for our innovative problem solving combined with holistic design, engineering, manufacturing, logistic and global support services.

Job Title: Sr. Azure Data Engineer
Location: Minneapolis, MN (100% Remote)

Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.",1987,Information Technology Support Services,$25 to $100 million (USD),Information Technology,201 to 500 Employees,Company - Private,False
Sr Engineer - Advanced Data Storage Physicist,"Seagate Technology
","Bloomington, MN",$110K - $154K (Glassdoor est.),3.6,"About our group:


The Advanced Data Storage and Memory Devices group is responsible for guiding component and system design for the Company’s ten-year product roadmap – providing insight that informs both near term and +100 TB HDD designs. It does so through the execution of advanced experiments as proof of concept of advanced magnetic recording and emerging memory technologies. The team also performs modeling and simulation of magnetic, optical, thermal, and transport phenomena to understand experimental observations and guide future experiments and designs. Machine learning and artificial intelligence is used extensively for analyzing experimental data, model acceleration, and design optimization.


This position will focus on research into advanced recording heads, media, and system designs for high-capacity hard drives. This will likely involve extensive experimentation and/or modeling using state-of-the-art tools, as well as looking into innovations and novel concepts from academia and other industries and translating them into HDD designs. Leveraging of AI/ML tools will be expected for accelerating design and understanding of physical mechanisms.

About the role - you will:



Conduct research on advanced energy assisted magnetic recording systems.
Work closely with engineers and scientists from different organizations within Seagate to deliver design ideas for the Company’s ten-year roadmap.
Develop models and/or perform experiments to explore advanced recoding physics.
Contribute to Seagate’s IP portfolio in the form of invention disclosures, patents, trade secrets, peer-reviewed journal articles, and/or white papers, as well as participation in academic conferences

About you:



You must be able to manage multiple tasks concurrently.
You must be able to work and communicate effectively with a diverse group of engineers and technicians.
You have demonstrated innovation and are a self-motivated worker.
You have an intense desire to learn.
You must have excellent verbal and written communication skills.

Your experience includes:

Experience in programming languages, data analysis software, and machine learning frameworks preferred.
A Ph.D. in an engineering or scientific discipline.

Location:


Our Normandale campus spans two suburbs, Edina and Bloomington, and serves as the Recording Head development and manufacturing arm of Seagate. Located in the heart of a bustling community, Seagate offers an on-site café, or if you prefer, you can drive to one of many restaurants just minutes away. Need to grab a gift over lunch time? Shopping is abundant in the area. If working out is your thing, the on-site fully equipped fitness center hosts wellness programs, outdoor activities, tournaments and group workouts. Looking for something more laid back? Reset in one of our meditation rooms, or simply take a walk around our indoor walking path. On-site cultural festivals, celebrations and community volunteer opportunities also abound.


Location: Normandale, United States
Travel: None",1979,Computer Hardware Development,$10+ billion (USD),Information Technology,10000+ Employees,Company - Public,False
Cloud Data Engineer 3,"Bremer Bank
","Lake Elmo, MN",$87K - $125K (Glassdoor est.),3.8,"Why Work at Bremer?



Are we a bank? Absolutely. But when it comes to careers, you'll find that there's a lot more to our business than you might expect. We’re passionate about helping customers throughout Minnesota, Wisconsin and North Dakota succeed and grow. Our commitment to authentic and enduring relationships starts by taking time to listen and learn about our customers’ goals, and innovate to solve their challenges, not only through financial resources but also deep knowledge, insights and ideas.

We expect, encourage and reward entrepreneurial thinking from our employees. We have a shared purpose of cultivating thriving communities, inspired by our founder, Otto Bremer, who spent his lifetime supporting the communities that his banks served in new and unexpected ways. That commitment to community and creative problem-solving contributes to Bremer’s culture.

If you want the next chapter in your story to be one where you make a real difference for our customers, communities and economies, we’re the bank to join. We foster diversity, equity and inclusion, building on the legacy of our immigrant founder. If you’re curious, enjoy collaborating, and start each day with the customer in mind, we invite you to join our team!

Job Description


At Bremer Bank, we are looking for an experienced AWS Data Engineer to help us build a unified cloud data platform and move our on-prem data infrastructure to that platform.

The ideal candidate will have experience working with AWS data services, such as S3, Glue, Lambda, and Redshift. They will be responsible for designing, building, and maintaining our data pipelines, as well as ensuring the quality and accuracy of the data. Additionally, they will work with our analytics engineering team to onboard data and with our business partners for reverse ETL data to outside parties. The successful candidate will be a self-starter with strong problem-solving skills and the ability to work independently.

The candidate will work in a highly collaborative team environment, engage in daily standups, operate within Agile development practices, and participate in a GitOps development model in that everything is code.

QUALIFICATIONS:

The ideal candidate will have a B.S. degree in a related field and 5 years professional experience OR a total of 9 plus years' experience as Data Engineer, Software Engineer, Data Scientist, or equivalent.
3 plus years working in AWS Cloud (AWS Certified Cloud Practitioner certification is a plus) and hybrid cloud environments.
3 plus years working with data warehouse platforms.
Experience in SQL and comparable scripting (Python and Bash).
Experience working with and supporting Data Virtualization and Data Visualization tools.
Experience working with large and complex data sets with multi-terabyte scale.
Knowledge of Agile and GitOps methodologies.


#LI-Hybrid

Bremer Values & Benefits



Our Values

Collaboration: We work elbow-to-elbow with our customers, making a difference for them in good times and bad. We share our skills, knowledge and experience to support each other every day.

Commitment: We’re deeply invested in the places we live and work, and we’re always ready to do whatever it takes to help our neighbors and communities thrive and grow.

Creativity: Imagination and innovation are essential to helping our customers and Bremer succeed. If there’s a better way to do something, we’ll find it or create it.


Our Benefits

At Bremer, we are committed to supporting and growing the communities we call home. Our people understand that they represent our name and reputation in every conversation, and that they’re backed by an organization that values finding a better way to get things done. Let’s talk about benefits. Not just the 401(k), pension plan, healthcare, and vacation kind (we’ve got those), but the kind of everyday benefits that get you excited for what each day holds.

Equal Employment Opportunity



Bremer Bank is committed to a diverse and inclusive workforce. We’ve built an organization of experienced, intelligent and collaborative people from a range of all backgrounds, and all of them contribute something essential to helping our customers thrive and grow.

All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by applicable law.

For your reference, here are additional details for your awareness:
Know Your Rights: Workplace Discrimination is Illegal (dol.gov)
Family and Medical Leave Act (dol.gov)
Employee Polygraph Protection Act rights (dol.gov)

Miscellaneous Items to Note

Must be legally authorized to work in the U.S. without Bremer sponsorship for employment visa status (e.g., H1B status, 0-1, TN, EB-2)
Notice to California Applicants - for your awareness, please see Bremer Bank California Notice at Collection and Privacy Policy
Accessibility Commitment - Bremer is committed to accessibility, diversity and inclusion for all our applicants. We believe everyone should be able to have equal and effective access to our websites and our services. We have worked very hard to ensure that our website and digital content meet the needs of our applicants and visitors. Click here for more information on our accessibility and how to get in touch with us.
Notice to Third Party Recruiters - Bremer Bank, National Association, including its affiliates, does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed agreement for recruiting or placement services, Bremer will have no obligation and will not agree to pay any referral or recruiting fee. In the event a recruiter or agency submits a candidate for consideration, Bremer explicitly reserves the right to pursue and/or hire any such candidate(s) without any financial or other obligation to the recruiter or agency. Any unsolicited resumes, including those submitted to hiring managers, will be deemed to be the property of Bremer.",1943,Banking & Lending,$10+ billion (USD),Financial Services,1001 to 5000 Employees,Company - Private,False
Senior Information Security Engineer - IAM Data Analytics,"Wells Fargo
","Minneapolis, MN",-1,3.7,"About this role:

Wells Fargo is seeking a Senior Information Security Engineer to assist with our data driven, machine learning automation efforts for the Identity and Access Management processes. This person will provide data engineering activities, including but not limited to, the capture of key data and the development and automation of said data for automated intelligence for business processes. Prefer a candidate that has extensive knowledge in data lakes, data warehousing, data modeling and data integration for automating of analytics integration within business processes. Should have knowledge with various Identity and Access data elements and various IAM functions.

This position will support the IAM Organization in data process and engineering within the Information and Cyber Security and IAM organizations for information needs.

Will be responsible for:
Designs, documents, tests, maintains, and provides issue resolution recommendations for moderately complex security
solutions related to networking, cryptography, cloud, authentication/directory services, email, internet, applications, and/or endpoint security. Provides security consulting on medium projects for internal clients to ensure conformity with corporate information security policy, and standards. Leads computer security incident response activities for moderately
complex events, conducts technical investigation of security-related incidents and conducts post-incident digital forensics to identify causes and recommend future mitigation strategies. Reviews and correlates security logs. Identifies security vulnerabilities/issues, performs risk assessments, and evaluates remediation alternatives. Possesses subject matter
expertise in industry leading security solutions and best practices used to implement one or more components of information security such as availability, integrity, confidentiality, risk management, threat identification/modeling/monitoring, incident response, access management, and business continuity. May interface with senior management.

In this role, you will:
Lead or participate in computer security incident response activities for moderately complex events
Conduct technical investigation of security related incidents and post incident digital forensics to identify causes and recommend future mitigation strategies
Provide security consulting on medium projects for internal clients to ensure conformity with corporate information, security policy, and standards
Design, document, test, maintain, and provide issue resolution recommendations for moderately complex security solutions related to networking, cryptography, cloud, authentication and directory services, email, internet, applications, and endpoint security
Review and correlate security logs
Utilize subject matter knowledge in industry leading security solutions and best practices to implement one or more components of information security such as availability, integrity, confidentiality, risk management, threat identification, modeling, monitoring, incident response, access management, and business continuity
Identify security vulnerabilities and issues, perform risk assessments, and evaluate remediation alternatives
Collaborate and consult with peers, colleagues and managers to resolve issues and achieve goals
Required Qualifications, US:
4+ years of Information Security Engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education.
4+ years of experience in: ETL and Server Integration Services (SSIS)
4+ years of experience with databases such as but not limited to: Oracle, DB2, SQL server, or Teradata
6+ years of data engineering experience to include: task automation, developing database architecture and or data warehousing
6+ years of experience with end-to-end design and delivery of data warehouse applications
Desired Qualifications:
6+ years of experience with ETL tools such as NDM, Autosys, SSIS, etc
Experience in C# development
2+ years of Agile experience
Ability to transform conceptual design to technical implementation.
Ability to identify challenges, anticipate obstacles, influence and resolve issues.
An industry-standard technology certification
Analytical skills with a keen ability to see how to translate needs of the teams into tangible deliverables.
High level understanding of various development data technologies and development environments.
Bachelor's degree, preferably in a technology related field, or four (4) plus years of experience.
Job Expectation:
Onsite work is required weekly. Check eligible posting locations.
No visa sponsorship/transfer
No C2C
No relocation assistance
We Value Diversity

At Wells Fargo, we believe in diversity, equity and inclusion in the workplace; accordingly, we welcome applications for employment from all qualified candidates, regardless of race, color, gender, national origin, religion, age, sexual orientation, gender identity, gender expression, genetic information, individuals with disabilities, pregnancy, marital status, status as a protected veteran or any other status protected by applicable law.

Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.

Candidates applying to job openings posted in US: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.

Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.

Drug and Alcohol Policy

Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.",1852,Banking & Lending,$10+ billion (USD),Financial Services,10000+ Employees,Company - Public,False
Senior Electrical Engineer - Instrumentation and Data Acquisition Hardware Design,"Nordson
","Minneapolis, MN",$85K - $116K (Glassdoor est.),3.3,"At Nordson Electronics Solutions we have big goals, an innovative spirit, and a vision to become the preferred partner to electronics manufacturers worldwide. If you believe in big goals, consider joining our team to help solve reliability challenges for the world's largest semiconductor, printed circuit board, and precision assembly manufacturers. Our fluid dispensing and surface treatment solutions help make reliable electronics an everyday reality – from mobile devices to the Internet of Things to self-driving vehicles, life-saving medical equipment, and beyond.

Job Summary

CyberOptics-Nordson is a global leader in high-precision sensor technology in the areas of 3D machine vision and semiconductor process measurement. Our sensors are deployed in electronics and semiconductor factories across the planet. The device you are using to read this notice most likely contains integrated circuits inspected by our sensors during their manufacture. Our products are a tight integration of optics, electronics, embedded and application software, and algorithms.

We are looking for a Senior Electrical Engineer who will play an integral role in our R&D team designing the next generation of high precision electronics and semiconductor inspection sensors. You will be involved in this process from product conception to production. This position requires design conceptualization to meet product requirements, project planning and tracking, design implementation, test and verification to specifications, transition to production, continuation, and documentation.

Essential Job Duties and Responsibilities

Design of electronic circuits for products that sense physical phenomenon such as capacitance, temperature, humidity, light, vibration, inclination, gases, particles, resistivity, and more.

Experience in the amplification, signal conditioning, filtering, and digitization of low-level signals for measurement and logging.

Design wireless low powered battery-operated support circuitry with embedded processors, FPGAs, ADCs and DACs, Bluetooth or WIFI communication, and power management.

Ability to solve challenging problems with creative designs and processes.

Supervision of PCB layout that packages electronics into challenging spaces and environments (temperature and vacuum) that often require high performance materials.

Interpret internal and customer requirements and translate those into product specifications and design concepts.

Risk reduction of design concepts through simulation, prototype testing, and feasibility studies.

Project planning requiring the estimation of tasks, resources, schedules, NRE, production costs, and manufacturing support.

Manage suppliers that fabricate prototype and production electrical designs.

Design verification testing of designs to product specifications.

Ability to design products to meet CE and other international compliance requirements.

Education and Experience Requirements

MS Electrical Engineering degree preferred, or BS considered with additional experience.

5+ Years work experience desired.

Experienced in the design of data acquisition and analog circuitry that measures physical properties for the purpose of datalogging or real time reporting via a Bluetooth or WIFI radio.

Low power embedded processor or FPGA hardware design experience.

Digital filtering and signal processing in DSP or FPGA hardware.

Verilog or VHDL experience.

Experience with high-speed digital designs and communication protocols.

Analog and Digital Simulation skills with tools like Spice or MATLAB.

Lithium-Ion batteries, Qi wireless charging, power management, and fuel gauge estimation.

High efficiency low-noise power supply design.

Photodiode, Laser Diode, CMOS imager, and LED Illumination design experience considered a plus.

Skills and Abilities

Ability to execute new electronic designs from concept to production.

Self-management, planning, communicating, and working effectively in a multi-functional team.

Proficient with high-end schematic capture tools, design constraints, documentation source control, component database management, and approved vendor list (AVL) management.

Verify high speed signal integrity compliance by either test methods or third party PCB simulations.

Incorporation of manufacturability and testability into designs.

Skilled at troubleshooting with high end scopes, DVMs, Logic and Protocol Analyzers, and other tools.

Working with the manufacturing department to help development calibration and alignment processes and fixtures.

Keep up to date on the latest electronics technology and its applicability to the problems we are trying to solve.

Manage outside contractors, suppliers and supply chain issues affecting the production of products.

Address continuation issues on released products.

Familiarity with ISO development processes.

Mentoring of Junior Engineers and Technicians.

Working Conditions and Physical Demands

To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodation may be made to enable individuals with disabilities to perform the essential functions.

Travel Required

None

#LI-CL1

Nordson Corporation provides equal employment opportunity to all applicants and employees. No person is to be discriminated against in any aspect of the employment relationship due to race, religion, color, sex, age, national origin, ancestry, disability, sexual orientation, gender identity, genetic information, citizenship status, marital status, pregnancy, veteran status or any other status protected by applicable federal, state, or local law. All employment offers are contingent upon successful completion of our pre-employment drug screening and background/criminal check, consistent with applicable laws.Third party recruiters and agencies should not contact employees of Nordson or its subsidiaries directly. Any resumes sent to a hiring manager or submitted to Nordson employees are considered unsolicited and property of Nordson. Nordson will not pay a placement fee unless the agency or recruiter has a signed contract with Nordson's Human Resources department in advance of submitting a candidate for consideration. Verbal and written approvals will not be considered a valid contract for service.",1956,Machinery Manufacturing,$1 to $5 billion (USD),Manufacturing,5001 to 10000 Employees,Company - Public,False
Senior Software Engineer – API & Data Integration,"Object Partners
","Minneapolis, MN",$112K - $143K (Glassdoor est.),4.4,"Variety of consulting; new technologies, projects, and people on a regular basis.
Stability; we’ve been around since 1996 and have a diverse mix of clients and technologies to keep us busy, very busy. And we keep a bench. If you’re not on a project, you’re writing software for our internal business functions or you’re learning new technologies. It’s beneficial to make our consultants as marketable as possible. That’s good for your career.
No politics or management; we don’t get in the way. Why sit in meetings all day when you can code and be productive?
Awesome benefits; robust healthcare plan, 28 days of PTO, semi-annual profit sharing bonuses, you get paid OT, company trips, various quarterly company events, new MacBook Pro’s, free beer/soda, chips, candy, and so much more.
You work with the best. Do an Object Partners search on LinkedIn and see the types of talent we hire. You truly get to work with intelligent, passionate engineers that share the same goal of building great software the right way.
Low company overhead. It all means more money back into our consultants pockets (profit sharing) or company trips and events to share in the financial success.
Qualifications
You are passionate about the software development process and solving big problems. You dive in regardless of the tools, tech, team, and business at hand.

Experience designing and implementing highly available/scalable backend services
Enjoy developing clean, testable code
Proficient in deploying, monitoring, and maintaining said code
Actively share knowledge across the team
Can discuss one or more projects that utilize technologies from the following categories:
Language: Java, Kotlin, Groovy, Golang, Python, C#, etc.
Automated Testing: Spock, JUnit, go testing package, Geb, etc.
Frameworks: Spring Boot, Micronaut, Quarkus, .NET, etc.
Reactive Libraries: RxJava, Ratpack, Reactor, Akka, Vert.x, etc.
Data:
Relational: MySQL, Postgres, Oracle, etc.
NoSQL: Cassandra, DynamoDB, MongoDB, Elastic, etc.
Platform:
Environment: AWS, Azure, GCP, Containerized On-Prem
CI/CD: Jenkins, Gitlab, CircleCI, AWS CodePipeline, etc.
Observability: Log Aggregation, Metrics, Tracing, etc.
Alerting on the observability data
Dashboarding the observability data

A consultant does not know everything, but they should have the motivation and means to learn anything. The best consultant isn’t the most technical (although that sure helps), but someone who will do whatever it takes to see a client succeed, no matter what gets thrown at them.",1996,Business Consulting,$5 to $25 million (USD),Management & Consulting,51 to 200 Employees,Company - Private,False
"Senior Data Engineer, Public Company","Recruiting From Scratch
","Minneapolis, MN",$100K - $200K (Employer est.),4.0,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:

Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.

With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.

What’s so interesting about this role?

Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .

In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.

What’s the job?

Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.

What we’ll love about you

5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.

Salary Range: $165,000-$210,000 USD base. Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/",2019,Staffing & Subcontracting,$1 to $5 million (USD),Human Resources & Staffing,1 to 50 Employees,Company - Private,True
Senior Snowflake Data Engineer,"Gullview Technologies
","Minneapolis, MN",$83K - $119K (Glassdoor est.),4.0,"Sr. Snowflake Data Engineer

Gullview Technologies is an exciting and rapidly growing technology company focused on taking on the most vital and challenging business and technical challenges our clients face in our highly connected business world today. We have an exceptional focus at Gullview on developing deep and long-lasting relationships (several years and counting) with our clients. This focus enables us to deliver an ongoing continuum of projects and solutions to them with high value, meaningful impact, and predictable performance.

Position Overview

We have a great opportunity to help refine a Snowflake Data Solution for a strategic client of ours as part of their Enterprise Data journey. This is a fantastic opportunity to join our firm, and work with our client in the foundational stages of a Data Practice for Gullview and the Enterprise Data Practice for the client with significant opportunities to influence Enterprise Data and Analytics capabilities.

Initially, this role will be an individual contributor with opportunities of leadership in supporting mission critical systems for our clients.

Focus of Role

The Sr. Snowflake Data Engineer will lead a discovery and resolution for our client’s current Snowflake implementation. The talented individual in this role will collaborate with Gullview and Client Leadership, Architects, and Engineers to build out a Snowflake Data Solution focusing on delivering business value. This role will initially be an individual contributor with future opportunities for leadership if desired. You will help with the overall Data Strategy including infrastructure, software, utilities/tools, Public Cloud Solutions, and Business Intelligence Solutions for the organization. This is a Sr. level engineering position, not an architecture role.

Responsibilities

Provide understanding of Snowflake, Data Warehouse, Data Movement, Data Curation, and Data Staging best practices in relation to existing Snowflake implementations
Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Create and contribute to automation of essential processes related to infrastructure solution deployment creating repeatable and robust deployments
Provide Snowflake solutions, including hands on keyboard (implementation)
Overall ownership of the “platform” and supporting essential platform capabilities
Responsible for day-to-day sustainment and configuration of the platform including
System Health and Telemetry
Performance and performance tuning
Data Quality, Protection, and Availability
Responsible for developing and implementing solutions related to Snowflake and preparing data for Business Intelligence
Fully document all solution work including designs and configurations
Play an essential role in troubleshooting data system problems and provide viable solution options

Requirements

Qualifications

Minimum 10 years of experience in IT with focus on Enterprise Data Solutions
Minimum 5 of years of experience in:
Snowflake Data Solutions, hands-on
Data warehousing methodologies and modelling techniques Data migration methods of on-prem to cloud data solutions including ELT/ETL Tools and concepts
Working with Batch and Stream data
SQL, preferrable Snowflake SQL
Massively Parallel Processing (MPP) Analytical Datastores
Experience in Snowflake utilities including SnowSLQ, Snowpipe, Snowlight for handling Streaming data is a plus
Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, Zero copy clone, time travel and understand how to use these features
Experience in in re-clustering Snowflake data with good understanding of Micro-Partition within Snowflake
Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns
Experience in handling semi-structured data (JSON, XML) in Snowflake
Minimum of 2 years of hands-on experience in Cloud technologies such as
o Azure - Blob Storage, Cool Blob Storage, Virtual Machine, Functions, SQL Datawarehouse
Certified Snowflake cloud data warehouse Architect (Desirable).
Should be able to troubleshoot problems across infrastructure, platform and application domains.
Must have experience of Agile development methodologies
Strong written communication skills. Is effective and persuasive in both written and oral communication
Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)

Education Requirements

Bachelor’s degree in computer science, engineering, or Management Information Systems, or related IT field.

Position Type

Full time employee",-1,-1,Unknown / Non-Applicable,-1,1 to 50 Employees,Company - Private,True
Senior Staff AI Data Engineer,"Recruiting From Scratch
","Minneapolis, MN",$160K (Employer est.),4.0,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/

This is a hybrid role based in our Palo Alto or San Francisco offices and will require you to be in office Tuesdays and Thursdays.

What’s so interesting about this role?

We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.

What’s the job?

We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.

In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.

Responsibilities:

Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure

What we’ll love about you

Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)

We’ll really swoon if you have

2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects

What you'll love about us

Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/",2019,Staffing & Subcontracting,$1 to $5 million (USD),Human Resources & Staffing,1 to 50 Employees,Company - Private,True
