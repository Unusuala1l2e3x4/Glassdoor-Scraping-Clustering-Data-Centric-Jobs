{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time, re, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_script = \"\"\"var overviewItems = Array.from(document.querySelectorAll('.JobDetails_overviewItem__35s2T'));\n",
    "var overviewData = {\n",
    "    'Size': -1,\n",
    "    'Founded': -1,\n",
    "    'Type': -1,\n",
    "    'Industry': -1,\n",
    "    'Sector': -1,\n",
    "    'Revenue': -1\n",
    "};\n",
    "overviewItems.forEach(function(item) {\n",
    "    var label = item.querySelector('.JobDetails_overviewItemLabel__5vi0o');\n",
    "    if (label) {\n",
    "        var labelText = label.textContent.trim();\n",
    "        if (overviewData.hasOwnProperty(labelText)) {\n",
    "            var value = item.querySelector('.JobDetails_overviewItemValue__5TqNi');\n",
    "            if (value) {\n",
    "                value = value.textContent.trim()\n",
    "                if (value != '--') {\n",
    "                    overviewData[labelText] = value\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "});\n",
    "return overviewData;\n",
    "\"\"\"\n",
    "easy_apply_script = \"\"\"var applyContainer = document.querySelector('div.JobDetails_applyButtonContainer__Fzd66');\n",
    "return applyContainer ? applyContainer.textContent.includes('Easy Apply') : false;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states = [\n",
    "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n",
    "    \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n",
    "    \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n",
    "    \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n",
    "    \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n",
    "    \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \n",
    "    \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n",
    "    \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n",
    "    \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington, DC\", \"Washington State\",\n",
    "    \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs(keyword, location=None, num_jobs=None, verbose=False):\n",
    "    \n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "    \n",
    "    #Initializing the webdriver\n",
    "    options = Options()\n",
    "    # options = webdriver.ChromeOptions()\n",
    "    # options = webdriver.EdgeOptions()\n",
    "    options.page_load_strategy = 'normal'\n",
    "    # options.add_experimental_option(\"detach\", True)\n",
    "    \n",
    "    #Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n",
    "    # options.add_argument('headless')\n",
    "    \n",
    "    #Change the path to where chromedriver is in your home folder.\n",
    "    # driver = webdriver.Chrome(executable_path=\"chromedriver\", options=options)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    # driver = webdriver.Edge(options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    url = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=' + keyword \n",
    "    # + '\"&locT=C&locId=1147401&jobType=all&fromAge=-1&minSalary=0&includeNoSalaryJobs=true&radius=100&cityId=-1&minRating=0.0&industryId=-1&sgocId=-1&seniorityType=all&companyId=-1&employerSizes=0&applicationType=0&remoteWorkType=0'\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.XPATH,\"//iframe[@title='Widget containing a Cloudflare security challenge']\")))\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//label[@class='ctp-checkbox-label']\"))).click()\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        pass\n",
    "        \n",
    "    jobs = []\n",
    "    job_buttons = []\n",
    "    startingIndex = 0\n",
    "    \n",
    "    # driver.minimize_window()\n",
    "    driver.switch_to.default_content()\n",
    "    # time.sleep(2)\n",
    "    \n",
    "    # driver.refresh()\n",
    "    \n",
    "    print(location)\n",
    "    if location:\n",
    "        try:\n",
    "            # Wait for the location input field to be available\n",
    "            location_input = WebDriverWait(driver, 60).until(\n",
    "                # EC.presence_of_element_located((By.CSS_SELECTOR, \"#searchBar-location\"))\n",
    "                EC.presence_of_element_located((By.XPATH, \"//input[@id='searchBar-location']\"))\n",
    "            )\n",
    "            location_input.clear()  # Clearing the field before entering new text\n",
    "            location_input.send_keys(location)\n",
    "            location_input.send_keys(Keys.ENTER)  # Pressing the Enter key\n",
    "            time.sleep(2)\n",
    "            # driver.refresh()\n",
    "            # time.sleep(2)\n",
    "        except TimeoutException:\n",
    "            print(\"Location input field not found within the specified time\")\n",
    "            pass\n",
    "        \n",
    "    driver.switch_to.default_content()\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'h1.SearchResultsHeader_jobCount__12dWB')))\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        pass\n",
    "    \n",
    "    num_jobs_found = int(re.sub(r'[^\\d.]', '', driver.execute_script(\"return document.querySelector('h1.SearchResultsHeader_jobCount__12dWB').innerText;\")))\n",
    "    print(num_jobs_found,'total jobs found')\n",
    "    \n",
    "    if num_jobs_found > 900:\n",
    "        print('WARNING: More than 900 jobs found. At most 900 are retrievable at once.')\n",
    "    \n",
    "    # if not num_jobs or num_jobs < 0:\n",
    "    #     print('num_jobs not specified. Getting all jobs...')\n",
    "    #     # num_jobs = num_jobs_found + 100 # upper bounding just in case\n",
    "    #     num_jobs = min(900, num_jobs_found) # 6000\n",
    "    # else:\n",
    "    #     print('Getting the first',str(num_jobs),'jobs...')\n",
    "\n",
    "    # pbar = tqdm(total=num_jobs)\n",
    "    # while len(jobs) < num_jobs:\n",
    "    pbar = tqdm()\n",
    "    while True:\n",
    "        \n",
    "        # Take the current snapshot of job buttons\n",
    "        current_count = len(job_buttons)\n",
    "        \n",
    "        # Wait until new job list items are loaded or until a timeout\n",
    "        try:\n",
    "            WebDriverWait(driver, 40, 1).until(\n",
    "                lambda d: d.execute_script(\"return document.querySelectorAll('li.JobsList_jobListItem__JBBUV').length;\") > current_count\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"No more jobs were loaded. Ending scraping with {} jobs.\".format(len(jobs)))\n",
    "            break\n",
    "\n",
    "        # Retrieve the updated list of job buttons\n",
    "        job_buttons = driver.execute_script(\"return document.querySelectorAll('li.JobsList_jobListItem__JBBUV');\")\n",
    "\n",
    "        # Check if new jobs have been loaded\n",
    "        if len(job_buttons) > current_count:\n",
    "            if verbose:\n",
    "                print(f\"{len(job_buttons) - current_count} new jobs found, processing...\")\n",
    "        else:\n",
    "            print(\"No more jobs were loaded. Ending scraping with {} jobs.\".format(len(jobs)))\n",
    "            break\n",
    "        \n",
    "        \n",
    "        ##################\n",
    "\n",
    "        for job_button in job_buttons[startingIndex:]:\n",
    "            # if len(jobs) >= num_jobs:\n",
    "            #     break\n",
    "\n",
    "            driver.execute_script(\"arguments[0].click();\", job_button)\n",
    "            # time.sleep(0.4)\n",
    "\n",
    "            try:\n",
    "                company_name = job_button.find_element(By.CSS_SELECTOR, 'div.EmployerProfile_employerInfo__GaPbq').text\n",
    "            except NoSuchElementException:\n",
    "                company_name = ''\n",
    "                continue\n",
    "            try:\n",
    "                rating = job_button.find_element(By.CSS_SELECTOR, 'span.EmployerProfile_employerRating__3ADTJ').text\n",
    "                if rating != '':\n",
    "                    company_name = company_name[:-len(rating)]\n",
    "                    rating = float(re.sub(r'[^\\d.]', '', rating))\n",
    "                else:\n",
    "                    rating = -1\n",
    "            except NoSuchElementException:\n",
    "                rating = -1\n",
    "            try:\n",
    "                salary_estimate = job_button.find_element(By.CSS_SELECTOR, 'div.JobCard_salaryEstimate___m9kY').text\n",
    "            except NoSuchElementException:\n",
    "                salary_estimate = -1\n",
    "                \n",
    "            try:\n",
    "                # WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'div.JobDetails_location__MbnUM')))\n",
    "                # WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'div.JobDetails_jobTitle__Rw_gn')))\n",
    "                WebDriverWait(driver, 60).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'div.JobDetails_jobDescription__6VeBn')))\n",
    "                location = driver.execute_script(\"return document.querySelector('div.JobDetails_location__MbnUM').innerText;\")\n",
    "                job_title = driver.execute_script(\"return document.querySelector('div.JobDetails_jobTitle__Rw_gn').innerText;\")\n",
    "                job_description = driver.execute_script(\"return document.querySelector('div.JobDetails_jobDescription__6VeBn').innerText;\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to collect data for a job due to: {e}\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "            company_data = driver.execute_script(company_script)\n",
    "            # print(company_data)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Job Title: {}\".format(job_title))\n",
    "                # print(\"Salary Estimate: {}\".format(salary_estimate))\n",
    "                # print(\"Job Description: {}\".format(job_description[:500]))\n",
    "                # print(\"Rating: {}\".format(rating))\n",
    "                print(\"Company Name: {}\".format(company_name))\n",
    "                # print(\"Location: {}\".format(location))\n",
    "                \n",
    "            items = {\n",
    "                \"Job Title\": job_title,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location,\n",
    "                \"Salary Estimate\": salary_estimate,\n",
    "                \"Rating\": rating,\n",
    "                \"Job Description\": job_description,\n",
    "            }\n",
    "            items.update(company_data)\n",
    "            items.update({\"Easy Apply\" : driver.execute_script(easy_apply_script)})\n",
    "            jobs.append(items)\n",
    "            pbar.update(1)\n",
    "                \n",
    "        ##################\n",
    "\n",
    "        # Attempt to click the 'load more' button to load additional jobs\n",
    "        # load_more_button_xpath = \"//button[.//span[contains(text(),'Show more jobs')]]\"\n",
    "        # try:\n",
    "        #     WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, load_more_button_xpath)))\n",
    "        #     driver.execute_script(\"arguments[0].click();\", driver.find_element(By.XPATH, load_more_button_xpath))\n",
    "        # except (NoSuchElementException, TimeoutException):\n",
    "        #     print(\"No 'Show more jobs' button found. Ending scraping with {} jobs.\".format(len(jobs)))\n",
    "        #     break\n",
    "        \n",
    "        load_more_button_xpath = \"#left-column > div.JobsList_wrapper__wgimi > div > button\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, load_more_button_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", driver.find_element(By.CSS_SELECTOR, load_more_button_xpath))\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(\"No 'Show more jobs' button found. Ending scraping with {} jobs.\".format(len(jobs)))\n",
    "            break\n",
    "        \n",
    "        # Update startingIndex to the length of the current job_buttons for the next iteration\n",
    "        startingIndex = len(job_buttons)-1\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # print('ending len(job_buttons):',len(job_buttons))\n",
    "    # print(len(jobs), num_jobs, num_jobs_found)\n",
    "    # print(len(jobs), num_jobs_found)\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(jobs)  #This line converts the dictionary object into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This line will open a new chrome window and start the scraping.\n",
    "# keyword = \"data analyst\"\n",
    "# # df = get_jobs(keyword, 60, False)\n",
    "# df = get_jobs(keyword, 'Illinois, US', None, False)\n",
    "# pd.DataFrame(df).to_csv(os.path.join('scraper results test',keyword+' '+str(len(df))+'.csv'), index=False)\n",
    "# # df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results df from all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_states(keyword):\n",
    "    path = os.path.join('scraper results', f'{keyword} state results')\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    for state in us_states:\n",
    "        df = get_jobs(keyword, f'{state}, US', None, False)\n",
    "        pd.DataFrame(df).to_csv(os.path.join(path, f'{keyword} in {state} {str(len(df))}.csv'), index=False)\n",
    "        # df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_jobs_states(\"data scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_jobs_states(\"data analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_jobs_states(\"business analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_jobs_states(\"data engineer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through the list of all states with <= 900 postings, saving a dataframe for each state\n",
    "\n",
    "for states with more > 900 postings, create a separate loop for each, where you loop through all of its cities (glassdoor doesnt do counties)\n",
    "\n",
    "\n",
    "finally, join all dataframes together, remove duplicates then save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_count(keyword, location=None, num_jobs=None, verbose=False):\n",
    "    \n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "    #Initializing the webdriver\n",
    "    options = Options()\n",
    "    options.page_load_strategy = 'normal'\n",
    "    # options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    url = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=' + keyword \n",
    "    driver.get(url)\n",
    "    \n",
    "    # time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.XPATH,\"//iframe[@title='Widget containing a Cloudflare security challenge']\")))\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//label[@class='ctp-checkbox-label']\"))).click()\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        pass\n",
    "    \n",
    "    driver.minimize_window()\n",
    "    driver.switch_to.default_content()\n",
    "    # time.sleep(2)\n",
    "    \n",
    "    if location:\n",
    "        try:\n",
    "            # Wait for the location input field to be available\n",
    "            location_input = WebDriverWait(driver, 60).until(\n",
    "                # EC.presence_of_element_located((By.CSS_SELECTOR, \"#searchBar-location\"))\n",
    "                EC.presence_of_element_located((By.XPATH, \"//input[@id='searchBar-location']\"))\n",
    "            )\n",
    "            location_input.clear()  # Clearing the field before entering new text\n",
    "            location_input.send_keys(location)\n",
    "            location_input.send_keys(Keys.ENTER)  # Pressing the Enter key\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"Location input field not found within the specified time\")\n",
    "            pass\n",
    "    driver.switch_to.default_content()\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'h1.SearchResultsHeader_jobCount__12dWB')))\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        pass\n",
    "    \n",
    "    num_jobs_found = int(re.sub(r'[^\\d.]', '', driver.execute_script(\"return document.querySelector('h1.SearchResultsHeader_jobCount__12dWB').innerText;\")))\n",
    "    return num_jobs_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_as_df(d, keyword):\n",
    "    df = pd.DataFrame().from_dict(d.items())\n",
    "    df.columns=['State','Postings']\n",
    "    df.to_csv( os.path.join('state counts', f'{keyword} state counts.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = \"data scientist\"\n",
    "# counts_per_state = dict()\n",
    "\n",
    "# for state in us_states:\n",
    "#     c = get_job_count(keyword, f'{state}, US', None, False)\n",
    "#     counts_per_state[state] = c\n",
    "#     print(state, c)\n",
    "# save_dict_as_df(counts_per_state, keyword)\n",
    "# counts_per_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = \"data analyst\"\n",
    "# counts_per_state = dict()\n",
    "\n",
    "# for state in us_states:\n",
    "#     c = get_job_count(keyword, f'{state}, US', None, False)\n",
    "#     counts_per_state[state] = c\n",
    "#     print(state, c)\n",
    "# save_dict_as_df(counts_per_state, keyword)\n",
    "# counts_per_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = \"business analyst\"\n",
    "# counts_per_state = dict()\n",
    "\n",
    "# for state in us_states:\n",
    "#     c = get_job_count(keyword, f'{state}, US', None, False)\n",
    "#     counts_per_state[state] = c\n",
    "#     print(state, c)\n",
    "# save_dict_as_df(counts_per_state, keyword)\n",
    "# counts_per_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = \"data engineer\"\n",
    "# counts_per_state = dict()\n",
    "\n",
    "# for state in us_states:\n",
    "#     c = get_job_count(keyword, f'{state}, US', None, False)\n",
    "#     counts_per_state[state] = c\n",
    "#     print(state, c)\n",
    "# save_dict_as_df(counts_per_state, keyword)\n",
    "# counts_per_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data scientist in California 900.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data analyst in California 900.csv\n",
      "data analyst in Texas 900.csv\n",
      "data analyst in Washington, DC 900.csv\n",
      "business analyst in California 900.csv\n",
      "business analyst in Texas 898.csv\n"
     ]
    }
   ],
   "source": [
    "for keyword in ['data scientist', 'data analyst', 'business analyst', 'data engineer']:\n",
    "    fnames = set(os.listdir(os.path.join('scraper results', f'{keyword} state results')))\n",
    "    for state in us_states:\n",
    "        fname = [a for a in fnames if state in a][0]\n",
    "        counts = pd.read_csv(os.path.join('scraper results', f'{keyword} state results',fname))\n",
    "        if len(counts) > 800:\n",
    "            print(fname)\n",
    "        # df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract postigs from city level for the states that have more than the 900 postings limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'data scientist'\n",
    "# [\"CA\", \"WA\"]\n",
    "# 'data analyst'\n",
    "# [\"CA\", \"TX\", \"WA\"]\n",
    "# 'business analyst'\n",
    "# [\"CA\", \"TX\", \"WA\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_state_cities(keyword, state, cities):\n",
    "    if not os.path.isdir(os.path.join(f'{keyword} state results', f'{keyword} in {state} results')):\n",
    "        os.mkdir(os.path.join(f'{keyword} state results', f'{keyword} in {state} results'))\n",
    "    for city in cities:\n",
    "        if len([a for a in os.listdir(os.path.join(f'{keyword} state results', f'{keyword} in {state} results')) if city in a]) == 1:\n",
    "            continue\n",
    "        df = get_jobs(keyword, f'{city}, {state}', None, False)\n",
    "        pd.DataFrame(df).to_csv(os.path.join(f'{keyword} state results', f'{keyword} in {state} results',f'{keyword} in {city}, {state} {str(len(df))}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword, state = 'data scientist', \"CA\"\n",
    "# cities = ['Cupertino','Los Angeles','Menlo Park','Mountain View','Palo Alto','San Diego','San Francisco','San Jose','Santa Clara','Sunnyvale']\n",
    "# get_jobs_state_cities(keyword, state, cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword, state = 'data analyst', \"CA\"\n",
    "# cities = ['El Segundo','Irvine','Los Angeles','Mountain View','Oakland','Sacramento','San Diego','San Francisco','San Jose','Santa Clara']\n",
    "\n",
    "# get_jobs_state_cities(keyword, state, cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword, state = 'data analyst', \"TX\"\n",
    "# cities = ['Austin','Dallas','El Paso','Fort Worth','Frisco','Houston','Irving','Plano','Richardson','San Antonio']\n",
    "# get_jobs_state_cities(keyword, state, cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "business analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword, state = 'business analyst', \"CA\"\n",
    "# cities = ['Burbank','Folsom','Irvine','Los Angeles','Oakland','Sacramento','San Diego','San Francisco','San Jose','Santa Clara']\n",
    "# get_jobs_state_cities(keyword, state, cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword, state = 'business analyst', \"TX\"\n",
    "# cities = ['Austin','Dallas','El Paso','Fort Worth','Frisco','Houston','Irving','Plano','Richardson','San Antonio']\n",
    "# get_jobs_state_cities(keyword, state, cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df:pd.DataFrame, exclude=None):\n",
    "    \n",
    "    subset_columns = ['Job Title', 'Company Name', 'Location', 'Salary Estimate', 'Rating', 'Job Description']\n",
    "    \n",
    "    if exclude:\n",
    "        subset_columns.remove(exclude)\n",
    "    \n",
    "    df.sort_values(subset_columns, inplace=True)\n",
    "\n",
    "    df['Duplicate Count'] = df.groupby(subset_columns)['Job Title'].transform('count')\n",
    "    df.drop_duplicates(subset=subset_columns, inplace=True, keep='last')  \n",
    "    # combined_data.drop_duplicates(inplace=True, keep='last')  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(dirname, exclude=None):\n",
    "    dataframes = []\n",
    "    for root, _, files in os.walk(dirname):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                data = pd.read_csv(file_path)\n",
    "                dataframes.append(data)\n",
    "    combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    remove_duplicates(combined_data, exclude)\n",
    "\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more rows remaining = less duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = [None, 'Job Title', 'Company Name', 'Location', 'Salary Estimate', 'Rating','Job Description']\n",
    "# total = 0\n",
    "\n",
    "# for i in ['business analyst', 'data analyst', 'data scientist', 'data engineer']:\n",
    "#     print('\\n',i)\n",
    "#     for exclude_col in test:\n",
    "#         d = combine_results(os.path.join('scraper results', f'{i} state results'), exclude_col)\n",
    "#         # print(f'{exclude_col}\\t       \\t{d.shape[0]}')\n",
    "#         if exclude_col is None:\n",
    "#             total = d.shape[0]\n",
    "#             print(f'{exclude_col}\\t       \\t{total}')\n",
    "#         else:\n",
    "#             print(f'{exclude_col}\\t       \\t{d.shape[0]}\\t-{total-d.shape[0]}\\t-{100*(total-d.shape[0])/total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Location differences cause the most repeat postings (this is expected)\n",
    "- Different job descriptions causing the next most repeat postings.\n",
    "  - Question - should I remove different job descriptions for the same job when doing skill analysis?\n",
    "    - effective is small, <= 1% --> no, keep them\n",
    "  - Question - should I remove different locations for the same job when doing skill analysis?\n",
    "    - effective is ~ 4-9% --> yes, remove them\n",
    "    - combine_results(exclude='Location')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business analyst\n",
      "(10830, 14)\n",
      "(10411, 14)\n",
      "data analyst\n",
      "(9923, 14)\n",
      "(9560, 14)\n",
      "data scientist\n",
      "(5539, 14)\n",
      "(5023, 14)\n",
      "data engineer\n",
      "(2465, 14)\n",
      "(2304, 14)\n"
     ]
    }
   ],
   "source": [
    "for i in ['business analyst', 'data analyst', 'data scientist', 'data engineer']:\n",
    "    print(i)\n",
    "    d = combine_results(os.path.join('scraper results', f'{i} state results'))\n",
    "    print(d.shape)\n",
    "    d.to_csv(os.path.join('combined scraper results', f'{i.replace(\"state \",\"\")}.csv') ,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
