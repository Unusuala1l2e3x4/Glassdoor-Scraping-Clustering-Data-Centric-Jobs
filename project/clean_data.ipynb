{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import spatial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'during', \"weren't\", 'hasn', 'no', \"haven't\", \"wouldn't\", 'were', 'is', 'than', 'in', 'same', 'itself', \"couldn't\", 'she', 'y', \"didn't\", 'just', 'being', 'some', 'my', 'had', 'those', 'yours', 'against', 'himself', 'them', 'out', \"aren't\", 'whom', 'your', \"shan't\", \"you'd\", 'that', 'be', \"wasn't\", 'their', \"hasn't\", 'after', 'will', 'or', 'his', 'won', 'are', 'below', 's', 'more', 'where', 'have', 'then', 'me', \"won't\", 'only', 'not', 't', 'we', 'if', 'by', 'about', 'here', 'any', 'too', 'through', 'most', 'at', 'why', 'needn', 'and', 'wouldn', 'haven', \"doesn't\", \"you'll\", 'few', 'themselves', 'theirs', 'there', 'm', 'ourselves', 'her', 'didn', 'herself', 'into', \"it's\", 'doing', 'once', \"don't\", 'does', 'doesn', 'a', \"needn't\", 'who', 'as', 'how', 'of', 'll', 'to', 'isn', 'this', 'now', \"should've\", \"that'll\", 'shouldn', 'did', 'between', 'so', 'am', 'couldn', \"you're\", 'can', 'wasn', 'with', 'on', 're', 'which', 'it', \"she's\", 'such', 'him', 'ours', 'been', 'hadn', 'our', 'over', 'hers', 'other', 'they', 'from', 'both', 'the', 'what', \"hadn't\", 'an', 'yourself', 'shan', 'until', 'has', 'off', 'when', 'up', 'further', 'don', 'was', 'having', 'nor', 'weren', 'all', 'above', \"you've\", 'do', \"mightn't\", 'because', 've', 'each', 'yourselves', 'for', 'very', 'aren', 'mustn', 'while', 'its', 'again', 'but', 'under', 'i', 'own', 'ma', \"shouldn't\", 'myself', 'down', 'before', 'd', 'he', 'o', 'mightn', 'ain', \"mustn't\", 'these', 'you', \"isn't\", 'should'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n",
    "\n",
    "def tokenize_lemmatize_remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # tokens = [word for word in tokens if word not in stopwords and not bool(re.search(r'\\d', word))]\n",
    "    tokens = [word for word in tokens if not bool(re.search(r'\\d', word))]\n",
    "    return tokens\n",
    "\n",
    "def clean_job_description(df: pd.DataFrame):\n",
    "    df['job_description_cleaned'] = df['Job Description'].str.lower() \\\n",
    "        .str.replace(r\"\\n+\", \" \", regex=True) \\\n",
    "        .str.replace(r'[^\\w\\s]', ' ', regex=True) \\\n",
    "        .apply(tokenize_lemmatize_remove_stopwords).str.join(' ') \\\n",
    "        .str.replace(r'\\bsr\\b', 'senior', regex=True) \\\n",
    "        .str.replace(r'\\bjr\\b', 'junior', regex=True)\n",
    "\n",
    "def clean_job_title(df: pd.DataFrame):\n",
    "    df['Job Title clean'] = df['Job Title'].str.lower() \\\n",
    "        .str.replace(r\"\\(.*?\\)\", \"\", regex=True) \\\n",
    "        .str.replace(r\"\\n+\", \" \", regex=True) \\\n",
    "        .str.replace('#NAME?', '-') \\\n",
    "        .str.replace('with', '-') \\\n",
    "        .str.replace(r'[^\\w\\s]', ' ', regex=True) \\\n",
    "        .apply(tokenize_lemmatize_remove_stopwords).str.join(' ') \\\n",
    "        .str.replace(r'\\bsr\\b', 'senior', regex=True) \\\n",
    "        .str.replace(r'\\bjr\\b', 'junior', regex=True) \\\n",
    "\n",
    "    \n",
    "# def get_word_counts_df(df):\n",
    "#     temp = df[\"job_description_cleaned\"].str.split(expand=True).stack().value_counts()\n",
    "#     ret = pd.DataFrame(temp, columns=[\"count\"])\n",
    "#     return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data_jobs_data/data/DataScientist.csv').drop(['Unnamed: 0', 'index'],axis=1)\n",
    "# # df = pd.read_csv('scraper results/data scientist 900.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df:pd.DataFrame, exclude=None):\n",
    "    subset_columns = ['Job Title', 'Company Name', 'Location', 'Salary Estimate', 'Rating', 'Job Description']\n",
    "    if exclude:\n",
    "        if isinstance(exclude, list):\n",
    "            subset_columns = [a for a in subset_columns if a not in exclude]\n",
    "        else:\n",
    "            subset_columns.remove(exclude)\n",
    "    \n",
    "    df.sort_values(subset_columns, inplace=True)\n",
    "\n",
    "    df['Duplicate Count'] = df.groupby(subset_columns)['Job Title'].transform('count')\n",
    "    print(df['Duplicate Count'].value_counts())\n",
    "    df.drop_duplicates(subset=subset_columns, inplace=True, keep='last')\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Job Title',\n",
       " 'Company Name',\n",
       " 'Location',\n",
       " 'Salary Estimate',\n",
       " 'Rating',\n",
       " 'Job Description',\n",
       " 'Founded',\n",
       " 'Industry',\n",
       " 'Revenue',\n",
       " 'Sector',\n",
       " 'Size',\n",
       " 'Type',\n",
       " 'Easy Apply']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['Job Title', 'Company Name', 'Location', 'Salary Estimate', 'Rating', 'Job Description', 'Founded', 'Industry', 'Revenue', 'Sector', 'Size', 'Type', 'Easy Apply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_abbreviations = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n",
    "    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n",
    "    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n",
    "    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n",
    "state_abbreviations_vals = set(state_abbreviations.values())\n",
    "\n",
    "# Function to convert full state names to abbreviations\n",
    "def convert_state_name(state):\n",
    "    return state_abbreviations.get(state, state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Job Title,Salary Estimate,Job Description,Rating,Company Name,Location,Headquarters,Size,Founded,Type of ownership,Industry,Sector,Revenue,Competitors,Easy Apply\n",
    "def convert_sqft_to_num(x,agg):\n",
    "    if x == '-1' or not x or x == np.NaN:\n",
    "        return None\n",
    "    tokens = x.split('-')\n",
    "\n",
    "    try:\n",
    "        if len(tokens) == 2:\n",
    "            if agg == 'mean':\n",
    "                return (float(tokens[0])+float(tokens[1]))/2\n",
    "            elif agg == 'min':\n",
    "                return min(float(tokens[0]),float(tokens[1]))\n",
    "            elif agg == 'max':\n",
    "                return max(float(tokens[0]),float(tokens[1]))\n",
    "        return float(x)\n",
    "    except Exception as e:\n",
    "        print(tokens)\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def isrevenue(x):\n",
    "    if x==\"Non-Applicable\":\n",
    "        return 0\n",
    "    elif x==\"(USD)\":\n",
    "        return 1\n",
    "\n",
    "def salary_type(s):\n",
    "    if not isinstance(s,str):\n",
    "        return None\n",
    "    if 'Per Hour' in s:\n",
    "        return 'Per Hour'\n",
    "    else:\n",
    "        return 'Annual (K)'\n",
    "\n",
    "\n",
    "def reorder_dataframe_columns_inplace(df:pd.DataFrame, desired_order, prefix='delete_'):\n",
    "    filtered_order = [col for col in desired_order if col in df.columns]\n",
    "    df.rename(columns={col: prefix + col for col in filtered_order}, inplace=True)\n",
    "    for col in filtered_order:\n",
    "        df[col] = df[prefix + col]\n",
    "        del df[prefix + col]\n",
    "\n",
    "def preprocessing(df: pd.DataFrame):\n",
    "    df.rename(columns = {'Type of ownership':'Type'}, inplace = True) # ,'Salary Estimate':'salary'\n",
    "    for col in {'Unnamed: 0', 'index', 'Competitors', 'Headquarters'}:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    reorder_dataframe_columns_inplace(df, ['Job Title', 'Company Name', 'Location', 'Salary Estimate', 'Rating', 'Job Description', 'Founded', \n",
    "                                           'Industry', 'Revenue', 'Sector', 'Size', 'Type', 'Easy Apply', 'Duplicate Count'])\n",
    "\n",
    "    df['Company Name'] = df['Company Name'].apply(lambda x:str(x).strip('\\n'))\n",
    "    df['Company Name'] = df['Company Name'].apply(lambda x:str(x).split('\\n')[0] if '\\n' in x else x)\n",
    "\n",
    "    if df['Easy Apply'].dtype != bool:\n",
    "        df['Easy Apply'] = df['Easy Apply'].replace(['-1'],0).replace(['FALSE'],1)\n",
    "        df['Easy Apply'] = df['Easy Apply'].astype(bool)\n",
    "\n",
    "\n",
    "    # df.dropna(how='any',axis=0,inplace=True)\n",
    "    df.fillna(-1,inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    df['Founded']=df['Founded'].astype(int)\n",
    "\n",
    "    df['Size'] = df['Size'].apply(lambda x:x.replace('to','-'))\n",
    "    df['Size'] = df['Size'].apply(lambda x:x.replace('+',' - 0'))\n",
    "    df['Size'] = df['Size'].apply(lambda x:x.replace('employees',' '))\n",
    "    df['Size'] = df['Size'].apply(lambda x:x.replace('Employees',' ')).str.replace(' ','')\n",
    "\n",
    "    df['Salary Type'] = df['Salary Estimate'].apply(salary_type)\n",
    "\n",
    "\n",
    "    # # Drop the rows where salary missing\n",
    "    # df.drop(df[df['Salary Estimate'].isna() | (df['Salary Estimate'] == '-1')].index, inplace=True)\n",
    "\n",
    "    # df['Salary Estimate'].replace([-1, None], '-1')\n",
    "    df['Salary Estimate'] = df['Salary Estimate'].str.replace('[^0-9\\-.]', '', regex=True).str.strip('.')\n",
    "    df['Salary Estimate'].fillna('-1',inplace=True)\n",
    "\n",
    "    df['min_salary'] = df['Salary Estimate'].apply(lambda x: convert_sqft_to_num(x, 'min'))\n",
    "    df['max_salary'] = df['Salary Estimate'].apply(lambda x: convert_sqft_to_num(x, 'max'))\n",
    "\n",
    "    # replace -1 with nan\n",
    "    df['Revenue'] = df['Revenue'].replace(['-1', 'Unknown / Non-Applicable'],None)\n",
    "    df['Sector'] = df['Sector'].replace(['-1'],None)\n",
    "    df['Type'] = df['Type'].replace(['-1', 'Unknown'],None)\n",
    "    df['Size'] = df['Size'].replace(['-1', 'Unknown'],None)\n",
    "    df['Founded'] = df['Founded'].replace([-1],None)\n",
    "    df['Rating'] = df['Rating'].replace([-1],None)\n",
    "    \n",
    "    \n",
    "    df['State'] = df['Location'].apply(lambda x: str(x).split(', ')[1] if ', ' in str(x) else x)\\\n",
    "        .apply(lambda x: state_abbreviations.get(x, x)) #\\\n",
    "#            .apply(lambda x: x if x in state_abbreviations_vals else None)\n",
    "    \n",
    "    \n",
    "def extract_years_experience(description):\n",
    "    # Updated regex pattern to match numbers with an optional '+' sign followed by 'year' or 'years'\n",
    "    # This will match '2 years', '5-7 years', '3+ years', '3 year', etc.\n",
    "    pattern = r'(\\d+\\+?|\\d+-\\d+)\\s+years?'\n",
    "\n",
    "    # Search for the pattern in the description\n",
    "    match = re.search(pattern, description.lower())\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    print(df.shape)\n",
    "    # remove_duplicates(df, exclude='Location') # remove location duplicates of the same role\n",
    "    remove_duplicates(df)\n",
    "    print(df.shape)\n",
    "    preprocessing(df)\n",
    "    print(df.shape)\n",
    "    \n",
    "    clean_job_description(df)\n",
    "    clean_job_title(df)\n",
    "\n",
    "    # Apply the function to create a new column\n",
    "    df['Years Experience'] = df['Job Description'].apply(extract_years_experience)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', 50)\n",
    "# df_new = pd.read_csv('combined scraper results/data engineer 2465.csv')\n",
    "# clean_data(df_new)\n",
    "# df_new.to_csv('combined scraper results/data engineer 2465 clean.csv',index=False)\n",
    "# df_new.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business analyst.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10830, 14)\n",
      "Duplicate Count\n",
      "1.0    10791\n",
      "Name: count, dtype: int64\n",
      "(10830, 14)\n",
      "(10830, 18)\n",
      "(10830, 21)\n",
      "data analyst.csv\n",
      "(9923, 14)\n",
      "Duplicate Count\n",
      "1.0    9882\n",
      "Name: count, dtype: int64\n",
      "(9923, 14)\n",
      "(9923, 18)\n",
      "(9923, 21)\n",
      "data engineer.csv\n",
      "(2465, 14)\n",
      "Duplicate Count\n",
      "1.0    2450\n",
      "Name: count, dtype: int64\n",
      "(2465, 14)\n",
      "(2465, 18)\n",
      "(2465, 21)\n",
      "data scientist.csv\n",
      "(5539, 14)\n",
      "Duplicate Count\n",
      "1.0    5514\n",
      "Name: count, dtype: int64\n",
      "(5539, 14)\n",
      "(5539, 18)\n",
      "(5539, 21)\n"
     ]
    }
   ],
   "source": [
    "path = 'combined scraper results'\n",
    "for fname in os.listdir(path):\n",
    "    if 'clean' in fname:\n",
    "        continue\n",
    "    print(fname)\n",
    "    fn = fname.strip('.csv')\n",
    "    data = pd.read_csv(os.path.join(path, fn+'.csv'))\n",
    "    clean_data(data)\n",
    "    data.to_csv(os.path.join(path, fn+' clean.csv'),index=False)\n",
    "    # print(data.head(4))\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BusinessAnalyst.csv\n",
      "(4092, 15)\n",
      "Duplicate Count\n",
      "1    4092\n",
      "Name: count, dtype: int64\n",
      "(4092, 16)\n",
      "(4092, 18)\n",
      "(4092, 21)\n",
      "DataAnalyst.csv\n",
      "(5631, 15)\n",
      "Duplicate Count\n",
      "1.0    5629\n",
      "Name: count, dtype: int64\n",
      "(5631, 16)\n",
      "(5631, 18)\n",
      "(5631, 21)\n",
      "DataEngineer.csv\n",
      "(2528, 15)\n",
      "Duplicate Count\n",
      "1    2504\n",
      "2      24\n",
      "Name: count, dtype: int64\n",
      "(2516, 16)\n",
      "(2516, 18)\n",
      "(2516, 21)\n",
      "DataScientist.csv\n",
      "(3909, 17)\n",
      "Duplicate Count\n",
      "1    3909\n",
      "Name: count, dtype: int64\n",
      "(3909, 18)\n",
      "(3909, 18)\n",
      "(3909, 21)\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('data_jobs_data','data')\n",
    "for fname in os.listdir(path):\n",
    "    if 'clean' in fname:\n",
    "        continue\n",
    "    print(fname)\n",
    "    fn = fname.strip('.csv')\n",
    "    data = pd.read_csv(os.path.join(path, fn+'.csv'))\n",
    "    clean_data(data)\n",
    "    data.to_csv(os.path.join(path, fn+' clean.csv'),index=False)\n",
    "    # print(data.head(4))\n",
    "    print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si671",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
